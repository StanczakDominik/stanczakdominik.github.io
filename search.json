[
  {
    "objectID": "posts/particle-in-cell-methods.html",
    "href": "posts/particle-in-cell-methods.html",
    "title": "Particle in Cell methods",
    "section": "",
    "text": "I think it might finally be about time to do some plasma physics discussion on this blog, stay true to the name and so on…\nBasically the only actual “scientific” work I have actually done with plasmas up until now is writing a PIC simulation, PIC standing for Particle-in-Cell. I thought I would take this opportunity to explain in my own words what the concept is - I think it’s a clever one.\nThere are many reasons why you might want to simulate a plasma. Simulations are often way cheaper than making a tokamak and causing the plasma to develop turbulence, or sending out a probe to watch check solar flares for traces of magnetic reconnection. There’s also the case of needing simulations to understand and explain your experimental results. For now, however, let’s just assume you have a burning desire to make a few pretty plots and animations like these using data that you don’t have simple access to via experiment."
  },
  {
    "objectID": "posts/particle-in-cell-methods.html#the-advantages",
    "href": "posts/particle-in-cell-methods.html#the-advantages",
    "title": "Particle in Cell methods",
    "section": "The advantages",
    "text": "The advantages\n\nIt’s close to fundamental physics and thus understandable! You get a full picture of what each of the particles does, how the fields behave, while making very few assumptions.\nIt’s lightning fast! The \\(O(N^2)\\) force calculation is reduced to the complexity of your three replacement steps. While you can expect deposition and gathering to be roughly \\(O(Nm)\\) (\\(m\\) being the number of cells), \\(N\\) is much larger than \\(m\\), and the field solver is going to scale independently of \\(N\\) - so that’s still a massive gain over direct summation.\nIt’s easy to parallelize! Each particle is essentially independent for the pushing step (as they only interact with each other via fields), so those movements are trivially parallel. Grid operations can also be done in parallel (though admittedly I haven’t looked into that much, yet - I fully intend to do so).\n\nAnd of course, no description of a simulation method is complete without…"
  },
  {
    "objectID": "posts/particle-in-cell-methods.html#the-disadvantages",
    "href": "posts/particle-in-cell-methods.html#the-disadvantages",
    "title": "Particle in Cell methods",
    "section": "The disadvantages",
    "text": "The disadvantages\n\nThe method is mostly explicit, so that limits your time step and grid size quite a lot. Otherwise, you get spurious instabilities.\nStatistical noise makes life a pain when you’re working on PIC simulations, precisely because you’re modeling your large numbers of real particles with fewer virtual discrete ones. The trick seems to be increasing the particle numbers, but Wikipedia claims that this source of error is more figured out for traditional grid methods. In a way, this also means PICs are a prime target for GPUs, as exhibited by PIConGPU.\n\nStill, PICs are used in many awesome applications, such as plasma turbulence research, and their parallelizability means they’re only going to get more important in the coming exascale computing era.\nI’ll be writing a few follow-up posts going over particular aspects of PIC codes - tricks I’ve picked up along the way, etc. Stay tuned!\n\nReferences\n\nRelativistic kinetic turbulence video by Joonas Nättilä, using the plasmabox code.\nPythonPIC, my less-than-amazing engineering thesis code.\nVlasov Equation - Wikipedia\nPlasma simulation via molecular dynamics example.\nBarnes-Hut plasma simulation example\nParticle in Cell applications -Wikipedia\nGPU PIC\nPIConGPU"
  },
  {
    "objectID": "posts/particle-in-cell-methods.html#footnotes",
    "href": "posts/particle-in-cell-methods.html#footnotes",
    "title": "Particle in Cell methods",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want to learn more about those, I don’t feel like I can give this subject justice better than chapter 1-1 of Birdsall and Langdon’s seminal Plasma Physics via Computer Simulation text.↩︎"
  },
  {
    "objectID": "posts/post-mortem-for-my-engineering-thesis-code-pythonpic.html",
    "href": "posts/post-mortem-for-my-engineering-thesis-code-pythonpic.html",
    "title": "Post mortem for my engineering thesis code, PythonPIC",
    "section": "",
    "text": "I’m giving a presentation on this less-than-glorious subject on Friday, so I figured, hey, it might be a nice time to write a summary of what that old repository on my GitHub page is. In a single video:\nAdmittedly, this post is going to be rather personal - this messy little code was basically my life for a few hundreds of hours."
  },
  {
    "objectID": "posts/post-mortem-for-my-engineering-thesis-code-pythonpic.html#the-motivation",
    "href": "posts/post-mortem-for-my-engineering-thesis-code-pythonpic.html#the-motivation",
    "title": "Post mortem for my engineering thesis code, PythonPIC",
    "section": "The motivation",
    "text": "The motivation\nAt the time, I was quite enamored with Python, NumPy, the ideals of open source scientific software and literate computing. I had seen how most scientific software seems to be written in uglier, less maintainable languages and thoroughly disliked the notion that We, the Scientific Community would have to keep struggling with those, no no no.\nSo I figured, hey, I can probably do better. I know NumPY! It’s basically writing FORTRAN without ever touching FORTRAN!\n\nnarrator voice: It isn’t.\n\nSo I went to the amazing Sławomir Jabłoński of IPPLM (to whom, a shout out, for he is truly an amazing person without whom this work would have gone nowhere). I discussed the idea with him and he seemed to like it. He agreed to supervise me on this idea.\nWhat happened next?"
  },
  {
    "objectID": "posts/post-mortem-for-my-engineering-thesis-code-pythonpic.html#the-breakdown",
    "href": "posts/post-mortem-for-my-engineering-thesis-code-pythonpic.html#the-breakdown",
    "title": "Post mortem for my engineering thesis code, PythonPIC",
    "section": "The breakdown",
    "text": "The breakdown\nWell, stuff happened, not least important of which was procrastination on a scale I had never performed. One of my personal flaws is a tendency to isolate myself and work alone on projects that are better undertaken in groups. I basically started writing a framework, refactoring it endlessly, delaying work on the critical bugs like the actual physics of my simulation, running test cases… What I had thought would have been simple turned out not to be. Don’t get me wrong, I learned a metric ton of Python and numerics knowledge - but I wasn’t getting much closer, and I wasn’t reaching out for feedback that what I was doing was, pretty much, crap.\nI may have had a tiny nervous breakdown then. Eventually I reached out again to IPPLM and mr. Jabłoński, who agreed the situation is pretty bad but didn’t think it worth giving up on. The code ended up working after many adventures:\n\n|\\\n* | 207371b wat\n| |\\  \n| * | 00b554c started writing diagnostics\n| * | 3942c6e continue writing diagnostics, abandoning idea of multifunction simulation load'\n| | * e726000 revert to old simulation init, energy calculation\n| |/  \n| *   ca0742a merged\n| * 62fe945 energy still blows up\n|/  \n*   6e172d6 merged\n...\n* 5068dfb langmuir waves kinda sorta running?\n...\n* b9580ac Finish fixing tests\n* 864d27a Turn recurrent deposition into deposition on a while loop\n* 280d176 Start fixing Laser simulation\n...\n* a5c4623 Fix difficult to find bugs in longitudinal current deposition\n...\n* 080a226 what may amount to release\nand I was able to benchmark it nicely.\n\n\n\nPython runtimes"
  },
  {
    "objectID": "posts/post-mortem-for-my-engineering-thesis-code-pythonpic.html#the-benchmarking",
    "href": "posts/post-mortem-for-my-engineering-thesis-code-pythonpic.html#the-benchmarking",
    "title": "Post mortem for my engineering thesis code, PythonPIC",
    "section": "The benchmarking",
    "text": "The benchmarking\nI then - only then, certainly a failure of foresight on my part - realized that I would need a comparable C/C++ code to benchmark my Python NumPy monstrosity against. That was not a happy thought.\nBut there was no way the low level drudgery could be avoided, so I went ahead, started learning the Eigen3 linear algebra library that has many similarities to NumPy - I needed something that in principle could work similarly, with mostly whole array based computations. The result of that is in this repository.\nAdmittedly, to this day I’m not even one hundred percent sure it does the same thing! The results seemed to be correct for the test cases I did run, but I’m not exactly sure I got all of the bugs. Most of the problems stem from the fact I used numpy.bincount for most of the current and charge deposition functionality (don’t do that, by the way, as that was the least efficient part of the code). That functionality is lacking in Eigen (or at the very least I was unable to find it), so I went ahead and implemented it by hand.\nI then started benchmarking the two codes for identical initial conditions, and it turned out that -\n\n\n\nPython vs C\n\n\nsuccess! PythonPIC is just as fast as the compiled C version!\n… until you compile the C code with -O, the simplest optimization flag:\n\n\n\nPython vs any kind of optimized C\n\n\nI didn’t bother checking -O3, though a friend was happy to remind me of its existence. I might thus be the proud author of the cleanest (well, not the least clean) and least efficient particle-in-cell code on the planet. Gotta start somewhere, right?"
  },
  {
    "objectID": "posts/post-mortem-for-my-engineering-thesis-code-pythonpic.html#the-defense",
    "href": "posts/post-mortem-for-my-engineering-thesis-code-pythonpic.html#the-defense",
    "title": "Post mortem for my engineering thesis code, PythonPIC",
    "section": "The defense",
    "text": "The defense\nThere’s not much to say. I went ahead, printed out a few copies of the thesis, realized via review I had made mistakes in plots etc. and had to print them out again. I went ahead, took my final exam, didn’t get grilled too hard, got a random exam question about - I think, as it’s been a while - the Schroedinger equation, and poof, trust me, I’m an engineer of Applied Physics. I’m still not sure how that happened.\nI took a break from PIC codes for a while afterwards. A snippet of PythonPIC later went into PlasmaPy’s particle stepper capabilities. I still have a thorough dislike for that code and think it could be optimized.\nAll in all, I’m not sure about Python for PICs. On the one hand, they’re pretty inevitably going to be slower than C, C++ for now. You could write the computationally intensive parts in Julia or something - that would probably work and I’ve been itching to try my hand at that recently - but by that logic, you could probably implement the whole thing in Julia anyway, because Python really isn’t giving you anything of value there - maybe besides analysis, but that’s done post-run anyway.\nBut on the other hand, not all PICs are HPiCs - and if you’re just studying plasma physics alongside, say, Birdsall and Langdon, then you can probably live with having your code run a little slower than the sickest optimal C run time. I’ve been able to use my code (mostly before the break… remember the part where I was adding a ton of test cases?) to reproduce a bunch of their results and it’s worked out nicely.\nI’d like to think PythonPIC has, thus, at least some value and use. It’s probably not too useful for high performance and research, but you need to funnel people into those somehow."
  },
  {
    "objectID": "posts/sc2-parse-replays.html",
    "href": "posts/sc2-parse-replays.html",
    "title": "Parsing SC2 replays for later analysis",
    "section": "",
    "text": "I’ve realized I owe you an explanation on how to parse your own SC2 replays for the series of posts on Bayesian SC2 replay data analysis. Let’s go through it here!\nWe’ll use ZephyrBlu’s zephyrus-sc2-parser library, which you can download via pip install zephyrus-sc2-parser."
  },
  {
    "objectID": "posts/sc2-parse-replays.html#parsing-the-replays",
    "href": "posts/sc2-parse-replays.html#parsing-the-replays",
    "title": "Parsing SC2 replays for later analysis",
    "section": "Parsing the replays",
    "text": "Parsing the replays\nThis process currently dumps a boatload of warnings and exceptions, so I’m choosing to wrap the former in a try-except and simply ignore those, and ignore the thrown warnings with warnings.simplefilter(\"ignore\"). Feel free to disable them on your end; but don’t say I didn’t warn you!\nNote that this process takes a while, and we’ll have to do some wrangling later on, so it makes more sense to parse all the replays first and have them all in memory for later. It might fail for larger datasets.\n\nimport pathlib\nimport warnings\n\nimport tqdm.auto as tqdm\nimport zephyrus_sc2_parser\n\nREPLAY_DIRECTORY = \"/home/dominik/Links/SC2Reps\"\nPLAYER_NAME = \"Perfi\"\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    replays = list(pathlib.Path(REPLAY_DIRECTORY).glob(\"*.SC2Replay\"))\n    parsed_replays = {}\n    for replay_file in tqdm.tqdm(replays):\n        try:\n            replay = zephyrus_sc2_parser.parse_replay(replay_file, local=True)\n        except Exception as e:\n            print(f\"Failed for {replay_file}: {e}\")\n            continue\n        parsed_replays[replay_file] = replay\n\n\n\n\nFailed for /home/dominik/Links/SC2Reps/Ephemeron LE (27).SC2Replay: 'NoneType' object has no attribute 'race'\nFailed for /home/dominik/Links/SC2Reps/Triton LE (4).SC2Replay: local variable 'game_length' referenced before assignment\nFailed for /home/dominik/Links/SC2Reps/Winter's Gate LE (4).SC2Replay: local variable 'game_length' referenced before assignment\nFailed for /home/dominik/Links/SC2Reps/Zen LE (4).SC2Replay: 'NoneType' object has no attribute 'race'\nFailed for /home/dominik/Links/SC2Reps/World of Sleepers LE (48).SC2Replay: local variable 'game_length' referenced before assignment\nFailed for /home/dominik/Links/SC2Reps/Whitewater Line LE.SC2Replay: 'NoneType' object has no attribute 'race'\nFailed for /home/dominik/Links/SC2Reps/Bone Temple LE.SC2Replay: 'NoneType' object has no attribute 'race'\nFailed for /home/dominik/Links/SC2Reps/Acropolis LE (19).SC2Replay: 'NoneType' object has no attribute 'race'\nFailed for /home/dominik/Links/SC2Reps/World of Sleepers LE (15).SC2Replay: 'NoneType' object has no attribute 'race'\nFailed for /home/dominik/Links/SC2Reps/Ephemeron LE (25).SC2Replay: 'NoneType' object has no attribute 'race'\n\n\n--- Logging error ---\nTraceback (most recent call last):\n  File \"/home/dominik/.local/lib/python3.8/site-packages/zephyrus_sc2_parser/parser.py\", line 154, in parse_replay\n    players = create_players(player_info, events)\n  File \"/home/dominik/.local/lib/python3.8/site-packages/zephyrus_sc2_parser/utils.py\", line 68, in create_players\n    new_player.race = non_english_races[new_player.race.encode('utf-8')]\nKeyError: b''\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/logging/__init__.py\", line 1081, in emit\n    msg = self.format(record)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/logging/__init__.py\", line 925, in format\n    return fmt.format(record)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/logging/__init__.py\", line 664, in format\n    record.message = record.getMessage()\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/logging/__init__.py\", line 369, in getMessage\n    msg = msg % self.args\nTypeError: not all arguments converted during string formatting\nCall stack:\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/runpy.py\", line 193, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/ipykernel_launcher.py\", line 16, in &lt;module&gt;\n    app.launch_new_instance()\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n    app.start()\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 583, in start\n    self.io_loop.start()\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 149, in start\n    self.asyncio_loop.run_forever()\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n    self._run_once()\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n    handle._run()\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/asyncio/events.py\", line 81, in _run\n    self._context.run(self._callback, *self._args)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/tornado/ioloop.py\", line 690, in &lt;lambda&gt;\n    lambda f: self._run_callback(functools.partial(callback, future))\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n    ret = callback()\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 787, in inner\n    self.run()\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 748, in run\n    yielded = self.gen.send(value)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n    self.do_execute(\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/tornado/gen.py\", line 209, in wrapper\n    yielded = next(result)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2857, in run_cell\n    result = self._run_cell(\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2886, in _run_cell\n    return runner(coro)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n    coro.send(None)\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3062, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3254, in run_ast_nodes\n    if (await self.run_code(code, result,  async_=asy)):\n  File \"/progs/miniconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3331, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"&lt;ipython-input-1-d51f8f2ea671&gt;\", line 16, in &lt;module&gt;\n    replay = zephyrus_sc2_parser.parse_replay(replay_file, local=True)\n  File \"/home/dominik/.local/lib/python3.8/site-packages/zephyrus_sc2_parser/parser.py\", line 162, in parse_replay\n    logging.critical('A KeyError error occured:', error, 'unreadable file info')\nMessage: 'A KeyError error occured:'\nArguments: (KeyError(b''), 'unreadable file info')\n\n\nFailed for /home/dominik/Links/SC2Reps/Eternal Empire LE (20).SC2Replay: local variable 'game_length' referenced before assignment\n\n\n\nAnd I have absolutely no idea how to explain the Logging error. We aren’t missing out on many games, though:\n\nprint(f\"We successfully parsed {len(parsed_replays)} replays, which is {len(parsed_replays)/len(replays):.2%} of the total!\")\n\nWe successfully parsed 464 replays, which is 97.68% of the total!\n\n\nThat was the first step; now, we continue to…"
  },
  {
    "objectID": "posts/sc2-parse-replays.html#pull-the-interesting-data",
    "href": "posts/sc2-parse-replays.html#pull-the-interesting-data",
    "title": "Parsing SC2 replays for later analysis",
    "section": "Pull the interesting data",
    "text": "Pull the interesting data\nNote that this mostly handles 1v1 data; it might be a bit more difficult to filter out stuff such as coop and team games. I would probably recommend filtering them out at an earlier stage, by filename.\n\n# utility function to get our own player ID\ndef grab_player_id(players, name = PLAYER_NAME):\n    for key, player in players.items():\n        if player.name == name:\n            break        \n    else:\n        key = None\n    return key\n\n\nresults = []\nfor replay_file, replay in parsed_replays.items():\n    players, timeline, engagements, summary, meta = replay\n    if all(item is None for item in replay):\n        print(f\"Failed to parse for {replay_file}\")\n        continue\n    my_id = grab_player_id(players, PLAYER_NAME)\n    enemy_id = 1 if (my_id == 2) else 2\n    \n    results.append(\n        dict(\n            replay_file = replay_file,\n            time_played_at = meta['time_played_at'],\n            win = meta[\"winner\"] == my_id,\n            \n            race = players[my_id].race,\n            enemy_race = players[enemy_id].race,\n            \n            mmr = summary['mmr'][my_id],\n            \n            enemy_mmr = summary['mmr'][enemy_id],\n            enemy_nickame = players[enemy_id].name,\n            \n            map_name = meta[\"map\"],\n            duration = meta['game_length'],\n        )\n    )\n\nprint(f\"We successfully pulled data out of {len(results)} replays, which is {len(results)/len(replays):.2%} of the total!\")\n\nFailed to parse for /home/dominik/Links/SC2Reps/Ephemeron LE (24).SC2Replay\nFailed to parse for /home/dominik/Links/SC2Reps/Malwarfare.SC2Replay\nWe successfully pulled data out of 462 replays, which is 97.26% of the total!\n\n\nWhat I’m showing you here is the end result, but if you wanted to add some other metrics, you might be interested in the answer to:\n\nHow do I pick the interesting data?\nWe’ll use the entries from the last replay. Most of them are dictionaries, so it’s pretty easy to get access to their contents:\n\nmeta\n\n{'time_played_at': datetime.datetime(2020, 4, 30, 18, 34, 1, tzinfo=&lt;UTC&gt;),\n 'map': 'Nightshade LE',\n 'game_length': 1040,\n 'winner': 2}\n\n\nIf you run this notebook locally, IPython has a nice widget to browse this data. If you’re reading this on the website, you’ll probably unfortunately see only &lt;IPython.core.display.JSON object&gt;:\n\nfrom IPython.display import JSON\nJSON(summary)\n\n&lt;IPython.core.display.JSON object&gt;\n\n\n\nsummary.keys()\n\ndict_keys(['mmr', 'avg_resource_collection_rate', 'avg_unspent_resources', 'apm', 'spm', 'resources_lost', 'resources_collected', 'workers_produced', 'workers_killed', 'workers_lost', 'supply_block', 'sq', 'avg_pac_per_min', 'avg_pac_action_latency', 'avg_pac_actions', 'avg_pac_gap', 'race'])\n\n\nAs you can (possibly) see, there’s plenty of interesting data that I might use sometime. Beyond what we’re already pulling out:\n\nAverage resource collection rate\nthe spending quotient, a (possibly flawed) measure of macro skill\ntime spent supply blocked\nworkers lost, killed and produced\nper-race statistics:\n\nOrbital Command energy efficiency and idle time\nlikewise for Nexii (Nexuses?)\nSplash efficiency for Protoss\n\n\nI probably wouldn’t use Bayesian inference on all of them, though - it gets hard to come up with a model that involves all of them. Maybe a random forest model would be nice?\nEither way, once we’ve found something interesting it’s simple to access the fields:\n\nsummary['apm'][1]\n\n151.0\n\n\nIt’s a bit more difficult to pull data out of players, as there are dedicated objects storing the data there; we can still make do:\n\nclean_data = {}\nfor player_id, player in players.items():\n    d = player.__dict__.copy()\n    # we have to drop some data that contains custom objects:\n    for dropped_key in [\"current_selection\", \"objects\", \"control_groups\", \"pac_list\", \"current_pac\", \"active_ability\"]:\n        d.pop(dropped_key)\n    clean_data[player_id] = d\nJSON(clean_data)\n\n&lt;IPython.core.display.JSON object&gt;\n\n\nI’ll showcase a few:\n\nplayers[2].upgrades\n\n['WarpGateResearch',\n 'ExtendedThermalLance',\n 'Charge',\n 'ProtossGroundWeaponsLevel1',\n 'PsiStormTech',\n 'GraviticDrive',\n 'BlinkTech',\n 'ProtossGroundWeaponsLevel2',\n 'AdeptPiercingAttack',\n 'ProtossGroundArmorsLevel1',\n 'ProtossGroundWeaponsLevel3',\n 'ProtossShieldsLevel1']\n\n\n\nplayers[2].supply_block\n\n1568\n\n\n\nplayers[2].resources_collected\n\n{'minerals': 31370, 'gas': 11109}\n\n\nA bunch of these keys, such as unspent_resources, are time data, taken at discrete snapshots during the game. There’s more time data, of course, in timeline:\n\nJSON(timeline)\n\n&lt;IPython.core.display.JSON object&gt;\n\n\nAnd I haven’t yet been able to figure this one out:\n\nengagements\n\n[]"
  },
  {
    "objectID": "posts/sc2-parse-replays.html#saving-our-results-to-dataframe-then-to-csv",
    "href": "posts/sc2-parse-replays.html#saving-our-results-to-dataframe-then-to-csv",
    "title": "Parsing SC2 replays for later analysis",
    "section": "Saving our results to DataFrame, then to CSV",
    "text": "Saving our results to DataFrame, then to CSV\nWe’ll also calculate the MMR difference at this step.\n\nimport pandas as pd\ndf = pd.DataFrame(results)\ndf['mmr_diff'] = df.mmr - df.enemy_mmr\ndf\n\n\n\n\n\n\n\n\ntime_played_at\nwin\nrace\nenemy_race\nmmr\nenemy_mmr\nenemy_nickame\nmap_name\nduration\nmmr_diff\n\n\n\n\n0\n2020-05-27 10:32:29+00:00\nTrue\nProtoss\nTerran\n4004\n4173\ngiletjaune\nNightshade LE\n601\n-169\n\n\n1\n2020-06-09 17:11:15+00:00\nFalse\nProtoss\nZerg\n4186\n4147\ndjakette\nEternal Empire LE\n420\n39\n\n\n2\n2020-02-02 17:27:27+00:00\nTrue\nProtoss\nTerran\n3971\n3913\nSyocto\nEphemeron LE\n10\n58\n\n\n3\n2019-12-20 18:53:00+00:00\nTrue\nZerg\nTerran\n2984\n3090\nJason\nSimulacrum LE\n569\n-106\n\n\n4\n2019-12-09 20:36:21+00:00\nTrue\nProtoss\nZerg\n4015\n4024\n&lt;OGCOСK&gt;&lt;sp/&gt;ShushYo\nNightshade LE\n454\n-9\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n457\n2019-11-04 20:53:20+00:00\nFalse\nProtoss\nTerran\n3800\n3883\n&lt;MiClan&gt;&lt;sp/&gt;MiSHANYA\nDisco Bloodbath LE\n396\n-83\n\n\n458\n2020-05-04 12:43:06+00:00\nTrue\nProtoss\nTerran\n3926\n3831\nStaMinA\nGolden Wall LE\n784\n95\n\n\n459\n2020-02-02 17:15:06+00:00\nFalse\nProtoss\nZerg\n4012\n4092\n&lt;0mg&gt;&lt;sp/&gt;Sroljo\nWorld of Sleepers LE\n264\n-80\n\n\n460\n2020-04-19 11:48:32+00:00\nTrue\nProtoss\nZerg\n0\n0\nshadowofmich\nSimulacrum LE\n297\n0\n\n\n461\n2020-04-30 18:34:01+00:00\nTrue\nProtoss\nTerran\n3964\n4055\n&lt;BRs&gt;&lt;sp/&gt;GoodFellas\nNightshade LE\n1040\n-91\n\n\n\n\n462 rows × 10 columns\n\n\n\nAnd we dump that to CSV, and we’re done!\n\ndf.to_csv(\"/home/dominik/Writing/blog/files/replays.csv\")"
  },
  {
    "objectID": "posts/sc2-parse-replays.html#tldr-version",
    "href": "posts/sc2-parse-replays.html#tldr-version",
    "title": "Parsing SC2 replays for later analysis",
    "section": "TL;DR version",
    "text": "TL;DR version\nFeel free to take this script and modify as you see fit!\n\nimport pathlib\nimport warnings\n\nimport tqdm.auto as tqdm\nimport zephyrus_sc2_parser\n\nREPLAY_DIRECTORY = \"/home/dominik/Links/SC2Reps\"\nPLAYER_NAME = \"Perfi\"\nOUTPUT_CSV = \"/home/dominik/Writing/blog/files/replays.csv\"\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    replays = list(pathlib.Path(REPLAY_DIRECTORY).glob(\"*.SC2Replay\"))\n    parsed_replays = {}\n    for replay_file in tqdm.tqdm(replays):\n        try:\n            replay = zephyrus_sc2_parser.parse_replay(replay_file, local=True)\n        except Exception as e:\n            print(f\"Failed for {replay_file}: {e}\")\n            continue\n        parsed_replays[replay_file] = replay\n        \nprint(f\"We successfully pulled data out of {len(results)} replays, which is {len(results)/len(replays):.2%} of the total!\")\n\n# utility function to get our own player ID\ndef grab_player_id(players, name = PLAYER_NAME):\n    for key, player in players.items():\n        if player.name == name:\n            break        \n    else:\n        key = None\n    return key\n\n\nresults = []\nfor replay_file, replay in parsed_replays.items():\n    players, timeline, engagements, summary, meta = replay\n    if all(item is None for item in replay):\n        print(f\"Failed to parse for {replay_file}\")\n        continue\n    my_id = grab_player_id(players, PLAYER_NAME)\n    enemy_id = 1 if (my_id == 2) else 2\n    \n    mmr = summary['mmr'][my_id]\n\n    enemy_mmr = summary['mmr'][enemy_id]\n    results.append(\n        dict(\n            replay_file = replay_file,\n            time_played_at = meta['time_played_at'],\n            win = meta[\"winner\"] == my_id,\n            mmr=mmr,\n            enemy_mmr=enemy_mmr,\n            mmr_diff = mmr - enemy_mmr\n            race = players[my_id].race,\n            enemy_race = players[enemy_id].race,\n            enemy_nickame = players[enemy_id].name,\n            map_name = meta[\"map\"],\n            duration = meta['game_length'],\n        )\n    )\n\nprint(f\"We successfully pulled data out of {len(results)} replays, which is {len(results)/len(replays):.2%} of the total!\")\n\nimport pandas as pd\ndf = pd.DataFrame(results)\ndf['mmr_diff'] = df.mmr - df.enemy_mmr\ndf.to_csv(OUTPUT_CSV)\n\nIf you have questions about this sort of thing, I’ll be happy to help - ask away! :)"
  },
  {
    "objectID": "posts/quantitative-data-analysis-of-the-2d-ising-model.html",
    "href": "posts/quantitative-data-analysis-of-the-2d-ising-model.html",
    "title": "Quantitative data analysis of the 2D Ising model",
    "section": "",
    "text": "Last time, we made a neat little implementation of a 2D Ising model simulation, and we showed that it looks reasonable. Well, that it might, but we can’t be certain of that! I know I said that next time we would, er, %timeit, put it on the GPU and make it GO FAST, but perhaps it’s a better idea to start with some data analysis first, making sure the result we’re getting are quantitatively what we would like them to be - physical.\n\nOne more errata: conveniently forgetting to mention that we were working with the assumption that there’s no external magnetic field. Let it be known that we’re neglecting any external magnetic field. Let’s resummarize the results here:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline\n\n\nplt.rcParams['figure.figsize'] = [11, 6]  # for larger pictures\nsize = (64, 64)\n\nnp.random.seed(0)\na = np.random.randint(low=0, high=2, size=size) * 2 - 1\nplt.imshow(a, cmap='binary')\nplt.colorbar();\n\n\n\n\n\n\n\n\nI’ll modify the better_iteration function we arrived at in two ways: the name could be better, as in, it tries to flip half the spins on the grid, so we’ll call it half_iteration. We’ll also make it so that the spin matrix a is modified in-place, without copying, to save memory and computation time:\n\ndef half_iteration(a, mask, J = 1, beta = 1):\n    interactions = np.roll(a, 1, 0) + np.roll(a, 1, 1) + np.roll(a, -1, 0) + np.roll(a, -1, 1)\n    deltaE = 2 * J * a * interactions\n    boltzmann = np.exp(-beta * deltaE) * mask\n    flip_these = np.random.random(a.shape) &lt; boltzmann\n    a[flip_these] *= -1\n\n# make a checkerboard pattern to preserve causality, updating spins on alternating diagonals in each iteration\ndef mask(a):\n    a_mask = np.ones_like(a)\n    a_mask[::2, ::2] = 0\n    a_mask[1::2, 1::2] = 0\n    return a_mask\n\ndef full_iteration(a, mask, J = 1, beta = 1):\n    # this now becomes a VERY thin wrapper!\n    half_iteration(a, mask, J, beta)\n    half_iteration(a, 1-mask, J, beta)\n\n\na_mask = mask(a)\nfull_iteration(a, a_mask)\nplt.imshow(a, cmap='binary')\n\n\n\n\n\n\n\n\nWe are now in the exact spot where we left off a week ago. Let’s do some quantitative calculation now!\nThere are three main diagnostics (that I know of) that you would be interested in when looking at this kind of system:\n\nthe average magnetization, which is simply the average of all spins\nthe internal energy, which is -0.5 * deltaE in our half_iteration code for each spin (and globally, we would take the average of that, so technically it’s the average internal energy per spin)\nthe heat capacity, which is the partial derivative of internal energy with respect to the temperature\n\nThe first two can be grabbed straightforwardly from each snapshot of the iteration, and then averaged to decrease the effect of fluctuations. The third, as seen on Richard Fitzpatrick’s website, can be calculated as\n\\[C_v = \\frac{dU}{dT} = \\frac{\\sigma_U^2}{k_B T^2} \\] Let’s get to it!\n\ndef magnetization(a):\n    return a.mean(), a.std()\n\ndef internal_energy(a, J = 1):\n    interactions = np.roll(a, 1, 0) + np.roll(a, 1, 1) + np.roll(a, -1, 0) + np.roll(a, -1, 1)\n    current_energy = -J * a * interactions\n    return current_energy.mean(), current_energy.std()\n\nmagnetization(a), internal_energy(a)\n\n((0.0205078125, 0.9997896926986519), (-2.123046875, 1.816938556075228))\n\n\nWell, that’s a whole lot of variance! Our system has definitely not converged to any stable state yet. Let’s repeat that after a bunch of iterations:\n\na_bunch = 100\nfor i in range(a_bunch):\n    full_iteration(a, a_mask)\n\nplt.imshow(a, cmap='binary')\nmagnetization(a), internal_energy(a)\n\n\n\n\n\n\n\n\nWell, that’s better. A little bleak, but it certainly seems stable.\nAll right! This is what we wanted. The system has nearly converged to a stable state, so its magnetization (remember that we’re way under the critical temperature) is almost fully in one direction, and the internal energy is way lower than what we’ve had before, suggesting that this state is much closer to equilibrium.\nLet’s try to get the energies and plot this:\n\nk_b = 1\nT_range = np.linspace(1.5, 3.5, 300)\niterations = 100\nenergies = []\nmagnetizations = []\nfor T in tqdm_notebook(T_range):\n    beta = 1 / (k_b * T)\n    np.random.seed(0)\n    a = np.random.randint(low=0, high=2, size=size) * 2 - 1\n    a_mask = mask(a)\n    for i in range(iterations):\n        full_iteration(a, a_mask, beta=beta)\n    energies.append(internal_energy(a))    \n    magnetizations.append(magnetization(a))   \n\nE, dE = np.array(energies).T\nM, dM = np.array(magnetizations).T\n\n\n\n\n\n\n\nWe’ll also need a bunch of plots, and we’d like to use \\(C_v\\) to let us find the critical temperature as exhibited by our simulation as its maximum. Let’s stick the analysis we want into a function:\n\nOnsager_critical_temperature = 2 / np.log(1 + 2**0.5)  # a theoretical value\n\ndef analysis(T_range, E, dE, M, dM, plotting=True):\n    Cv = dE**2 / k_b / T_range**2  # see Fitzpatrick\n    maximum_index = np.argmax(Cv)\n    our_critical_temperature = T_range[maximum_index]\n    if plotting:\n        plt.plot(T_range, E)\n        plt.xlabel(r\"Temperature [in units where $k_B = 1$]\")\n        plt.ylabel(\"Average energy per spin\")\n        plt.xlim(T_range.min(), T_range.max());\n        plt.vlines(Onsager_critical_temperature, E.min(), E.max())\n\n        plt.figure()\n\n        plt.plot(T_range, M)\n        plt.xlabel(r\"Temperature [in units where $k_B = 1$]\")\n        plt.ylabel(\"Average magnetization\")\n        plt.xlim(T_range.min(), T_range.max());\n        plt.vlines(Onsager_critical_temperature, M.min(), M.max())\n\n\n        plt.figure()\n        plt.plot(T_range, Cv, \"o-\")\n        plt.xlabel(r\"Temperature [in units where $k_B = 1$]\")\n        plt.ylabel(\"Heat capacity per spin\")\n        plt.xlim(T_range.min(), T_range.max());\n        plt.vlines(Onsager_critical_temperature, Cv.min(), Cv.max())\n\n    return our_critical_temperature\nanalysis(T_range, E, dE, M, dM)\n\n2.2959866220735785\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd if I’ve ever seen a plot that says nothing, it’s this one. It seems all random. There’s a simple issue underneath this: since we’re starting from a randomized grid, there is no telling which spin state the system will converge to at a low temperature. Note how there’s much less noise above the critical temperature as found by Onsager back in 1944, as denoted by the vertical line. This makes intuitive sense: above the critical temperature the system converges to an essentially random state, and each of those is basically equivalent.\nLet’s try this again, from a cold start (all spins up):\n\nenergies = []\nmagnetizations = []\nfor T in tqdm_notebook(T_range):\n    beta = 1 / (k_b * T)\n    a = np.ones(size)\n    a_mask = mask(a)\n    for i in range(iterations):\n        full_iteration(a, a_mask, beta=beta)\n    energies.append(internal_energy(a))    \n    magnetizations.append(magnetization(a))   \n\nE, dE = np.array(energies).T\nM, dM = np.array(magnetizations).T\n\nanalysis(T_range, E, dE, M, dM)\n\n\n\n\n\n\n\n2.322742474916388\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s a bit better, but not quite enough. The measurement is still noisy. What we need to do is average the energies and magnetizations over a bunch of iterations. We’ll also stick our entire logic into a function:\n\ndef simulation(iterations, size=size, T_range=T_range, k_b=k_b, plotting=True):\n    energies = []\n    magnetizations = []\n    a_mask = np.ones(size)\n    a_mask[::2, ::2] = 0\n    a_mask[1::2, 1::2] = 0\n    \n    for T in tqdm_notebook(T_range):\n        beta = 1 / (k_b * T)\n        a = np.ones(size)\n        E = np.zeros(2)\n        M = np.zeros(2)\n        for i in range(iterations):\n            E += internal_energy(a)\n            M += magnetization(a)\n            full_iteration(a, a_mask, beta=beta)\n        energies.append(E / iterations)\n        magnetizations.append(M / iterations)   \n\n    E, dE = np.array(energies).T\n    M, dM = np.array(magnetizations).T\n    return analysis(T_range, E, dE, M, dM, plotting)\nsimulation(1000)\n\n\n\n\n\n\n\n2.32943143812709\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo we’re getting relatively good agreement with Fitzpatrick’s results, but on the other hand… our critical temperature is slightly off and the peak is not as sharp as it should probably be. Perhaps this is an issue of small grid size?\n\nsimulation(1000, (128, 128))\n\n\n\n\n\n\n\n2.3361204013377925\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOkay, and what if we go in the other direction, towards smaller sizes:\n\nsimulation(500, (16, 16))\n\n\n\n\n\n\n\n2.3896321070234112\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterestingly, while the peak’s behavior is not changing, the estimated critical temperature does change a bit. However, I’m trying to write this blog in half-hour chunks, and running multiple simulations does eat into that period. In other words, while we have shown that the magnetization and internal energy qualitatively behave just as they should for these few runs, it’d be nice to find out whether we converge to the critical temperature - and we’re going to need to run a whole bunch of simulations to do that. We do need to speed this up, after all!"
  },
  {
    "objectID": "posts/first-joss-review.html",
    "href": "posts/first-joss-review.html",
    "title": "First JOSS review!",
    "section": "",
    "text": "Several months ago, I stumbled upon the journal they call Joss. Well, actually, JOSS - the Journal of Open Source Software, “a developer friendly, open access journal for research software packages”. It’s a completely free, open-source and open-access alternative to established, for-(often-a-lot-of)-profit journals such as those by Reed-Elsevier or Springer.\nAnd a few days ago, I’ve been called into service to review VlaPy, “1D-1V Vlasov-Poisson(-Fokker-Planck) Plasma Physics Simulation Tool”. While I’m digging into that code, I thought I’d write something up about JOSS in general!\nJOSS, as a software-centric journal, is mostly managed via GitHub. Reviewers sign up here (which is a link I’d like to recommend that you follow!). It’s mostly meant to solve one issue:"
  },
  {
    "objectID": "posts/first-joss-review.html#research-software-attribution",
    "href": "posts/first-joss-review.html#research-software-attribution",
    "title": "First JOSS review!",
    "section": "Research software attribution",
    "text": "Research software attribution\nAcknowledgement and funding for developing and maintaining research software tends to be sparse. Remember the black hole image from last year? To quote Andreas Mueller on Twitter:\n\n\n\nSlightly ironic that in the same week @NSF rejects a grant to fund the scipy ecosystem saying that working on it is not impactful enough and hiring developers to work on it is too expensive. Cc @thisgreyspirit (Katie doesn't seem to be on Twitter?)\n\n— Andreas Mueller (@amuellerml) April 14, 2019\n\n\n\nTo counteract that, scientific developers tend to chase exciting new results that accompany new releases of their software. However, that often leads to “more software” instead of “better and more stable software”.\nAs for attribution, citations are everything in the current (rather flawed, in my opinion - I’m not ready with a pull request just yet, though) system of evaluation of scientific work. Package authors tend to try to write books about their works that are then cited. This seems to have improved in the recent years, but I distinctly remember that most __citation__ atributes for packages in the Pythonosphere were books a while ago. Getting software that “just works” and simplifies your life published, from what I’ve heard, can be difficult if not accompanied by a “novel” result.\nJOSS is sort of a hack on this system - it allows for software to be thoroughly reviewed and appreciated for its own merits. Of course, it’s not a permanent solution; but I’m going to leave deliberating on another one for another time :)"
  },
  {
    "objectID": "posts/numpy-ish-gpu-computations-with-cupy.html",
    "href": "posts/numpy-ish-gpu-computations-with-cupy.html",
    "title": "NumPy-ish GPU computations with CuPy",
    "section": "",
    "text": "I’ve recently come across the amazing CuPy library, and given that I haven’t updated this blog in a while, I figured this would be a great opportunity to showcase a few of its capabilities.\nIf you haven’t heard yet, CuPy is NumPy, but on the GPU, and it’s amazing how close that simple description is to reality.  First things first! Make sure you’ve installed it (I used Conda with Python 3.6) and that your Nvidia drivers are on. On my laptop, running an integrated Intel and dedicated Nvidia GPU, I had to simply run sudo modprobe nvidia. Let’s see if that worked:\n!nvidia-smi\n\nTue Jan 22 08:08:35 2019       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 415.27       Driver Version: 415.27       CUDA Version: 10.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 106...  Off  | 00000000:01:00.0 Off |                  N/A |\n| N/A   65C    P0    24W /  N/A |      0MiB /  6078MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\nYup! Let’s get to it. We’ll compare it with NumPy, of course:\nimport cupy as cp\nimport numpy as np\nI’m mostly interested in operations on dense matrices, so let’s get ourselves a sample one. I’m not using an insanely large array due to MemoryErrors, but 2**20 floats should be a reasonable benchmark.\nN = 1024\nA = np.random.random((N, N))\nA\n\narray([[0.10388936, 0.27674225, 0.09349157, ..., 0.59858586, 0.01545899,\n        0.20201765],\n       [0.81588711, 0.19722361, 0.66885061, ..., 0.83687175, 0.15600763,\n        0.6171922 ],\n       [0.73374963, 0.66466975, 0.55082473, ..., 0.68605053, 0.93384799,\n        0.84729118],\n       ...,\n       [0.76718438, 0.40130284, 0.81041205, ..., 0.42829758, 0.42465592,\n        0.67533214],\n       [0.11546777, 0.35548417, 0.645703  , ..., 0.24879487, 0.58897384,\n        0.98993676],\n       [0.96847189, 0.21391942, 0.70259718, ..., 0.32546387, 0.97123257,\n        0.99439515]])\nThe CuPy API is basically Numpy’s API, with a few minor differences here and there:\nB = cp.random.random((N, N))\nB\n\narray([[0.5967192 , 0.51631595, 0.49980612, ..., 0.52830527, 0.4521689 ,\n        0.27857874],\n       [0.80999042, 0.32971922, 0.74034167, ..., 0.7316576 , 0.05339145,\n        0.67494372],\n       [0.66954774, 0.08282191, 0.06237442, ..., 0.85821394, 0.33912042,\n        0.00146102],\n       ...,\n       [0.87827673, 0.58662314, 0.97428079, ..., 0.1239315 , 0.90813556,\n        0.55808706],\n       [0.59890383, 0.54480358, 0.59180028, ..., 0.03094922, 0.54241454,\n        0.45274242],\n       [0.34639887, 0.49254118, 0.28915567, ..., 0.86708966, 0.97695957,\n        0.63873008]])\nOne thing that can be noticed already - that displayed numbers! Right there, in Jupyter! All the memory transfer is done for you as need be, though you can also force it as needed. To me, that’s pretty amazing! Let’s make sure this is actually on the GPU:\n!nvidia-smi\n\nTue Jan 22 08:08:36 2019       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 415.27       Driver Version: 415.27       CUDA Version: 10.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 106...  Off  | 00000000:01:00.0 Off |                  N/A |\n| N/A   66C    P2    24W /  N/A |     95MiB /  6078MiB |      1%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0     12081      C   ...ik/.miniconda3/envs/nbody3.6/bin/python    85MiB |\n+-----------------------------------------------------------------------------+\nClearly a bunch of memory is allocated."
  },
  {
    "objectID": "posts/numpy-ish-gpu-computations-with-cupy.html#a-few-benchmarks",
    "href": "posts/numpy-ish-gpu-computations-with-cupy.html#a-few-benchmarks",
    "title": "NumPy-ish GPU computations with CuPy",
    "section": "A few benchmarks",
    "text": "A few benchmarks\nAll right, let’s get to the actual number crunching. Let’s take the simple element-wise log of each element in the array (on CPU that’s going to run with the MKL-accelerated Numpy on an i7):\n\n%%timeit -o\nnp.log(A)\n\n24.4 ms ± 1.53 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n&lt;TimeitResult : 24.4 ms ± 1.53 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)&gt;\n\n\nRespectable, I suppose. Let’s see how CuPy fares against that:\n\n%%timeit -o\ncp.log(B)\n\n453 µs ± 735 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n\n&lt;TimeitResult : 453 µs ± 735 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)&gt;\n\n\nInstantly, I noticed two things:\n\nMy laptop fan started spinning up immediately after running that command. Clearly something more intense is going on there.\nMy screen went black. Fun fact: I wrote this post out of bed, without having plugged my laptop in - and my current system configuration did not enjoy having a power-hungry GPU try to run on battery, so it just switched off instantly. Consider yourself warned!\n\nAfter rebooting and using the classic Restart and run all, a third fun fact occured to me: a different SI prefix on the GPU result!\n\n__.average / _.average\n\n53.877025466882685\n\n\nThat’s a pretty okay speedup for swapping n to c in the import statement.\nLet’s see how well it’s going to parallelize a matrix multiplication:\n\ncpu_operator = %timeit -o A @ A\n\n25.2 ms ± 3.19 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n\n\n\ngpu_operator = %timeit -o B @ B\n\n18.7 ms ± 48.6 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\nNote how that’s literally the same operation in terms of code, as we’re not using Numpy’s functions, rather - both of these classes define an @ operator. This is going to come up later…\n\ncpu_operator.average / gpu_operator.average\n\n1.3472357440051654\n\n\nWell, suprisingly, this is nowhere near as large of a speedup as I would expect! My results seem to vary a bit, though:\n\ngpu_operator_saved = %timeit -o B2 = B @ B\n\n18.7 ms ± 4.83 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n\n\n\ngpu_dot = %timeit -o cp.dot(B, B)\n\n9.37 ms ± 10.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\ngpu_dot_saved = %timeit -o B3 = cp.dot(B, B)\n\n17.4 ms ± 3.26 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\n\nBtarget= cp.empty_like(B)\ngpu_dot_out = %timeit -o cp.dot(B, B, Btarget)\n\n18.7 ms ± 9.19 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n\n\nI think I may come back to the matrix multiplication issue in the future, because it seems like there are multiple ways to do it and it’s not clear which one is the best. Weirdly, the winner seems to be .dot(B, B), but…without saving. Let’s keep this post to an overview of CuPy’s functionality and possibly revisit that in the future. This may have been a BLAS/cuBLAS issue that I don’t quite understand yet."
  },
  {
    "objectID": "posts/numpy-ish-gpu-computations-with-cupy.html#further-functionality-review",
    "href": "posts/numpy-ish-gpu-computations-with-cupy.html#further-functionality-review",
    "title": "NumPy-ish GPU computations with CuPy",
    "section": "Further functionality review",
    "text": "Further functionality review\nOkay, but what actually is B?\n\ntype(B)\n\ncupy.core.core.ndarray\n\n\nAll right, some internal Cupy ndarray class. It’s pretty simple to turn it into something in host device memory, though:\n\ntype(cp.asnumpy(B))\n\nnumpy.ndarray\n\n\n\nB[0], cp.asnumpy(B)[0]\n\n(array([0.5967192 , 0.51631595, 0.49980612, ..., 0.52830527, 0.4521689 ,\n        0.27857874]),\n array([0.5967192 , 0.51631595, 0.49980612, ..., 0.52830527, 0.4521689 ,\n        0.27857874]))\n\n\nJust to make sure this is actually the same array:\n\nnp.allclose(B, cp.asnumpy(B))\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-23-8ff31bdd3597&gt; in &lt;module&gt;\n----&gt; 1 np.allclose(B, cp.asnumpy(B))\n\n~/.local/lib/python3.6/site-packages/numpy/core/numeric.py in allclose(a, b, rtol, atol, equal_nan)\n   2268 \n   2269     \"\"\"\n-&gt; 2270     res = all(isclose(a, b, rtol=rtol, atol=atol, equal_nan=equal_nan))\n   2271     return bool(res)\n   2272 \n\n~/.local/lib/python3.6/site-packages/numpy/core/numeric.py in isclose(a, b, rtol, atol, equal_nan)\n   2352             return less_equal(abs(x-y), atol + rtol * abs(y))\n   2353 \n-&gt; 2354     x = asanyarray(a)\n   2355     y = asanyarray(b)\n   2356 \n\n~/.local/lib/python3.6/site-packages/numpy/core/numeric.py in asanyarray(a, dtype, order)\n    551 \n    552     \"\"\"\n--&gt; 553     return array(a, dtype, copy=False, order=order, subok=True)\n    554 \n    555 \n\nValueError: object __array__ method not producing an array\n\n\n\nClose, but no cigar! I think this may be getting out of date relatively soon, but right now NumPy doesn’t know how to handle our B GPU array. Another example of this is:\n\nnp.log(B)\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-24-31256e41f3b1&gt; in &lt;module&gt;\n----&gt; 1 np.log(B)\n\nValueError: object __array__ method not producing an array\n\n\n\nWhat you could do instead is compare this right on the GPU, going from GPU to host to GPU again:\n\ncp.allclose(B, cp.asarray(cp.asnumpy(B)))\n\n\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-25-159d67e41f21&gt; in &lt;module&gt;\n----&gt; 1 cp.allclose(B, cp.asarray(cp.asnumpy(B)))\n\nAttributeError: module 'cupy' has no attribute 'allclose'\n\n\n\nAnd this is, actually, the first time I saw cupy not implementing something in NumPy’s API! It’s pretty easy to get around this, in this instance:\n\ncp.all(cp.isclose(B, cp.asarray(cp.asnumpy(B))))\n\narray(True)\n\n\nI guess that’s no proof that this works correctly, but it’s at least an argument :)\nHowever… Wait a minute. What’s that array thing doing there? As far as I have been able to figure out, this is a single element array allocated in GPU memory that .all() reduces our boolean NxN isclose array to. It’s pretty simple to convert to a normal Python bool, though:\n\nbool(_)\n\nTrue\n\n\nReshape works, as does summing along an axis:\n\nB.reshape(N, N, 1).sum(axis=1)\n\narray([[504.85090841],\n       [524.895922  ],\n       [511.81485662],\n       ...,\n       [505.07597442],\n       [505.44331639],\n       [508.71877327]])\n\n\nSo do statistical functions:\n\nB.mean(axis=0)\n\narray([0.49501721, 0.51289292, 0.51160649, ..., 0.49777249, 0.50287185,\n       0.49873693])\n\n\nYou can raise stuff to powers and sum to scalars:\n\n(A**3).sum(), (B**3).sum()\n\n(262231.4456098349, array(262330.20455528))\n\n\nAnd of course, once again we need to force a cast to a Python float:\n\nfloat(_[1])\n\n262330.2045552799\n\n\nYou can also, if you want to, sum into previously allocated arrays (I was thinking of using this to test performance differences between cupy and numba.cuda, haven’t gotten to that yet, though):\n\nAx = np.linspace(0, 1, N)\nAx\n\narray([0.00000000e+00, 9.77517107e-04, 1.95503421e-03, ...,\n       9.98044966e-01, 9.99022483e-01, 1.00000000e+00])\n\n\n\nA.shape, Ax.shape\n\n((1024, 1024), (1024,))\n\n\n\nA.sum(axis=1, out=Ax)\n\narray([497.48857494, 510.25752741, 493.57497492, ..., 515.3009242 ,\n       499.92205554, 512.74308963])\n\n\n\nBx = cp.linspace(0, 1, N)\nBx\n\narray([0.00000000e+00, 9.77517107e-04, 1.95503421e-03, ...,\n       9.98044966e-01, 9.99022483e-01, 1.00000000e+00])\n\n\n\nB.shape, Bx.shape\n\n((1024, 1024), (1024,))\n\n\n\nB.sum(axis=1, out=Bx)\n\narray([504.85090841, 524.895922  , 511.81485662, ..., 505.07597442,\n       505.44331639, 508.71877327])\n\n\n\nBx\n\narray([504.85090841, 524.895922  , 511.81485662, ..., 505.07597442,\n       505.44331639, 508.71877327])\n\n\nRandom numbers start from different seeds:\n\ncp.random.seed(0)\nRgpu = cp.random.random()\nnp.random.seed(0)\nRcpu = np.random.random()\nRgpu - Rcpu\n\narray(0.01273565)\n\n\n\ncp.random.seed(0)\nRgpu2 = cp.random.random()\nRgpu2 - Rgpu\n\narray(0.)\n\n\nIndexing works just like we know and love it from numpy:\n\nBx\n\narray([504.85090841, 524.895922  , 511.81485662, ..., 505.07597442,\n       505.44331639, 508.71877327])\n\n\n\nBx[0] = 3\nBx\n\narray([  3.        , 524.895922  , 511.81485662, ..., 505.07597442,\n       505.44331639, 508.71877327])\n\n\n\nBx[1::2] = -1\nBx\n\narray([  3.        ,  -1.        , 511.81485662, ...,  -1.        ,\n       505.44331639,  -1.        ])\n\n\nThe amazing power tool that is einsum works as well, let’s use it to compute the array’s trace:\n\ncp.einsum('ii-&gt;', B), cp.sum(cp.diag(B))\n\n(array(493.15631992), array(493.15631992))"
  },
  {
    "objectID": "posts/simple-binder-usage-with-sphinx-gallery-through-jupytext.html",
    "href": "posts/simple-binder-usage-with-sphinx-gallery-through-jupytext.html",
    "title": "Simple Binder usage with Sphinx-Gallery through Jupytext",
    "section": "",
    "text": "It's been a busy week for PlasmaPy. I recently found out about Binder support in sphinx-gallery. The latter is a package that we use to turn python scripts with comments into Sphinx pages and Jupyter Notebooks. I figured adding that could be a nice fit for our existing example gallery .\nHowever, I quickly realized that the system in place is a bit unwieldy. Binder takes a link to an existing GitHub repository and executes .ipynb notebooks located there online. However, with sphinx-gallery, we don't have those notebooks in the repository - we have .py files with comments. The currently recommended way of setting this up with sphinx-gallery is keeping your built documentation in another repository and hosting it via something along the lines of GitHub Pages rather than ReadTheDocs, which we are currently using.\nI added the results of this investigation to sphinx-gallery's docs, but I didn't want to switch away from RTD, so I figured I'd go ahead and find another way. I think I've got something that works well enough now!\nTrigger warning: later on during this post, there may be monkeypatching of sphinx_gallery internals. Beware.\n\nUsing Jupytext\nThe Jupytext project is kind of like nbconvert, but two-way. It lets you turn notebooks into scripts and vice versa. The interesting thing is that, as per Jupytext's documentation, it is possible to let Binder's jupyter instance parse sphinx-gallery style .py files as jupyter notebooks. This was done in PlasmaPy#656 . First, let's instruct the in-binder Jupyter instance to parse .py files in .jupyter/jupyter_notebook_config.py via blatant copy-paste:\nc.NotebookApp.contents_manager_class = \"jupytext.TextFileContentsManager\"\nc.ContentsManager.preferred_jupytext_formats_read = \"py:sphinx\"\nc.ContentsManager.sphinx_convert_rst2md = True\nAnd then let's also add a binder/requirements.txt file that lets Binder know what Python packages to download while building the repository's image. The version I had there was pretty shoddy, as CI/setup.py/packaging errors surfaced while I was tinkering with this. Long story short, something like this should do:\n-r ../requirements/automated-documentation-tests.txt\njupytext\n.\nWhere, in didactic order:\n\njupytext should be pretty self-explanatory,\n. is the repository's package itself (here, PlasmaPy), as accessed by setup.py\n-r ../requirements/automated-documentation-tests.txt reads a pip requirements file specifying our documentation requirements. I think with a proper extras_require specified in setup.py, these two lines could be collapsed simply into .[dev] or some such. Note that -r takes a path relative to the file, thus the ../\n\nAt this point all this really is is implementing what's mentioned in Jupytext's docs. The result is as follows:\n\n\n\nimage\n\n\nBut you might notice an inconsistency in the Sphinx-rendered gallery itself: even if we were to configure docs to display Binder links they will point to a path as imagined by the current implementation in Sphinx-Gallery, such as:\nhttps://gke.mybinder.org/v2/gh/PlasmaPy/PlasmaPy/master?filepath=plasmapy/examples/auto_examples/plot_physics.ipynb\nNote the spurious auto_examples directory supposedly including an .ipynb file. This obviously doesn't work for our use case, so we'd like to be able to change the generated links somehow...\n\n\nMonkeypatching\nThis (or rather, PlasmaPy#658 ) is where it gets dirty. The solution developed in cooperation with Stuart Mumford (of SunPy fame, who contributed the idea which I implemented) is monkeypatching sphinx-gallery's link generation code. It's simple, yet effective.\nLet's use this config for sphinx-gallery:\nsphinx_gallery_conf = {\n    # path to your examples scripts\n    'examples_dirs': '../plasmapy/examples',\n    # path where to save gallery generated examples\n    'backreferences_dir': 'gen_modules/backreferences',\n    'gallery_dirs': 'auto_examples',\n    'binder': {\n        'org': 'PlasmaPy',\n        'repo': 'PlasmaPy',\n        'branch': 'master',\n        'binderhub_url': 'https://mybinder.org',\n        'dependencies': ['../binder/requirements.txt'],\n        'notebooks_dir': 'plasmapy/examples',\n    }\n}\nand add this fragment of sphinx_gallery.binder code with a modification into Sphinx's conf.py file:\n# Patch sphinx_gallery.binder.gen_binder_rst so as to point to .py file in repository\nimport sphinx_gallery.binder\ndef patched_gen_binder_rst(fpath, binder_conf, gallery_conf):\n    \"\"\"Generate the RST + link for the Binder badge.\n    ...\n    \"\"\"\n    binder_conf = sphinx_gallery.binder.check_binder_conf(binder_conf)\n    binder_url = sphinx_gallery.binder.gen_binder_url(fpath, binder_conf, gallery_conf)\n\n    # I added the line below:\n    binder_url = binder_url.replace(gallery_conf['gallery_dirs'] + os.path.sep, \"\").replace(\"ipynb\", \"py\")\n\n    rst = (\n        \"\\n\"\n        \"  .. container:: binder-badge\\n\\n\"\n        \"    .. image:: https://mybinder.org/badge_logo.svg\\n\"\n        \"      :target: {}\\n\"\n        \"      :width: 150 px\\n\").format(binder_url)\n    return rst\n\n# And then we finish our monkeypatching misdeed by redirecting sphinx-gallery to use our function:\nsphinx_gallery.binder.gen_binder_rst = patched_gen_binder_rst\nThe current gallery is located here, and an example link is https://mybinder.org/v2/gh/PlasmaPy/PlasmaPy/master?filepath=plasmapy/examples/particle_stepper.py - and you should instantly see it points to the right spot!\nObviously, it would be better to implement such link customization in sphinx-gallery itself somehow, but it's up to their maintainers to decide whether this kind of combo usage with Jupytext is in scope for their project. For now, the monkeypatch solution works decently. I'll try to update this post if that comes about.\n\n\nUpdate - requirements\n@jdkent on GitHub suggests that if the above doesn't work for you, you should make sure the Sphinx version you're using is 2 or newer."
  },
  {
    "objectID": "posts/plasmapy-v060-release.html",
    "href": "posts/plasmapy-v060-release.html",
    "title": "PlasmaPy v0.6.0 release!",
    "section": "",
    "text": "I’ll take the opportunity to cross-post this one from the PlasmaPy website; I’m pretty happy about how this went."
  },
  {
    "objectID": "posts/plasmapy-v060-release.html#two-fluid-dispersion-relations",
    "href": "posts/plasmapy-v060-release.html#two-fluid-dispersion-relations",
    "title": "PlasmaPy v0.6.0 release!",
    "section": "Two-fluid dispersion relations",
    "text": "Two-fluid dispersion relations\n\n\n\nIn PR #932 Ramiz Qudsi implemented P. M. Bellan’s 2012 full two-fluid dispersion relation for any electron-ion system. Take a look at the notebook introducing that."
  },
  {
    "objectID": "posts/plasmapy-v060-release.html#proton-radiography",
    "href": "posts/plasmapy-v060-release.html#proton-radiography",
    "title": "PlasmaPy v0.6.0 release!",
    "section": "Proton radiography",
    "text": "Proton radiography\n\n\n\nPeter Heuer designed and implemented a framework for synthetic proton radiography. This is a multi-PR tour de force that simulates particles moving through EM fields and hitting a detector plane, generating histograms such as the above. I’d point you to the notebook on that, but there’s three right now."
  },
  {
    "objectID": "posts/plasmapy-v060-release.html#analysis-diagnostic-framework-langmuir-probes",
    "href": "posts/plasmapy-v060-release.html#analysis-diagnostic-framework-langmuir-probes",
    "title": "PlasmaPy v0.6.0 release!",
    "section": "Analysis & Diagnostic framework; Langmuir probes",
    "text": "Analysis & Diagnostic framework; Langmuir probes\n\n\n\nErik Everson is spearheading work on our analysis & diagnostic framework. As the first example, we’ve got a new and improved set of tools for swept Langmuir analysis."
  },
  {
    "objectID": "posts/cupy_nbody_direct_force_calculation.html",
    "href": "posts/cupy_nbody_direct_force_calculation.html",
    "title": "CuPy speedup of naive N-Body vectorized force calculation",
    "section": "",
    "text": "I had intended to write a post about speeding up our Numpy Ising implementation, which we found out gave reasonable numerical values, though the small grids we were able to use limited the accuracy a fair bit. However, a few difficulties came up, so I thought instead (to keep writing these a habit!) I would write a little bit about using CuPy to speed up force calculations in N-body simulations. This might be a point I’ll come back to later on this blog, as I have an ongoing project implementing that.\n\nThe part of the n-body simulation we’ll look at is the calculation of forces, where the force on the i-th point particle or celestial object is:\n\\[ F_i = \\sum_j F_{ij} = G \\sum_j \\frac{m_i m_j}{|\\vec{r_i}-\\vec{r_j}|^3 } (\\vec{r_i}-\\vec{r_j}) \\]\nFrom this, Newton’s law gives \\(\\vec{a_i} = \\vec{F_i} / m_i\\). I was trying to use my own implementation of this vectorization, but I found a neat implementation by PMende on Stack that’s both more general and faster than what I had been doing. Let’s take a look!\n\nimport numpy\nimport cupy\nimport numpy_html\n\nI basically copypasted the following:\n\ndef accelerations(positions, masses, G = 1):\n    '''\n    https://stackoverflow.com/a/52562874\n    \n    Params:\n    - positions: numpy array of size (n,3)\n    - masses: numpy array of size (n,)\n    '''\n    xp = cupy.get_array_module(positions)\n    mass_matrix = masses.reshape((1, -1, 1))*masses.reshape((-1, 1, 1))\n    disps = positions.reshape((1, -1, 3)) - positions.reshape((-1, 1, 3)) # displacements\n    dists = xp.linalg.norm(disps, axis=2)\n    dists[dists == 0] = 1 # Avoid divide by zero warnings\n    forces = G*disps*mass_matrix/xp.expand_dims(dists, 2)**3\n    return forces.sum(axis=1)/masses.reshape(-1, 1)\n\nThe main change I made was adding this line:\n    xp = cupy.get_array_module(positions)\nWhich returns numpy if we pass in a numpy.ndarray and cupy if we pass in a CuPy array. This will make this function more generic for our purposes.\nLet’s take a look at what each of those lines does. For the illustrations, we’ll take some particularly simple values:\n\nN = 5\nm = numpy.arange(N) + 1\nr = (numpy.arange(N*3)**2).reshape((N, 3))\nm\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n\n\n\nPretty printing of arrays, by the way, is provided by the awesome numpy_html package. We can now start digging! Let’s first investigate the mass_matrix:\n\nmass_matrix = m.reshape((1, -1, 1)) * m.reshape((-1, 1, 1))\nmass_matrix.T\n\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n\n\n2\n4\n6\n8\n10\n\n\n3\n6\n9\n12\n15\n\n\n4\n8\n12\n16\n20\n\n\n5\n10\n15\n20\n25\n\n\n\n\n\n\n\n\nThis was a (5, 5, 1)-shaped array, but I used a .T transposition so that it would print more nicely, as a (1, 5, 5)-shaped array. This shows that the (i, j)-th entry is just \\(m_i m_j\\) - with the shape it has, we should be able to take advantage of Numpy broadcasting in our calculation.\nLet’s now look at the displacements - disps. The line is\n    disps = r.reshape((1, -1, 3)) - r.reshape((-1, 1, 3))\nbut let’s break it down a little bit more. r is\n\nr\n\n\n\n\n0\n1\n4\n\n\n9\n16\n25\n\n\n36\n49\n64\n\n\n81\n100\n121\n\n\n144\n169\n196\n\n\n\n\n\nwhile if you reshape it with a single added dimension (signified by 1) in the first place:\n\nr.reshape((1, -1, 3))\n\n\n\n\n\n\n\n\n\n\n0\n1\n4\n\n\n9\n16\n25\n\n\n36\n49\n64\n\n\n81\n100\n121\n\n\n144\n169\n196\n\n\n\n\n\n\n\n\nwhile if we were to add a dimension in the second slot:\n\nr.reshape((-1, 1, 3))\n\n\n\n\n\n\n\n\n\n\n0\n1\n4\n\n\n\n\n\n\n\n\n9\n16\n25\n\n\n\n\n\n\n\n\n36\n49\n64\n\n\n\n\n\n\n\n\n81\n100\n121\n\n\n\n\n\n\n\n\n144\n169\n196\n\n\n\n\n\n\n\n\nWe thus have a (1, 5, 3)-shaped array and a (5, 1, 3) array. Numpy (and anything implementing Numpy’s broadcasting API by extension) is going to expand that into a (5, 5, 3) array:\n\ndisps = r.reshape((1, -1, 3)) - r.reshape((-1, 1, 3))\ndisps.T\n\n\n\n\n\n\n\n\n\n\n0\n-9\n-36\n-81\n-144\n\n\n9\n0\n-27\n-72\n-135\n\n\n36\n27\n0\n-45\n-108\n\n\n81\n72\n45\n0\n-63\n\n\n144\n135\n108\n63\n0\n\n\n\n\n\n\n\n\n0\n-15\n-48\n-99\n-168\n\n\n15\n0\n-33\n-84\n-153\n\n\n48\n33\n0\n-51\n-120\n\n\n99\n84\n51\n0\n-69\n\n\n168\n153\n120\n69\n0\n\n\n\n\n\n\n\n\n0\n-21\n-60\n-117\n-192\n\n\n21\n0\n-39\n-96\n-171\n\n\n60\n39\n0\n-57\n-132\n\n\n117\n96\n57\n0\n-75\n\n\n192\n171\n132\n75\n0\n\n\n\n\n\n\n\n\nWhich I chose to print with a transpose (as a (3, 5, 5) array) so as to illustrate the structure a bit more. Each of the three (5, 5) arrays displays a different spatial component of \\(\\vec{r_i} - \\vec{r_j}\\). The arrays are antisymmetric as \\(\\vec{r_{ij}} = - \\vec{r_{ji}}\\).\nLet’s continue with the calculations! The next line simply calculates the norms of those inter-particle distances, outputting an (N, N) array (summing over the “spatial dimensions” axis):\n\ndists = np.linalg.norm(disps, axis=2)\ndists\n\n\n\n\n0.\n27.33130074\n84.85281374\n173.35224256\n292.95733478\n\n\n27.33130074\n0.\n57.78408085\n146.47866739\n266.22359024\n\n\n84.85281374\n57.78408085\n0.\n88.74119675\n208.53776636\n\n\n173.35224256\n146.47866739\n88.74119675\n0.\n119.81235329\n\n\n292.95733478\n266.22359024\n208.53776636\n119.81235329\n0.\n\n\n\n\n\nThe next step is pretty clever. Since in disps (see above) each diagonal element is zero (since that’s \\(\\vec{r_{ii}}\\)), and we’ll be dividing those by distances, we’re going to have Numpy screaming obscenities at us for dividing by zero. But since 0 / 1 = 0, we lose nothing and gain peace of mind by doing:\n\ndists[dists == 0] = 1\ndists\n\n\n\n\n1.\n27.33130074\n84.85281374\n173.35224256\n292.95733478\n\n\n27.33130074\n1.\n57.78408085\n146.47866739\n266.22359024\n\n\n84.85281374\n57.78408085\n1.\n88.74119675\n208.53776636\n\n\n173.35224256\n146.47866739\n88.74119675\n1.\n119.81235329\n\n\n292.95733478\n266.22359024\n208.53776636\n119.81235329\n1.\n\n\n\n\n\nSimple and effective! In my own implementation I had np.inf instead of 1 so as to get anything / np.inf == 0, but if anything = 0 for the problematic cases, that’s fine as well.\nThe next line simply adds a dimension:\n\ndists.shape, np.expand_dims(dists, 2).shape\n\n((5, 5), (5, 5, 1))\n\n\nFor those curious as to why not just .reshape((-1, -1, 1)):\n\ntry:\n    dists.reshape((-1, -1, 1))\nexcept ValueError as e:\n    print(e)\n\ncan only specify one unknown dimension\n\n\nAnd now we can finally calculate the forces themselves, first getting each of \\(\\vec{F_{ij}}\\):\n\nG = 1  # because let's be real...\nforces = G * disps * mass_matrix / np.expand_dims(dists, 2) ** 3\nforces\n\n\n\n\n\n\n\n\n\n\n0.\n0.\n0.\n\n\n0.00088164\n0.0014694\n0.00205716\n\n\n0.00017678\n0.0002357\n0.00029463\n\n\n6.2195164e-05\n7.60163116e-05\n8.98374591e-05\n\n\n2.86364625e-05\n3.34092063e-05\n3.81819501e-05\n\n\n\n\n\n\n\n\n-0.00088164\n-0.0014694\n-0.00205716\n\n\n0.\n0.\n0.\n\n\n0.00083963\n0.00102622\n0.00121281\n\n\n0.00018327\n0.00021382\n0.00024436\n\n\n7.15474501e-05\n8.10871102e-05\n9.06267702e-05\n\n\n\n\n\n\n\n\n-0.00017678\n-0.0002357\n-0.00029463\n\n\n-0.00083963\n-0.00102622\n-0.00121281\n\n\n0.\n0.\n0.\n\n\n0.00077271\n0.00087574\n0.00097877\n\n\n0.00017863\n0.00019848\n0.00021833\n\n\n\n\n\n\n\n\n-6.2195164e-05\n-7.60163116e-05\n-8.98374591e-05\n\n\n-0.00018327\n-0.00021382\n-0.00024436\n\n\n-0.00077271\n-0.00087574\n-0.00097877\n\n\n0.\n0.\n0.\n\n\n0.0007326\n0.00080237\n0.00087214\n\n\n\n\n\n\n\n\n-2.86364625e-05\n-3.34092063e-05\n-3.81819501e-05\n\n\n-7.15474501e-05\n-8.10871102e-05\n-9.06267702e-05\n\n\n-0.00017863\n-0.00019848\n-0.00021833\n\n\n-0.0007326\n-0.00080237\n-0.00087214\n\n\n0.\n0.\n0.\n\n\n\n\n\n\n\n\nAnd we can contract that to \\(\\vec{F_i}\\) by summing over the other-particle index:\n\nforces.sum(axis = 1)\n\n\n\n\n0.00114925\n0.00181453\n0.00247981\n\n\n0.00021281\n-0.00014827\n-0.00050936\n\n\n-6.50662895e-05\n-0.0001877\n-0.00031034\n\n\n-0.00028558\n-0.00036321\n-0.00044083\n\n\n-0.00101141\n-0.00111535\n-0.00121928\n\n\n\n\n\nAnd from here, a simple division by the masses suffices to get the acceleration, but we need to remember to turn the mass into a (N, 1) array via a simple reshape:\n\nforces.sum(axis=1)/m.reshape(-1, 1)\n\n\n\n\n0.00114925\n0.00181453\n0.00247981\n\n\n0.00010641\n-7.4137417e-05\n-0.00025468\n\n\n-2.16887632e-05\n-6.25669817e-05\n-0.00010345\n\n\n-7.13957375e-05\n-9.08016861e-05\n-0.00011021\n\n\n-0.00020228\n-0.00022307\n-0.00024386\n\n\n\n\n\nLet’s run a quick test first that also serves to illustrate the results. We’ll first write a simple function that prepares some reasonable parameters - masses and positions in arrays provided by our chosen packages.\n\ndef prep(N, np = numpy, seed = 0):\n    np.random.seed(seed)\n    m = np.abs(np.random.normal(loc=100, scale=20, size=N))\n    r = np.random.normal(size=(N, 3))\n    return r, m\n\nFor a test, we’ll set z = 0:\n\nr, m = prep(10, numpy, seed = 17)\nr[:, -1] = 0\n\nax, ay, az = accelerations(r, m).T\nx, y, z = r.T\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.scatter(x, y, m);\nplt.quiver(x, y, ax, ay);\n\n\n\n\n\n\n\n\nLooks just about right! The system is evidently self-gravitating. Let’s do the same for a few more bodies:\n\nr, m = prep(500, numpy, seed = 17)\nr[:, -1] = 0\n\nax, ay, az = accelerations(r, m).T\nx, y, z = r.T\n\nplt.scatter(x, y, m);\nplt.quiver(x, y, ax, ay);\n\n\n\n\n\n\n\n\nAnd that’s a proper mess, but the arrows seem to be oriented the right way (towards the system’s center of mass) and you get a bunch of very long arrows, signifying high forces at short distances - another issue that I might well come back to in another post!\nAnd the nice thing is that our function works just as well on the GPU!\n\nax, ay, az = cupy.asnumpy(accelerations(cupy.asarray(r), cupy.asarray(m)).T)\n\nplt.scatter(x, y, m);\nplt.quiver(x, y, ax, ay);\n\n\n\n\n\n\n\n\nLet’s now run a quick bechmark:\n\nresults = []\nnumbers_of_bodies = [2**n for n in range(4, 13)]\nfor np in [numpy, cupy]:\n    for N in numbers_of_bodies:\n        r, m = prep(N, np, seed=17)\n        time = %timeit -oq accelerations(r, m)\n        results.append({\"library\":np.__name__,\n                        \"N\": N,\n                        \"average\": time.average,\n                        \"stdev\": time.stdev})\n\n\nimport pandas\n\ndf = pandas.DataFrame(results)\ndf\n\n\n\n\n\n\n\n\nN\naverage\nlibrary\nstdev\n\n\n\n\n0\n16\n0.000076\nnumpy\n0.000003\n\n\n1\n32\n0.000179\nnumpy\n0.000002\n\n\n2\n64\n0.000589\nnumpy\n0.000020\n\n\n3\n128\n0.002245\nnumpy\n0.000139\n\n\n4\n256\n0.012159\nnumpy\n0.003883\n\n\n5\n512\n0.044007\nnumpy\n0.004827\n\n\n6\n1024\n0.187316\nnumpy\n0.007669\n\n\n7\n2048\n0.718909\nnumpy\n0.027526\n\n\n8\n4096\n2.867943\nnumpy\n0.055799\n\n\n9\n16\n0.001288\ncupy\n0.000021\n\n\n10\n32\n0.001316\ncupy\n0.000030\n\n\n11\n64\n0.001463\ncupy\n0.000146\n\n\n12\n128\n0.001430\ncupy\n0.000112\n\n\n13\n256\n0.001321\ncupy\n0.000024\n\n\n14\n512\n0.001656\ncupy\n0.000022\n\n\n15\n1024\n0.005480\ncupy\n0.000082\n\n\n16\n2048\n0.020252\ncupy\n0.000017\n\n\n17\n4096\n0.080994\ncupy\n0.000289\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.set_ylabel(\"Average runtime [s]\")\nfor label, g in df.groupby('library'):\n    g.plot('N', 'average', ax=ax, label=label, logx=True, logy=True, style=\"o--\")\nax.grid()\n\n\n\n\n\n\n\n\nThus at low numbers of particles, CuPy has a performance overhead, but at larger numbers of particles (as limited by MemoryErrors on my device, with the regime shifting around 100 particles), the GPU (predictably) wins!\nWe can also calculate the runtime ratios for a speedup estimate:\n\nspeedups = df[df.library =='numpy'].set_index('N').average / df[df.library =='cupy'].set_index('N').average\nspeedups\n\nN\n16       0.059079\n32       0.135752\n64       0.402321\n128      1.570570\n256      9.204320\n512     26.570801\n1024    34.179128\n2048    35.498837\n4096    35.409139\nName: average, dtype: float64\n\n\n\nspeedups.plot(logx=True, style=\"o--\", logy=True)\nplt.ylabel(\"GPU over CPU speedup\");\nplt.grid()\n\n\n\n\n\n\n\n\nWhile this is obviously not a full proper test (we don’t know how much host-device memory transfer would impact our timings, etc), it’s at least nice to see that we get 35 times the speed on the GPU for the pure acceleration stage basically for free!"
  },
  {
    "objectID": "posts/sympy-for-physics-homework-in-2020.html",
    "href": "posts/sympy-for-physics-homework-in-2020.html",
    "title": "SymPy for physics homework in 2020",
    "section": "",
    "text": "I’ve been doing my university physics homework in SymPy and Jupyter for a little bit, and I thought I could share a little about the workflow I’ve developed for it. It might come useful for you.\nLet’s go through with an abridged example of plasma physics homework with that toolset, and then go over the tricks used as they come up. I’ll put text that I’d put in the homework solution in quotes.\nWe’ll set up our SymPy namespace first. We’ll dump it all into the main namespace; not the best habit for libraries, but here we want to write less code, so it’ll do.\n%config InteractiveShell.ast_node_interactivity = 'last_expr_or_assign'\nfrom sympy import *\ninit_printing()\nI also added a little IPython magic in there. This is a little behavior I picked up from working in Julia, where the value last expression or assignment in a cell is displayed.\nNow, let’s define our variables and the set of equations we care about. Note how we’ll liberally use unicode characters to shorten our code. In Jupyter, you can enter \\(\\omega\\) in code cells by simply writing the LaTeX for it and hitting TAB afterwards: \\omega&lt;TAB&gt;\nm_i, m_e, γ, T, n_0, e, ϵ = symbols('m_i m_e gamma T n_0 e varepsilon_0', positive=True)\nω, k = symbols('omega k', positive=True)\nv_i, v_e, n_i, n_e = symbols('v_i v_e n_i n_e')  # to be understood as fourier modes\n\nequations = [\n    Eq(-I * ω * m_i * n_0 * v_i, +e * n_0 * E - I * k * γ * T * n_i),\n    Eq(-I * ω * m_e * n_0 * v_e, -e * n_0 * E - I * k * γ * T * n_e),\n    Eq(-I * ω * n_i + I * k * n_0 * v_i, 0),\n    Eq(-I * ω * n_e + I * k * n_0 * v_e, 0),\n    Eq(I * k * E , e * (n_i - n_e) / ϵ)\n]\n\n\\(\\displaystyle \\left[ - i m_{i} n_{0} \\omega v_{i} = - i T \\gamma k n_{i} + e e n_{0}, \\  - i m_{e} n_{0} \\omega v_{e} = - i T \\gamma k n_{e} - e e n_{0}, \\  i k n_{0} v_{i} - i n_{i} \\omega = 0, \\  i k n_{0} v_{e} - i n_{e} \\omega = 0, \\  e i k = \\frac{e \\left(- n_{e} + n_{i}\\right)}{\\varepsilon_{0}}\\right]\\)\nSee? That’s output right there. Neat! Anyway, &gt; We first solve equations 3 and 4 for the velocities:\nv_i_e_solutions = solve(equations[2:4], [v_i, v_e])\n\n\\(\\displaystyle \\left\\{ v_{e} : \\frac{n_{e} \\omega}{k n_{0}}, \\  v_{i} : \\frac{n_{i} \\omega}{k n_{0}}\\right\\}\\)\nmotion_equations = [eq.subs(v_i_e_solutions) for eq in equations[0:2]]\n\n\\(\\displaystyle \\left[ - \\frac{i m_{i} n_{i} \\omega^{2}}{k} = - i T \\gamma k n_{i} + e e n_{0}, \\  - \\frac{i m_{e} n_{e} \\omega^{2}}{k} = - i T \\gamma k n_{e} - e e n_{0}\\right]\\)\nn_i_e_solutions = solve(motion_equations, (n_i, n_e))\n\n\\(\\displaystyle \\left\\{ n_{e} : \\frac{e i e k n_{0}}{T \\gamma k^{2} - m_{e} \\omega^{2}}, \\  n_{i} : - \\frac{e i e k n_{0}}{T \\gamma k^{2} - m_{i} \\omega^{2}}\\right\\}\\)\npoisson_eq = equations[-1].subs(n_i_e_solutions).simplify()\n\n\\(\\displaystyle e i k = \\frac{e i e^{2} k n_{0} \\left(- 2 T \\gamma k^{2} + m_{e} \\omega^{2} + m_{i} \\omega^{2}\\right)}{\\varepsilon_{0} \\left(T \\gamma k^{2} - m_{e} \\omega^{2}\\right) \\left(T \\gamma k^{2} - m_{i} \\omega^{2}\\right)}\\)\nω2_solutions = solve(poisson_eq, ω**2)\n\n\\(\\displaystyle \\left[ \\frac{\\left(m_{e} + m_{i}\\right) \\left(T \\gamma k^{2} \\varepsilon_{0} + e^{2} n_{0}\\right) - \\sqrt{T^{2} \\gamma^{2} k^{4} m_{e}^{2} \\varepsilon_{0}^{2} - 2 T^{2} \\gamma^{2} k^{4} m_{e} m_{i} \\varepsilon_{0}^{2} + T^{2} \\gamma^{2} k^{4} m_{i}^{2} \\varepsilon_{0}^{2} + 2 T e^{2} \\gamma k^{2} m_{e}^{2} n_{0} \\varepsilon_{0} - 4 T e^{2} \\gamma k^{2} m_{e} m_{i} n_{0} \\varepsilon_{0} + 2 T e^{2} \\gamma k^{2} m_{i}^{2} n_{0} \\varepsilon_{0} + e^{4} m_{e}^{2} n_{0}^{2} + 2 e^{4} m_{e} m_{i} n_{0}^{2} + e^{4} m_{i}^{2} n_{0}^{2}}}{2 m_{e} m_{i} \\varepsilon_{0}}, \\  \\frac{\\left(m_{e} + m_{i}\\right) \\left(T \\gamma k^{2} \\varepsilon_{0} + e^{2} n_{0}\\right) + \\sqrt{T^{2} \\gamma^{2} k^{4} m_{e}^{2} \\varepsilon_{0}^{2} - 2 T^{2} \\gamma^{2} k^{4} m_{e} m_{i} \\varepsilon_{0}^{2} + T^{2} \\gamma^{2} k^{4} m_{i}^{2} \\varepsilon_{0}^{2} + 2 T e^{2} \\gamma k^{2} m_{e}^{2} n_{0} \\varepsilon_{0} - 4 T e^{2} \\gamma k^{2} m_{e} m_{i} n_{0} \\varepsilon_{0} + 2 T e^{2} \\gamma k^{2} m_{i}^{2} n_{0} \\varepsilon_{0} + e^{4} m_{e}^{2} n_{0}^{2} + 2 e^{4} m_{e} m_{i} n_{0}^{2} + e^{4} m_{i}^{2} n_{0}^{2}}}{2 m_{e} m_{i} \\varepsilon_{0}}\\right]\\)\nlimit1 = limit(ω2_solutions[0], m_i, oo)\n\n\\(\\displaystyle \\frac{T \\gamma k^{2} \\varepsilon_{0} + e^{2} n_{0} - \\sqrt{T^{2} \\gamma^{2} k^{4} \\varepsilon_{0}^{2} + 2 T e^{2} \\gamma k^{2} n_{0} \\varepsilon_{0} + e^{4} n_{0}^{2}}}{2 m_{e} \\varepsilon_{0}}\\)\nThis is, unfortunately, a problem that SymPy tends to run into - this kind of issue needs some tinkering sometimes:\nnumerator, denominator = limit1.as_numer_denom()\nsum(arg.factor() for arg in numerator.args) / denominator\n\n\\(\\displaystyle 0\\)\nlimit2 = limit(ω2_solutions[1], m_i, oo).simplify()\nnumerator, denominator = limit2.as_numer_denom()\nlimit2_corrected = (sum(arg.factor() for arg in numerator.args) / denominator).simplify()\n\n\\(\\displaystyle \\frac{T \\gamma k^{2} \\varepsilon_{0} + e^{2} n_{0}}{m_{e} \\varepsilon_{0}}\\)\nMratio = symbols(\"M\", positive=True)\nseries_expansion = (ω2_solutions[1]).subs(m_i, m_e / Mratio).expand().series(Mratio, n = 2);\nNote how I’m using ; at the end to stop this from displaying.\nThis took a bunch of StackOverflow searching, which, if you’re on a deadline, may admittedly not be the most productive thing to do:\nsimplified_series_expansion = collect(series_expansion.removeO(), Mratio, simplify);\n\n\\(\\displaystyle \\frac{M \\left(T^{3} \\gamma^{3} k^{6} \\varepsilon_{0}^{3} + 3 T^{2} e^{2} \\gamma^{2} k^{4} n_{0} \\varepsilon_{0}^{2} - T^{2} \\gamma^{2} k^{4} \\varepsilon_{0}^{2} \\sqrt{T^{2} \\gamma^{2} k^{4} \\varepsilon_{0}^{2} + 2 T e^{2} \\gamma k^{2} n_{0} \\varepsilon_{0} + e^{4} n_{0}^{2}} + 3 T e^{4} \\gamma k^{2} n_{0}^{2} \\varepsilon_{0} - 2 T e^{2} \\gamma k^{2} n_{0} \\varepsilon_{0} \\sqrt{T^{2} \\gamma^{2} k^{4} \\varepsilon_{0}^{2} + 2 T e^{2} \\gamma k^{2} n_{0} \\varepsilon_{0} + e^{4} n_{0}^{2}} + e^{6} n_{0}^{3} + e^{4} n_{0}^{2} \\sqrt{T^{2} \\gamma^{2} k^{4} \\varepsilon_{0}^{2} + 2 T e^{2} \\gamma k^{2} n_{0} \\varepsilon_{0} + e^{4} n_{0}^{2}}\\right)}{2 m_{e} \\varepsilon_{0} \\left(T^{2} \\gamma^{2} k^{4} \\varepsilon_{0}^{2} + 2 T e^{2} \\gamma k^{2} n_{0} \\varepsilon_{0} + e^{4} n_{0}^{2}\\right)} + \\frac{T \\gamma k^{2} \\varepsilon_{0} + e^{2} n_{0} + \\sqrt{T^{2} \\gamma^{2} k^{4} \\varepsilon_{0}^{2} + 2 T e^{2} \\gamma k^{2} n_{0} \\varepsilon_{0} + e^{4} n_{0}^{2}}}{2 m_{e} \\varepsilon_{0}}\\)\nsquareroot = numer(simplified_series_expansion.args[0]).args[0]\nvery_simplified_series_expansion = simplified_series_expansion.subs(squareroot, squareroot.factor()).simplify()\n\n\\(\\displaystyle \\frac{M e^{4} n_{0}^{2} + T^{2} \\gamma^{2} k^{4} \\varepsilon_{0}^{2} + 2 T e^{2} \\gamma k^{2} n_{0} \\varepsilon_{0} + e^{4} n_{0}^{2}}{m_{e} \\varepsilon_{0} \\left(T \\gamma k^{2} \\varepsilon_{0} + e^{2} n_{0}\\right)}\\)\nion_correction = (very_simplified_series_expansion - limit2_corrected).simplify().subs(Mratio, m_e / m_i)\n\n\\(\\displaystyle \\frac{e^{4} n_{0}^{2}}{m_{i} \\varepsilon_{0} \\left(T \\gamma k^{2} \\varepsilon_{0} + e^{2} n_{0}\\right)}\\)\nAnd that’s that!"
  },
  {
    "objectID": "posts/sympy-for-physics-homework-in-2020.html#positives",
    "href": "posts/sympy-for-physics-homework-in-2020.html#positives",
    "title": "SymPy for physics homework in 2020",
    "section": "Positives",
    "text": "Positives\n\nYou get to skip out on algebraic errors.\nYou get beautiful LaTeX display, at no effort, for each step. This helps a ton if your handwriting is dreadful, like mine is.\nYou can resort to first-order corrections etc if you need a check, but you actually get full analytical solutions a lot of the time."
  },
  {
    "objectID": "posts/sympy-for-physics-homework-in-2020.html#neutrals",
    "href": "posts/sympy-for-physics-homework-in-2020.html#neutrals",
    "title": "SymPy for physics homework in 2020",
    "section": "Neutrals",
    "text": "Neutrals\n\nYou really need to watch your assumptions when you define your variables, as SymPy is pretty conservative with its simplifications. This is, of course, both a blessing and a curse.\nThe learning curve is a bit steep, but you can accomplish plenty with just a little knowledge; for the rest, you can google. I’ve been putting off picking up its physics subpackage for way too long now."
  },
  {
    "objectID": "posts/sympy-for-physics-homework-in-2020.html#negatives",
    "href": "posts/sympy-for-physics-homework-in-2020.html#negatives",
    "title": "SymPy for physics homework in 2020",
    "section": "Negatives",
    "text": "Negatives\n\nThere are kinks, as you undoubtedly saw; it can be tough to tell SymPy “look at that square root! Look at it!”. It’ll miss some seemingly obvious stuff, and it can take a fair bit of googling to get it to work properly.\nRuntimes can get steep-ish; it’s not really suited for live work. I’ll %timeit the two lines that caused a little trouble here. Admittedly, symbolic math is hard, so I’m not too surprised. Turns out I could only timeit the first line in reasonable time…\n\n\n%timeit series_expansion = (ω2_solutions[1]).subs(m_i, m_e / Mratio).expand().series(Mratio, n = 2);\n\n24.3 s ± 351 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nStill, all in all, I think it has plenty of potential. If it’s not there yet, it’s getting there! I’m likely to keep using it and if I come up with any further handy tools, I’ll get back here."
  },
  {
    "objectID": "posts/ifpilm-postmortem.html",
    "href": "posts/ifpilm-postmortem.html",
    "title": "A pandemic story, or, what I learned working with nuclear fusion",
    "section": "",
    "text": "As promised, here’s a post on the story of my time at IPPLM - the Institute of Plasma Physics and Laser Microfusion, where I worked for the last almost-three-years. Past tense, because since September I’ve successfully found remote work as a Python software developer at a large pharmaceutical company. Speaking of… I don’t think I can say as much as I’d hoped about the projects we’re working on, but, suffice to say, I think they have some real potential for helping people with neurodegenerative diseases. So I’m still trying to tackle one of the Large Problems, just… another one, and more so from a backend/support angle.\nWhile writing this, I realized that this story was also deeply intertwined with the story of the global COVID-19 pandemic, because that turned the whole situation on its head. I thought about separating the two, but decided instead to embrace it and tell it whole rather than in pieces. So, beware, this is going to get long. To alleviate that, I’ve broken it up more than usual and have placed the major takeaways in specifically formatted sections throughout.\nWith that disclaimer in place… onto the story!"
  },
  {
    "objectID": "posts/ifpilm-postmortem.html#the-road-to-ipplm",
    "href": "posts/ifpilm-postmortem.html#the-road-to-ipplm",
    "title": "A pandemic story, or, what I learned working with nuclear fusion",
    "section": "The road to IPPLM",
    "text": "The road to IPPLM\nI still had #3, though - reaching out to people locally who were already involved with fusion - and that’s how I found IPPLM. I initially signed up there for an apprenticeship, which was spent mostly helping out a senior researcher with debugging and visualization for a simulation he was running for a plasma focus system. That was neat and I learned a lot during that (I also wrote some code that I still shudder to think about!), but I didn’t get as much hands on experience with actual nuclear fusion, the cool machines and experiments, the twisted geometries of tokamaks and stellarators.\nDuring that time, I learned about the particle-in-cell method and figured I wanted to try doing something in that area. My supervisor put me in contact with professor Jabłoński, who’s absolutely one of the literal Best People on the planet and can also play a mean acoustic guitar. He guided me through the process of writing a particle in cell code in Python. I’ve told that story before. Still… it was a little… far from actual nuclear fusion, the cool machines and experiments, etc etc, you know the drill.\nSo in late 2019, having spent nine months with natural language translation at Samsung, having decided I’d go back to doing science because natural language translation seemed to be getting the help it needed and fusion less so, I went back to IPPLM and started working there, first part-time, then full time. For the first time I’d have hands on experience with nuclear fusion, etc etc!\nI spent maybe four months there before COVID-19 exploded in Asia and then globally."
  },
  {
    "objectID": "posts/ifpilm-postmortem.html#working-from-home",
    "href": "posts/ifpilm-postmortem.html#working-from-home",
    "title": "A pandemic story, or, what I learned working with nuclear fusion",
    "section": "Working from home",
    "text": "Working from home\nBecause it turns out that hey, as a massive introvert I actually do some of my best work on my own terms. Locked up tight in a tiny room with no interruptions but the work in front of me? Sign me up! Getting up before the sun and racing it to knock down one more bug before dawn? Hell yes! Writing code without trousers? Well let me tell you, I’ve written some of my worst code with trousers on and I do not think that a mere coincidence!1\nProfessionally, this was a weird time because I was assisting with one computational project and investigating my first master’s thesis attempt. I was lucky to be able to do the thesis as part of work, but it didn’t happen to pan out. The first attempt - because we basically had a massive miscommunication and didn’t reach out to the folks in Germany early enough. We vastly underestimated the difficulty of developing a Bayesian model for Thomson scattering and didn’t have enough experience with the subject to de-scope it into a “minimum thesis-able product”.\nThe latter was a re-implementation of an existing code and, well, let’s just say it was sufficiently unexciting and un-useful that I had trouble motivating myself to finish it. I had more important stuff to do! Through that time, I also worked on parsing LHD spectroscopic data and identifying spectral lines in it; backward inference of PHA data using IPPLM’s forward model for impurities; and essentially a ray-tracing parallelized algorithm for the CO-Monitor diagnostic, which I hear is coming online soon. I really enjoyed all of these.\nAll that taught me two things:\n\nTakeaway number two: it’s awesome to get results, but the total impact of the results gotten by all the people you help - even by simply writing tools - can often be far greater.2\n\nOf course, this may be a purely personal thing - some folks just prefer writing tools! In fact, while my official job title at IPPLM was “Pracownik badawczo-techniczny” (Technical/research worker/specialist, I guess?), there’s an increasingly popular term “Research Software Engineer”, or RSE, that describes the sort of job I did. In fact I’ll leave you with the RSE community’s website and jump straight to the next lesson learned:\n\nTakeaway number three: Research Software Engineers, or people straddling the boundary of science and software development, are extremely important. Scientists alone, without practical experience in software dev, have an incredibly rough road to good results in this digital age. But you can’t just point software devs at a tokamak and expect to get working fusion out of it, either. If you are proficient in both areas, even enough to know who to ask in each, this may be a good path you can take.\n\nOf course I’m not saying it’s impossible for “pure” scientists, physicists in my case, to get results - but the code they write is often obscure, inefficient and non-generalizable, so it takes a lot of time to rewrite it from one experiment to another. I’m not saying my code’s all that much better, far from it - I’m just pointing out a problem in the system.\nAll in all, though, the first two waves of the pandemic were actually a good period for me and my then-fiancee. Of course, we dodged a bunch of bullets - heavy infections, losses of people close to us…"
  },
  {
    "objectID": "posts/ifpilm-postmortem.html#footnotes",
    "href": "posts/ifpilm-postmortem.html#footnotes",
    "title": "A pandemic story, or, what I learned working with nuclear fusion",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI can neither confirm nor deny trousers were worn while writing this post. Honestly, it’s been in draft for a while so I can’t actually remember. I’d estimate like 40% trouserless, which may be why it’s gotten so lengthy.↩︎\nDisclaimer: whether the scientific research system we have now is set up to promote and reward writing tools is arguable. In fact I’ll argue for an emphatic no and just point you towards the announcement of JOSS for more on that.↩︎"
  },
  {
    "objectID": "posts/fusion1.html",
    "href": "posts/fusion1.html",
    "title": "Fusion 1/4: Nuclear power as of 2021",
    "section": "",
    "text": "Hi! In the new year, as promised, you’ll see more actual science discussions on this page. I have - I think, for the first time since I started writing here - a plan for the next four posts. The first will discuss nuclear power and its place in the world. The second will tell you a little about its most interesting future alternative - fusion power. Next up, we’ll discuss its history a little and where I predict it’ll head in the near future; and in the fourth, I’ll try to discuss a little about what we’re actually doing with it in terms of research work. I can’t wait to get to the fourth part, so that should be at least motivating. Let’s get to it!\nI shall not bore you with the details of nuclear fission’s use for power generations. Many have written on that before, and I don’t think I can add much to that. Basically, though, and in great simplification - take the following diagram of, essentially, how much energy, per atomic nucleus, you need to break up an atom:\nnotice that the most stable nuclei are at the “Fe peak” (which, coincidentally, you could also call the “iron hill”, and which, also coincidentally, is where most matter in the universe might reach steady state, and in other words, where time would seem to stand still, which sure puts a new, er, spin on that song). Once you’ve noticed that, you realize that if you can find some way to split heavy elements into light elements, you would probably release that energy (going by the plot alone, about 1MeV from U238 to stable Fe56) and could harness it. Hit some particularly heavy nuclei with enough neutrons…\nand you’ve got nuclear power, in a nutshell! Simple, right? Simple to the point of throwing out the actual science. I completely neglected a lot of factors: the fact that you want the neutrons to hit the nuclei slowly enough not to cause a runaway chain reaction chief among them. But this is meant to be a simple view, and that neglect is a tragedy we’ll have to live with. Anyway! We all know the pros and cons for nuclear, but, to recap:\nThat last point is, actually, usually blown out of proportion. Something I’ve learned about recently is that all nuclear waste, since the origin of nuclear power until today, stored comfortably, safely and up to proper regulations, would fill a football field. Does that sound like a lot? Does it, still, when I tell you that our Polish Bełchatów coal power plant’s coal ash storage area takes up 416 ha (which translates to ~590 football fields), and that was already 60% filled up by the 2000?\nYeah, didn’t think so."
  },
  {
    "objectID": "posts/fusion1.html#poland",
    "href": "posts/fusion1.html#poland",
    "title": "Fusion 1/4: Nuclear power as of 2021",
    "section": "Poland",
    "text": "Poland\n\n\n\nPoland, electricitymap.org\n\n\nWell… badly, as usual. We’re taking some baby steps in transitioning away from coal as our main power source, and our government hasn’t really been all that helpful with promoting the growth of solar or inland wind power. There is talk of adding 6 GW of nuclear power in the next decade(s), but not much to brag about right now.\nThis is, basically, the darkest scenario of “fossil fuels for base load, little renewable energy”."
  },
  {
    "objectID": "posts/fusion1.html#germany",
    "href": "posts/fusion1.html#germany",
    "title": "Fusion 1/4: Nuclear power as of 2021",
    "section": "Germany",
    "text": "Germany\nLet’s go to our western neighbor, Germany. Germany, under the leadership of CDU with major Green influences, has been a big proponent of renewable energy. That’s certainly a fine idea, but let’s see how it’s going for them:\n\n\n\nGermany, electricitymap.org\n\n\nWell, they sure have a lot of capacity (maximum power they could, given optimal conditions, generate) for solar and wind! It’s a truly awesome (as in, I’m awed right now) set up - they have more wind power capacity than the entire power capacity of Poland, from what I can see.\nBut as you can see, it’s a cloudy winter day and it’s not all that windy. So, all that capacity gives them squat Thus, coal and gas usage both spike up, which is less than ideal when you’re trying to limit the amount of CO2 that goes up into the atmosphere.\nNote that Germany did have a sizable amount of nuclear power generated, but, under the same CDU/Green leadership, closed them down. I don’t want to go into debate on the arguments made by both sides, whether those were made in good faith or not, whether they took all relevant factors into account or not; but, purely from a “let’s stop climate change” perspective, doesn’t look like it’s been a good call for them.\nIn contrast, let’s go further west:"
  },
  {
    "objectID": "posts/fusion1.html#france",
    "href": "posts/fusion1.html#france",
    "title": "Fusion 1/4: Nuclear power as of 2021",
    "section": "France",
    "text": "France\n\n\n\nFrance, electricitymap.org\n\n\nAnd this is the best example I know of what happens, in two similar countries, if one decides to close their nuclear and one does not. Notice how they still burn some natural gas. “Natural gas” is a nice name for what is still a fossil fuel, still emits CO2, less so than coal, but still - but even with that, you emit a way less CO2 than Germany."
  },
  {
    "objectID": "posts/first-dive-julia.html",
    "href": "posts/first-dive-julia.html",
    "title": "First dive into Julia",
    "section": "",
    "text": "I’m currently sitting in at a tutorial on the basics of Julia and parallel processing therein by Bogumił Kamiński. This is actually my first dive into Julia, so I thought I would write it down on-the-go here!\n\nLet’s jump right into it:\n\nR = randn(10000, 10000)\n\n10000×10000 Array{Float64,2}:\n -0.324424   -0.565317    0.74943    …  -0.518865    -0.0841272   0.935276 \n  2.7421      0.127783   -0.406756       1.69075      1.58605     0.302112 \n  2.00937    -0.36474    -1.6031         2.46846     -0.319774   -0.362626 \n  1.0957     -0.328512    0.0765665      0.551588    -0.63376    -0.642072 \n -1.5761      0.0990041   0.649661       0.123745     1.53702     0.748066 \n  0.0294794   0.841421    0.935812   …  -0.124979    -0.0319694  -0.308331 \n  2.4428     -0.0981946   2.16323       -1.74004     -0.838027   -0.562755 \n -0.362584   -0.342403    1.11269       -1.99102      2.13044     1.05996  \n -0.85741     0.224304    0.89256       -0.357627    -0.25959     0.271416 \n  1.02282    -0.470008    1.75296        1.34871     -0.16343     0.194525 \n -0.357741    0.252059   -1.02996    …  -0.125655    -1.20237     0.0220102\n  0.793983    0.334861   -0.628246      -0.768169     1.08063    -0.870663 \n -0.111529   -0.557087    0.714131      -0.0785655    0.577348   -0.659775 \n  ⋮                                  ⋱                                     \n  0.454754    0.905449   -1.04019       -2.14169     -0.830821    0.363394 \n  0.165472   -0.099097    1.58675       -0.314269    -0.500922   -2.24592  \n -0.74685     0.854795   -0.606661   …   0.390252    -1.45657     1.22648  \n -0.0369208  -0.139647    1.26695       -0.00442996  -2.24374     0.348733 \n  0.620604   -0.835141   -1.59741       -0.026424    -0.491713    0.705191 \n -2.49094     0.471711    0.677353       0.51443     -0.234433    1.61501  \n -0.600199    0.907787   -0.0977633      1.39034     -1.20908     1.06054  \n -1.26894     0.718772    0.334036   …   0.994015    -1.28285    -2.15419  \n -0.411544   -0.0794345   1.58904        1.14895      0.0363      2.14895  \n -0.437881   -0.451166   -0.0647529     -0.276704     0.392206   -0.128466 \n  0.86126     0.774654    0.429458      -2.03196     -0.371577    0.547281 \n -1.07246     0.421693   -0.244775      -0.479385    -0.858759   -0.300843 \n\n\nOkay, that did what you’d expect. There’s apparently a help statement that works inversely from what you’d expect from IPython:\n\n?randn\n\nsearch: randn rand transcode macroexpand @macroexpand1 @macroexpand\n\n\n\nrandn([rng=GLOBAL_RNG], [T=Float64], [dims...])\nGenerate a normally-distributed random number of type T with mean 0 and standard deviation 1. Optionally generate an array of normally-distributed random numbers. The Base module currently provides an implementation for the types Float16, Float32, and Float64 (the default), and their Complex counterparts. When the type argument is complex, the values are drawn from the circularly symmetric complex normal distribution.\n\nExamples\njulia&gt; rng = MersenneTwister(1234);\n\njulia&gt; randn(rng, ComplexF64)\n0.6133070881429037 - 0.6376291670853887im\n\njulia&gt; randn(rng, ComplexF32, (2, 3))\n2×3 Array{Complex{Float32},2}:\n -0.349649-0.638457im  0.376756-0.192146im  -0.396334-0.0136413im\n  0.611224+1.56403im   0.355204-0.365563im  0.0905552+1.31012im\n\n\n\nWhat about global help?\n\n?\n\nsearch: ⊻ ⊋ ⊊ ⊉ ⊈ ⊇ ⊆ ≥ ≤ ≢ ≡ ≠ ≉ ≈ ∪ ∩ ∛ √ ∘ ∌ ∋ ∉ ∈ ℯ π ÷ ~ | ^ \\ &gt; &lt; : / - +\n\n\n\nWelcome to Julia 1.1.0. The full manual is available at\nhttps://docs.julialang.org/\nas well as many great tutorials and learning resources:\nhttps://julialang.org/learning/\nFor help on a specific function or macro, type ? followed by its name, e.g. ?cos, or ?@time, and press enter. Type ; to enter shell mode, ] to enter package mode.\n\n\n\n;ls\n\nmnist.zip\nTutorial1.ipynb\n\n\nWe can use @time to benchmark this randn command:\n\n@time randn(10000, 10000);\n\n  0.813875 seconds (6 allocations: 762.940 MiB, 1.56% gc time)\n\n\nFor comparison:\nIn [3]: %timeit np.random.normal(size=(10000, 10000))             \n3.8 s ± 25.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nAnd the presenter’s R attempt took 5.81 using system.time. Wew, this is pretty fast.\nTo start with, we’re analyzing a function found on StackOverflow that sums over the lower triangular part of a matrix (apparently, the code is pretty bad):\n\nfunction upsum(M); n = size(M)[1]; sum = 0\n    for i = 1:n-1\n        for j = i+1:n\n            sum = sum + M[i,j]\n        end\n    end\n    return sum\nend\n\nupsum(R)\n\n-7802.649783031088\n\n\nLet’s check the performance:\n\n%timeit upsum(R);\n\nThe analogue of IPython’s %time statement (also %timeit) in Julia is @time statement. The analogue of %%time ...code... is\n@time begin\n    ...code...\nend\nNote, however, that you should put all performance-critical code into a function, avoiding global variables, before doing performance measurements in Julia; see the performance tips in the Julia manual.\nThe @time macro prints the timing results, and returns the value of evaluating the expression. To instead return the time (in seconds), use @elapsed statement.\nFor more extensive benchmarking tools, including the ability to collect statistics from multiple runs, see the BenchmarkTools package.\n\n\n… All right, I’m starting to like this cheeky little language. Trying again:\n\n@time upsum(R);\n\n  0.464638 seconds (5 allocations: 176 bytes)\n\n\nTo compare that with Python:\nIn [8]: %timeit np.sum(np.tril(R))                                \n245 ms ± 45.8 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nWell, that was faster, but we can improve the Julia code. Let’s first look at the inbuilt sum function:\n\nsum\n\nsum (generic function with 13 methods)\n\n\n\n@time sum(R);\n\n  0.090729 seconds (89.03 k allocations: 4.748 MiB, 10.32% gc time)\n\n\n\n@time sum(R);\n\n  0.100551 seconds (5 allocations: 176 bytes)\n\n\nOkay, now this is badass. Julia is dynamically compiled - it’s as if Numba came out of Python and became its own language. Apparently there are ways of avoiding the first-call overhead, but this is somehow more advanced.\nNote that all compiled-function cache is cleared on Julia restarts!\nTo compare with Python:\nIn [9]: %timeit np.sum(R)                                         \n53 ms ± 3.67 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\nNot too shabby for the ol’ snake!\nLet’s try to improve the function, though:\n\nfunction uppersum(M)\n    n = size(M, 1)\n    s = zero(eltype(M))  # a zero hard-typed as the same type as the entry of the matrix\n    # eltype stands for ELement TYPE - this is now fully generic\n    for i in 2:n        # Julia uses column-major storage order - faster to traverse any matrix in Julia column-wise\n        @simd for j in 1:(i-1)    # if I know I'm accessing a contiguous block of memory, I can tell Julia that using @simd\n            @inbounds s += M[j, i]    # ignore bound checking and just access memory C-style \n        end\n    end\n    s\nend\n\nuppersum(R)\n\n-7802.6497830305125\n\n\nWe can look at these @simd and @inbouds annotations:\n\n?@simd\n\n@simd\nAnnotate a for loop to allow the compiler to take extra liberties to allow loop re-ordering\n!!! warning This feature is experimental and could change or disappear in future versions of Julia. Incorrect use of the @simd macro may cause unexpected results.\nThe object iterated over in a @simd for loop should be a one-dimensional range. By using @simd, you are asserting several properties of the loop:\n\nIt is safe to execute iterations in arbitrary or overlapping order, with special consideration for reduction variables.\nFloating-point operations on reduction variables can be reordered, possibly causing different results than without @simd.\n\nIn many cases, Julia is able to automatically vectorize inner for loops without the use of @simd. Using @simd gives the compiler a little extra leeway to make it possible in more situations. In either case, your inner loop should have the following properties to allow vectorization:\n\nThe loop must be an innermost loop\nThe loop body must be straight-line code. Therefore, @inbounds is currently needed for all array accesses. The compiler can sometimes turn short &&, ||, and ?: expressions into straight-line code if it is safe to evaluate all operands unconditionally. Consider using the ifelse function instead of ?: in the loop if it is safe to do so.\nAccesses must have a stride pattern and cannot be “gathers” (random-index reads) or “scatters” (random-index writes).\nThe stride should be unit stride.\n\n!!! note The @simd does not assert by default that the loop is completely free of loop-carried memory dependencies, which is an assumption that can easily be violated in generic code. If you are writing non-generic code, you can use @simd ivdep for ... end to also assert that:\n\nThere exists no loop-carried memory dependencies\nNo iteration ever waits on a previous iteration to make forward progress.\n\n\n\n\n?@inbounds\n\n@inbounds(blk)\nEliminates array bounds checking within expressions.\nIn the example below the in-range check for referencing element i of array A is skipped to improve performance.\nfunction sum(A::AbstractArray)\n    r = zero(eltype(A))\n    for i = 1:length(A)\n        @inbounds r += A[i]\n    end\n    return r\nend\n!!! warning Using @inbounds may return incorrect results/crashes/corruption for out-of-bounds indices. The user is responsible for checking it manually. Only use @inbounds when it is certain from the information locally available that all accesses are in bounds.\n\n\nRight! Let’s time this implementation:\n\n@time uppersum(R);\n\n  0.054047 seconds (5 allocations: 176 bytes)\n\n\n\nfunction uppersum_boundcheck(M)\n    n = size(M, 1)\n    s = zero(eltype(M))  # a zero hard-typed as the same type as the entry of the matrix\n    # eltype stands for ELement TYPE - this is now fully generic\n    for i in 2:n        # Julia uses column-major storage order - faster to traverse any matrix in Julia column-wise\n        @simd for j in 1:(i-1)    # if I know I'm accessing a contiguous block of memory, I can tell Julia that using @simd\n            s += M[j, i]    # ignore bound checking and just access memory C-style \n        end\n    end\n    s    # this is sufficient for a `return` statement\nend\n\nuppersum(R)\n\n-7802.6497830305125\n\n\nLet’s see what kind of gain we get from losing boundchecking:\n\n@time uppersum_boundcheck(R);\n\n  0.115098 seconds (51.16 k allocations: 2.663 MiB)\n\n\nInteresingly, Julia apparently uses LLVM in the background.\n\nGoing parallel\nThe idea is: we have a triangle and we want to split it into pieces of equal “mass”. This is done in the code below, via an instruction that is relatively magical to me right now.\nFor threading to work, note that as described in the docs, you need to set the environment variable JULIA_NUM_THREADS. export JULIA_NUM_THREADS=4 in .bashrc worked fine for me.\n\nusing Base.Threads\n\nfunction upsum_threads(M)\n    n = size(M, 1)\n    chunks = nthreads()\n    sums = zeros(eltype(M), chunks)  # separate subsum for each thread\n    chunkend = round.(Int, n*sqrt.((1:chunks) ./ chunks))   #split jobs so that each thread has approx. same number of numbers to add\n    @assert minimum(diff(chunkend)) &gt; 0\n    chunkstart = [2; chunkend[1:end-1] .+ 1]\n    @threads for job in 1:chunks     # tell Julia that this part is safe for threading\n        s = zero(eltype(M))\n        for i in chunkstart[job]:chunkend[job]\n            @simd for j in 1:(i-1)\n                @inbounds s += M[j, i]\n            end\n        end\n        sums[job] = s\n    end\n    return sum(sums)\nend\n\nupsum_threads(R)\n\n-7802.649783030595\n\n\n\n@time upsum_threads(R);\n\n  0.037879 seconds (35 allocations: 2.000 KiB)\n\n\nOkay, now this is faster than the Numpy. I’m reasonably impressed - but confused as to that one magical line, though. Let’s dig into it.\n\nchunks = 4\nn = 10000\nround.(Int, n*sqrt.((1:chunks) ./ chunks))\n\n4-element Array{Int64,1}:\n  5000\n  7071\n  8660\n 10000\n\n\nHuh. Digging deeper:\n\n(1:chunks)\n\n1:4\n\n\nOkay, this is a range…\n\n(1:chunks) ./ chunks\n\n0.25:0.25:1.0\n\n\nAnd this is where it starts to hit me, as the presenter introduces the collect command:\n\ncollect((1:chunks) ./ chunks)\n\n4-element Array{Float64,1}:\n 0.25\n 0.5 \n 0.75\n 1.0 \n\n\nOOOOOOOOOOOOOH. So ./ is a a lazy operator! In other words, if you do this:\n\nsqrt((1:chunks) ./ chunks)\n\nMethodError: MethodError: no method matching sqrt(::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}})\nClosest candidates are:\n  sqrt(!Matched::Float16) at math.jl:1018\n  sqrt(!Matched::Complex{Float16}) at math.jl:1019\n  sqrt(!Matched::Missing) at math.jl:1070\n  ...\nMethodError: no method matching sqrt(::StepRangeLen{Float64,Base.TwicePrecision{Float64},Base.TwicePrecision{Float64}})\nClosest candidates are:\n  sqrt(!Matched::Float16) at math.jl:1018\n  sqrt(!Matched::Complex{Float16}) at math.jl:1019\n  sqrt(!Matched::Missing) at math.jl:1070\n  ...\n\nStacktrace:\n [1] top-level scope at In[43]:1\n\n\nThis errors because you’re operating on a range, but instead if you do this:\n\nsqrt.((1:chunks) ./ chunks)\n\n4-element Array{Float64,1}:\n 0.5               \n 0.7071067811865476\n 0.8660254037844386\n 1.0               \n\n\nChains of broadcasting operations “materialize” only once - skipping plenty of unnecessary overhead.\nThis is badass and I have to admit that I’m rather hyped up for Julia now!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! My name’s Dominik Stańczak-Marikin and I’m currently a Python software developer.\nI previously worked at a large pharma company with data processing pipelines for neurodegenerative disease research, at the Institute of Plasma Physics and Laser Microfusion in Warsaw, Poland as a research software engineer, where I helped analyse data from Wendelstein 7-X, the currently (2023) largest stellarator in the world, and at Samsung R&D Poland, helping with natural language translation. I consider myself a recovering physicist.\nIn my spare time, I usually:\n\nhelp out with open source software projects such as PlasmaPy or Beyond All Reason\ntinker, script and automate\nworry a lot (but I’m getting better about it!)\ncrack jokes and smith puns\nread books and learn lots\nwrite simulations to better understand physical systems and stare at pretty data viz\nplay games - classic RTSes made me who I am now. StarCraft II was a huge part of my life, but nowadays I mostly play Factorio with my wife.\ntake care of our doggo, Igornacy\nhike or bike in forests or mountains\nwrite on this here blog to help people out and scratch the teaching itch. In fact, if you’ve found this blog at all useful or enjoyable, there’s no better way to express that (and make my day, too!) than to tell me about it!"
  },
  {
    "objectID": "posts/parsing-and-plotting-latex-expressions-with-sympy.html",
    "href": "posts/parsing-and-plotting-latex-expressions-with-sympy.html",
    "title": "Parsing and plotting LaTeX expressions with SymPy",
    "section": "",
    "text": "Today let’s look into some pretty neat SymPy functionality. I was in a fluid dynamics lecture, practicing taking notes with LaTeX on the go and stumbled upon this monstrosity:\n\\[ \\Delta(k) = \\frac{\\rho_1-\\rho_2}{\\rho_1 + \\rho_2} gk + \\frac{\\gamma k^3}{\\rho_1 + \\rho_2} - \\frac{\\rho_1 \\rho_2}{(\\rho_1 + \\rho_2)^2} U^2 k^2 \\]\n(bonus points for whoever recognizes this!)\nWe were supposed to draw this for a few example sets of values. All right! I opened up pinta and scribbled a few squiggly lines with my small touchpad, following the blackboard drawings. It looked darn ugly, but that got me thinking. SymPy has parsers, right? Can’t I just parse that LaTeX equation into Python and make that plot pretty with matplotlib?\nWell, as it turns out, sure…\n\nthe_plot.show()\n\n\n\n\n\n\n\n\nBut it takes some tinkering.\n\nAll right, let the tinkering commence! Let’s get straight to the point. For this to run, you’ll need antlr4 (in current Jupyter, you can simply do %conda install antlr-python-runtime from within the Notebook).\nWe’re going to simply dump the LaTeX string into sympy.parsing.latex.parse_latex, with the important caveat - this needs to be a r\"raw string\". Otherwise, LaTeX is going to go wild put a carriage return into every \\rho.\n\nimport sympy\nfrom sympy.parsing.latex import parse_latex\nlatex_string = r\"\\Delta(k) = \\frac{\\rho_1-\\rho_2}{\\rho_1 + \\rho_2} gk + \\frac{\\gamma k^3}{\\rho_1 + \\rho_2} - \\frac{\\rho_1 \\rho_2}{(\\rho_1 + \\rho_2)^2} U^2 k^2\"\nequation = parse_latex(latex_string)\nequation\n\nEq(Delta(k), -U**2*k**2*rho_{1}*rho_{2}/(rho_{1} + rho_{2})**2 + (g*k)*((rho_{1} - rho_{2})/(rho_{1} + rho_{2})) + (gamma*k**3)/(rho_{1} + rho_{2}))\n\n\nWe can access the variables we’d like to substitute (as SymPy symbols) using equation.free_symbols:\n\nequation.free_symbols\n\n{U, g, gamma, k, rho_{1}, rho_{2}}\n\n\nIdeally what I’d like to do is use .subs on the equation to plug in numerical values. To achieve this, it would probably be easiest to turn the symbols into Python variables. However…\n\nU, g, gamma, k, rho_1, rho_2 = equation.free_symbols\nU, gamma, rho_1\n\n(g, U, gamma)\n\n\n… the unordered nature of Python’s set comes back with a vengeance! It’s not too trivial to get these out in the right order. You could try sorted, but one does not simply compare Symbols:\n\nsorted(equation.free_symbols)\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-8-f67f3a401c32&gt; in &lt;module&gt;\n----&gt; 1 sorted(equation.free_symbols)\n\n/progs/miniconda3/lib/python3.7/site-packages/sympy/core/relational.py in __nonzero__(self)\n    227 \n    228     def __nonzero__(self):\n--&gt; 229         raise TypeError(\"cannot determine truth value of Relational\")\n    230 \n    231     __bool__ = __nonzero__\n\nTypeError: cannot determine truth value of Relational\n\n\n\nWhat I ended up doing here is:\n\nU, g, gamma, k, rho_1, rho_2 = sorted(equation.free_symbols,\n                                      key = lambda x: str(x)   # the literal key part here - just sort them alphabetically!\n                                     )\nU, g, gamma, k, rho_1, rho_2\n\n(U, g, gamma, k, rho_{1}, rho_{2})\n\n\nAnd now we can simply use subs with a dictionary:\n\nequation.subs(dict(rho_1=1,\n                   rho_2=2,\n                   gamma=1,\n                   g=1,\n                  )\n             )\n\nEq(Delta(k), -U**2*k**2*rho_{1}*rho_{2}/(rho_{1} + rho_{2})**2 + k**3/(rho_{1} + rho_{2}) + k*(rho_{1} - rho_{2})/(rho_{1} + rho_{2}))\n\n\n… or can we? This does not work on rho_{1} and rho_{2}. Here’s why:\n\ndict(rho_1=1,\n     rho_2=2,\n     gamma=1,\n     g=1,\n)\n\n{'rho_1': 1, 'rho_2': 2, 'gamma': 1, 'g': 1}\n\n\nWell duh, those are string values when input this way, and \"rho_1\" != \"rho_{1}\"!\nWe could instead do the following:\n\nbetter_dict = {rho_1: 1,\n rho_2: 2,\n gamma: 1,\n g: 1,\n}\nbetter_dict\n\n{rho_{1}: 1, rho_{2}: 2, gamma: 1, g: 1}\n\n\nWill that work?\n\nequation.subs(better_dict)\n\nEq(Delta(k), -2*U**2*k**2/9 + k**3/3 - k/3)\n\n\nFinally! However, along the way you may have noticed a simpler way to do this:\n\nsimpler_equation = equation.subs({\n     \"rho_{1}\": 1,\n     \"rho_{2}\": 2,\n     \"gamma\": 1,\n     \"g\": 1,\n})\nsimpler_equation\n\nEq(Delta(k), -2*U**2*k**2/9 + k**3/3 - k/3)\n\n\nNote how this did not need us to even touch equation.free_symbols or mess around with sorted at all! I’m leaving the exploratory part here though - it might help someone looking to access variables in a parse_latex expression.\nWe may now plot it:\n\nDeltaK = simpler_equation.rhs\nDeltaK\n\n-2*U**2*k**2/9 + k**3/3 - k/3\n\n\n\nimport sympy.plotting\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = 12, 8\nk_range = (k, 0, 300)\ncolors = [\"blue\", \"green\", \"red\"]\nU_values = [1, 20, 50]\nplots = []\nfor u, color in zip(U_values, colors):\n    plot = sympy.plot(DeltaK.subs(U, u), k_range,\n                      show=False,\n                     line_color=color,\n                     legend=True,\n                     ylabel=r\"$\\Delta(k)$\",\n                     ylim = (-1e6, 1e6),\n                     xlim = (0, 300),\n                     title = f\"${latex_string}$\",\n                     )\n    plots.append(plot)\n\nplots[0].extend(plots[1])\nplots[0].extend(plots[2])\nthe_plot = plots[0]\nthe_plot.show()\n\n\n\n\n\n\n\n\nAnd, while not beautiful, it’s much more pretty than what I got together with pinta!"
  },
  {
    "objectID": "posts/backlogs.html",
    "href": "posts/backlogs.html",
    "title": "Dealing with backlogs",
    "section": "",
    "text": "I have a backlog problem.\nLike probably many of you in 2020, I suffer from having too much interesting content to read, watch, listen to or play through. I’ve been thinking about ways to tackle my gathering points, and I think I finally have something I’m willing to share that could be useful for you as well.\nThe journey for me started with Pocket, which is basically a centralized link gathering service for articles you’d like to read. I use it to dump interesting links from RSS and from all over the web that I don’t have a particular need to read right where and when I find them. This, of course, vastly exceeds the number of articles I actually read. And that’s fine; you have to say no to a lot of content nowadays, becuse a vast amount of news content (I’m looking at you, politics!) becomes out of date pretty much instantly. If you let them stew for a week or two, it often turns out you didn’t need to read them in the first place anyway.\nStill, my backlog has become a hoard of way too many articles to get through in my lifteime. I could just set a goal of “read 5 articles a day”, but that can be easily gamed - just go through all the trash politics news you don’t actually care about, bingo, go do something else while your backlog gently expands.\nWhile reading up on Beeminder (on which you can expect another post soon-ish!), I found this interesting method for dealing with this issue on beemind.me by Gal Tsubery. To your backlog, you assign a numerical value calculated as\n\\[ F = \\sum_{articles} \\text{Days since article was added} \\]\nAnd then you try to minimize that. You can easily plug this into Beeminder to get easy tracking of progress on decreasing your backlog, and a deadline if you slack off on getting through it:\nThis idea is particularly neat for a few reasons:\nI liked it, so I tinkered a bit with Python, Systemd (though a cron job to run the script periodically would do) and the Beeminder web API and some neat libraries to apply the same concept to:"
  },
  {
    "objectID": "posts/backlogs.html#todoist",
    "href": "posts/backlogs.html#todoist",
    "title": "Dealing with backlogs",
    "section": "Todoist",
    "text": "Todoist\nThis tracks my old tasks (all the Julia tutorials I’m never going to get to, stuff I’ve quietly given up on over the years, etc etc):\n\n\n\n\nA little helper function to do the Beeminder API call is nice to have:\nimport requests\nimport os\nbeeminder_auth_token = os.environ['BEEMINDER_TOKEN']\nbeeminder_username = os.environ['BEEMINDER_USERNAME']\n\ndef increment_beeminder(desc, beeminder_goal, value=1):\n    data = {\n        \"value\": value,\n        \"auth_token\": beeminder_auth_token,\n        \"comment\": desc,\n    }\n\n    response = requests.post(\n        f\"https://www.beeminder.com/api/v1/users/{beeminder_username}/goals/{beeminder_goal}/datapoints.json\",\n        data=data,\n    )\n    return response\nAnd now we use the Todoist API to get at out tasks:\n#!/usr/bin/python\nimport datetime\nimport numpy as np\nimport dateutil\nimport dateutil.parser\nimport todoist\nimport os\nnow = datetime.datetime.now(datetime.timezone.utc)\nkey = os.environ['TODOIST_KEY']\napi = todoist.TodoistAPI(key)\napi.sync()\n\nundone_tasks = api.items.all(lambda x: not x['checked'])\ndates = [dateutil.parser.parse(task['date_added']) for task in undone_tasks]\ndeltas = [date - now for date in dates]\ntotal = -np.sum(deltas)\ntotal_days = total.days + total.seconds / 3600 / 24\n\nmessage = f\"Incremented automatically from {len(undone_tasks)} tasks at {now}\"\nincrement_beeminder(message, \"todoist-backlog\", total_days)"
  },
  {
    "objectID": "posts/backlogs.html#youtube-videos",
    "href": "posts/backlogs.html#youtube-videos",
    "title": "Dealing with backlogs",
    "section": "Youtube videos",
    "text": "Youtube videos\n\n\n\n\nThis took a bit of legwork because YouTube’s Watch Later playlist is not accessible from remote API’s, as I’ve written about before; but once you get accustomed to using another playlist for you kitten vids SciPy tutorials, it’s as simple as the following:\n#!/usr/bin/python\nimport pafy\nimport dateutil.parser\nfrom datetime import datetime, timedelta\nfrom sc2replib import increment_beeminder\nurl = \"https://www.youtube.com/playlist?list=PL8B03F998924DA45B\"  # example playlist ID\nplaylist = pafy.get_playlist(url)\n\nnow = datetime.now()\ntotal = timedelta(seconds=0)\nfor item in playlist['items']:\n    added_date = dateutil.parser.parse(item['playlist_meta']['added'])\n    total += now - added_date\n\n\ntotal_days = total.days + total.seconds / 3600 / 24\n\nmessage = f\"Incremented automatically from {len(playlist['items'])} movies at {now}\"\nincrement_beeminder(message, \"youtube-backlog-upgrade\", total_days)"
  },
  {
    "objectID": "posts/backlogs.html#future-work",
    "href": "posts/backlogs.html#future-work",
    "title": "Dealing with backlogs",
    "section": "Future work?",
    "text": "Future work?\nI’m probably going to simplify these scripts using my Beeminder-CLI library I’ve been writing recently.\nI’m also wondering about other applications of this idea:\n\nMy pubs (another piece of really cool software!) articles-to-read list is definitely the next move. They store article add dates in .yaml files, so it’s going to be simple to loop over all those, filter out only the ones marked as actually to be read, then sum them up.\nI use Antennapod for podcasts, but that’s Android-only and I haven’t yet found a simple way to run Python scripts on mobile; even then, I have no clue how to extract podcast queue-addition dates from it.\nSteam games would be another neat idea, but I don’t know if they have a library API I could read. I also don’t know of any way to neatly remove a game from the immediate library. Tags could work, I guess, if they have them."
  },
  {
    "objectID": "posts/the-importance-of-good-notation.html",
    "href": "posts/the-importance-of-good-notation.html",
    "title": "The importance of good notation",
    "section": "",
    "text": "I’ve just spent the last four weeks hunting for a bug in my thesis code that I’ve mentioned last time. Since then, I’ve given up on the paper I’m reimplementing more times than I can count. Well… Can you guess how that went?\n\nIt turns out I missed the fact that \\(S_{pT}^{ai}\\) is very much not the same as \\(S_{pT\\theta}^{ai}\\), where the difference is conceptually a rescaling \\(S_{pT\\theta}^{ai} \\sim \\mu S_{pT}^{ai}\\), \\(\\mu\\) being a viscosity-like object.\nSo, that happened.\n\nI suspect I’m sort of writing this as a reminder to future!myself that hey, small details are crucial and deserve attention, unless you want to waste your own time.\nAt the same time, here’s my plea to you - when writing, especially when writing scientific literature (which, like your code, is going to be read many more times than the one time you’re writing it), pay attention to the notation you’re using for your concepts. Think ahead of how it might contribute to the reader’s understanding of what you’re trying to communicate, or hinder it.\nAnd maybe, just maybe, make use of the wide arsenal of typography you’ve got available and don’t reuse symbols for concepts that are different even just by their dimensionality!\n\nAt the same time, I’m really happy to be past this. This bug was really gnawing at me.\nAlso, I think this is a great testament to the power of physical unit packages in scientific computing. I wasted 4 weeks on this, sure - but without astropy.units notifying me that “your units are off, mate” on every test run, I have no idea how many weeks I would spend on it later on in the development process, with my results in disagreement with previous results!"
  },
  {
    "objectID": "posts/youtube-watch-later-export.html",
    "href": "posts/youtube-watch-later-export.html",
    "title": "Export YouTube’s Watch Later playlist",
    "section": "",
    "text": "Outdated by 2021-10-03\n\n\n\nAs pointed out in the comments by LiadNeumann, YouTube no longer takes the &disable_polymer=true parameter for the URL, so this method no longer works. I’m leaving it up for posterity’s sake. If anyone has better ideas how to solve this, I’d be much obliged for a comment. 😉\n\n\nYouTube is increasingly becoming a walled garden. One of the dark patterns I see in it is that the Watch Later playlist, which you may have gathered over what feels like a millenia (so many lectures, so little time...), cannot be exported - you can only add one video at a time to another playlist. That obviously doesn't scale. Here's a quick trick to solve that!\nTake this URL:\nhttps://www.youtube.com/playlist?list=WL\nAppend &disable_polymer=true to it:\nhttps://www.youtube.com/playlist?list=WL&disable_polymer=true\nAnd this should bring you to an older version of the website, where if you click the 3-dot \"More\" menu, you'll find an \"Add all to...\" button. This allows you to export your Watch Later playlist to another playlist, without the annoying limitations. Once you're there, you can use better software (NewPipe, for example, or another solution from the many available) to store the playlist elsewhere. I did not investigate those, as I simply wanted to mirror my Watch Later playlist in NewPipe.\nSadly, the same method does not work for the Liked Videos page - which is why you should probably not trust YouTube with your data as much as you likely do now."
  },
  {
    "objectID": "posts/distance_matrix_numba.html",
    "href": "posts/distance_matrix_numba.html",
    "title": "Better Numba calculation of inter-particle distance matrices",
    "section": "",
    "text": "Recently, I’ve been looking for efficient ways to compute a distance matrix in Python. I’m deliberately trying to implement a naive n-body simulation so as to find optimized ways of calculating those, as practice. Let’s do that using Numba.\nAs usual, we’re going to be using the standard Python scientific stack… and we’ll also use Numba, transitioning onto the GPU next week. Let’s get those imports prepped:\nimport numpy as np\nimport scipy, scipy.spatial\nimport numba\nimport sys\nnp.__version__, scipy.__version__, numba.__version__, sys.version\nfrom numpy.testing import assert_allclose\nLet’s get ourselves some sample 3D position data, for twenty thousand particles:\nN = int(1e4)\nnp.random.seed(743)\nr = np.random.random(size=(N, 3))\nr\n\narray([[0.83244056, 0.94442527, 0.57451672],\n       [0.09049263, 0.08428888, 0.43300003],\n       [0.29973189, 0.11463598, 0.27817412],\n       ...,\n       [0.49628111, 0.1462252 , 0.18381982],\n       [0.80535628, 0.07900376, 0.19831322],\n       [0.75236151, 0.02655101, 0.54791037]])"
  },
  {
    "objectID": "posts/distance_matrix_numba.html#direct-numpy-summation",
    "href": "posts/distance_matrix_numba.html#direct-numpy-summation",
    "title": "Better Numba calculation of inter-particle distance matrices",
    "section": "Direct numpy summation",
    "text": "Direct numpy summation\nThis is the classic approach, but with a major flaw - it allocates a lot of temporary arrays in the meantime, and that takes a while.\n\ndef pairwise_numpy(X):\n    \"\"\"\n    Reproduced from https://jakevdp.github.io/blog/2013/06/15/numba-vs-cython-take-2/\n    \"\"\"\n    return np.sqrt(((X[:, None, :] - X) ** 2).sum(-1))\npairwise_numpy_timing = %timeit -o pairwise_numpy(r)\npairwise_numpy_result = pairwise_numpy(r)\n\n5.02 s ± 43.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\nIt’s nice to have it for comparison, though."
  },
  {
    "objectID": "posts/distance_matrix_numba.html#direct-slow-python-loop",
    "href": "posts/distance_matrix_numba.html#direct-slow-python-loop",
    "title": "Better Numba calculation of inter-particle distance matrices",
    "section": "Direct (slow) Python loop",
    "text": "Direct (slow) Python loop\nWe’ll now switch over to doing things Numba-style. This means that we’ll use math instead of numpy, so that the \\(\\sqrt{x}\\) we’ll doing is explicitly a scalar operation.\n\nimport math\ndef scalar_distance(r, output):\n    N, M = r.shape\n    for i in range(N):\n        for j in range(N):\n            tmp = 0.0\n            for k in range(M):\n                tmp += (r[i, k] - r[j, k])**2\n            output[i,j] = math.sqrt(tmp)\noutput = np.zeros((N, N), dtype=float)\n\n\n# warning: LONG\ndirect_summation_timeit = %timeit -o -n1 -r1 scalar_distance(r, output)\n\n# sanity check!\nassert_allclose(pairwise_numpy_result, output)\n\nprint(f\"The direct summation implementation is {direct_summation_timeit.average / pairwise_numpy_timing.average:.2f} slower than NumPy.\")\n\n4min 6s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\nThe direct summation implementation is 49.11 slower than NumPy.\n\n\nAnd now, let’s simply wrap this in numba.njit.\nNote that the below is equivalent to\n@numba.njit\ndef scalar_distance(...):\n    ...\n\nnumba_jit_scalar_distance = numba.njit(scalar_distance)\nnumba_jit_timing = %timeit -o numba_jit_scalar_distance(r, output)\n\nassert_allclose(pairwise_numpy_result, output)\n\nprint(f\"Our Numba implementation is {pairwise_numpy_timing.average/numba_jit_timing.average:.2f} times faster than NumPy!\")\n\n408 ms ± 16.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nOur Numba implementation is 12.31 times faster than NumPy!\n\n\nNot bad! But we can still get speedups by replacing range with numba.prange, which tells Numba that “yes, this loop is trivially parallelizable”. To do so we use the parallel=True flag to njit:"
  },
  {
    "objectID": "posts/distance_matrix_numba.html#optimal-numba-solution",
    "href": "posts/distance_matrix_numba.html#optimal-numba-solution",
    "title": "Better Numba calculation of inter-particle distance matrices",
    "section": "Optimal numba solution",
    "text": "Optimal numba solution\n\n@numba.njit(parallel=True)\ndef numba_jit_scalar_distance_parallel(r, output):\n    N, M = r.shape\n    for i in numba.prange(N):\n        for j in numba.prange(N):\n            tmp = 0.0\n            for k in range(M):\n                tmp += (r[i, k] - r[j, k])**2\n            output[i,j] = math.sqrt(tmp)\n\nnumba_jit_parallel_timing = %timeit -o numba_jit_scalar_distance_parallel(r, output)\n\nassert_allclose(pairwise_numpy_result, output)\n\nprint(f\"Using `parallel=True` grants us a further {numba_jit_timing.average/numba_jit_parallel_timing.average:.2f}x speedup.\")\n\n105 ms ± 5.98 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\nUsing `parallel=True` grants us a further 3.90x speedup.\n\n\nNote that I’ve got four cores on this laptop, so this problem is truly trivially parallelilzable. This is nice because numba.prange is actually a no-op when not using it from within numba:\n\ndef scalar_distance_prange(r, output):\n    N, M = r.shape\n    for i in numba.prange(N):\n        for j in numba.prange(N):\n            tmp = 0.0\n            for k in range(M):\n                tmp += (r[i, k] - r[j, k])**2\n            output[i,j] = math.sqrt(tmp)\n\ndirect_summation_prange_timeit = %timeit -o -n1 -r1 scalar_distance_prange(r, output)\nassert_allclose(pairwise_numpy_result, output)\nprint(f\"{direct_summation_prange_timeit.average:.5f}s vs {direct_summation_timeit.average:.5f}s.\")\n\n4min 2s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n242.71444s vs 246.70353s.\n\n\nIt’s something you can just throw in “for free”, lets you debug stuff just as easily, and once you end up turning on parallel = True, it lets speed ups kick in.\nHowever, suppose we wanted to have this run really fast. What we then could do is turn to the GPU. And this is exactly what we’ll be doing next week!"
  },
  {
    "objectID": "posts/bayes-sc2-2.html",
    "href": "posts/bayes-sc2-2.html",
    "title": "Learning my per-matchup MMR in Starcraft II through PyMC3",
    "section": "",
    "text": "In this post we’ll continue our SC2 replay research, started last time. You may want to go back to that and pick up on the terminology!\nTo recap: we used replay data from my SC2 games over 2019 to estimate a “true MMR” value and infer the size of per-game fluctuations. This time, we’ll redo that analysis, except to get something more useful: we’ll look at the three matchups I played and infer separate MMR values for each of those. Let’s dig into it!\nI’ll redo the basic data cleaning steps here. If any of this is confusing, reviewing the previous post might really be a good idea - or you could ask a question below, as always!\nIf you take a close look, you might also find a teaser for one of the next posts in this series here :)\nimport pandas as pd\nimport altair\n\ndef MMR_winrate(diff):\n    return 1 / (1 + 10**(-diff/880))\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/StanczakDominik/stanczakdominik.github.io/src/files/replays.csv\", index_col=0)\ndf['time_played_at'] = pd.to_datetime(df.time_played_at)\ndf = df.sort_values('time_played_at')\nfor column in ['race', 'enemy_race', 'map_name']:\n    df[column] = pd.Categorical(df[column])\ndf['enemy_mmr'] = df['mmr'] - df['mmr_diff']\ndf['expected_winrate'] = MMR_winrate(df.mmr_diff)\nall_data = df[(df.mmr &gt; 0) & (df.enemy_mmr &gt; 0) & (df.race == \"Protoss\") & (df.duration &gt; 10)]\nall_data = all_data.rename({\"enemy_nickame\": \"enemy_nickname\"}, axis=1) # whoops\ndata = all_data[(all_data['time_played_at'] &gt; '2019-01-01') & (all_data['time_played_at'] &lt; '2020-01-01')]\ndata\n\n\n\n\n\n\n\n\ntime_played_at\nwin\nrace\nenemy_race\nmmr\nmmr_diff\nenemy_nickname\nmap_name\nduration\nenemy_mmr\nexpected_winrate\n\n\n\n\n8\n2019-10-06 12:36:36+00:00\nTrue\nProtoss\nProtoss\n3826\n78\nvasea\nWorld of Sleepers LE\n743\n3748\n0.550847\n\n\n325\n2019-10-08 19:33:28+00:00\nFalse\nProtoss\nProtoss\n3893\n-53\nWavelength\nEphemeron LE\n254\n3946\n0.465386\n\n\n54\n2019-10-10 07:41:27+00:00\nFalse\nProtoss\nZerg\n3828\n26\nPereiRa\nWinter's Gate LE\n45\n3802\n0.517001\n\n\n346\n2019-10-10 07:55:19+00:00\nTrue\nProtoss\nZerg\n3760\n-56\n&lt;PROOO&gt;&lt;sp/&gt;Jesperpro\nThunderbird LE\n801\n3816\n0.463433\n\n\n138\n2019-10-10 20:42:11+00:00\nTrue\nProtoss\nProtoss\n3827\n126\nPippuri\nAcropolis LE\n697\n3701\n0.581684\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n391\n2019-12-27 20:24:27+00:00\nFalse\nProtoss\nZerg\n3933\n-100\nHiveMind\nWorld of Sleepers LE\n262\n4033\n0.434956\n\n\n25\n2019-12-27 20:40:39+00:00\nTrue\nProtoss\nZerg\n3914\n0\nRacin\nNightshade LE\n911\n3914\n0.500000\n\n\n208\n2019-12-27 21:24:06+00:00\nTrue\nProtoss\nTerran\n3936\n-41\n&lt;DemuCl&gt;&lt;sp/&gt;Jazzz\nNightshade LE\n1277\n3977\n0.473206\n\n\n59\n2019-12-28 20:58:25+00:00\nTrue\nProtoss\nTerran\n3959\n22\nrOoSter\nSimulacrum LE\n76\n3937\n0.514387\n\n\n364\n2019-12-28 21:06:48+00:00\nTrue\nProtoss\nZerg\n3980\n-260\ncontremaitre\nNightshade LE\n478\n4240\n0.336192\n\n\n\n\n138 rows × 11 columns\nLet’s visualize the games on a per-matchup MMR vs enemy MMR basis. I added some fancy Altair selection magic, so you can look at winrates in specific MMR ranges.\nbrush = altair.selection(type='interval')\nscatter = altair.Chart(data).mark_circle().encode(\n    altair.X('enemy_mmr',\n             scale=altair.Scale(zero=False)),\n    altair.Y('mmr',\n             scale=altair.Scale(zero=False)),\n    facet='enemy_race',\n    size='expected_winrate',\n    color='win',\n    tooltip='enemy_nickname',\n).add_selection(brush)\n\nbar = altair.Chart(data).mark_bar().encode(\n    x=altair.X('mean(win):Q', scale=altair.Scale(domain=(0, 1))),\n    y='enemy_race:O',\n).transform_filter(brush)\n\nscatter & bar"
  },
  {
    "objectID": "posts/bayes-sc2-2.html#first-run",
    "href": "posts/bayes-sc2-2.html#first-run",
    "title": "Learning my per-matchup MMR in Starcraft II through PyMC3",
    "section": "First run",
    "text": "First run\nAnd now, let’s sample! We’ll add a predictive prior and posterior sample: this lets us easily see what sort of data we’d see from our initial assumptions and from the fully “learned” (“taught”?) model.\n\npredictive_var_names = \"win μ σ winrate\".split()\nwith split_model:\n    trace = pm.sample(2000, tune=2000, chains=4, random_seed=1)\n    output = az.from_pymc3(trace=trace,\n                           prior=pm.sample_prior_predictive(2000 , var_names=predictive_var_names, random_seed=1),\n                           posterior_predictive=pm.sample_posterior_predictive(trace, var_names=predictive_var_names, random_seed=1),\n                          )\noutput\n\nAuto-assigning NUTS sampler...\nInitializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 2 jobs)\nNUTS: [helper, σ, μ]\n\n\n\n    \n        \n      \n      100.00% [16000/16000 00:17&lt;00:00 Sampling 4 chains, 7 divergences]\n    \n    \n\n\nSampling 4 chains for 2_000 tune and 2_000 draw iterations (8_000 + 8_000 draws total) took 18 seconds.\nThere were 4 divergences after tuning. Increase `target_accept` or reparameterize.\nThere were 3 divergences after tuning. Increase `target_accept` or reparameterize.\n\n\n\n    \n        \n      \n      100.00% [8000/8000 00:07&lt;00:00]\n    \n    \n\n\n\n            \n              \n                arviz.InferenceData\n              \n              \n              \n            \n                  \n                  posterior\n                  \n                  \n                      \n                          \n\n\nShow/Hide data repr\n\n\n\n\n\nShow/Hide attributes\n\n\n\n\n\n\n\nxarray.DatasetDimensions:chain: 4draw: 2000race: 3replay: 138Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])race(race)&lt;U7'Terran' 'Protoss' 'Zerg'array(['Terran', 'Protoss', 'Zerg'], dtype='&lt;U7')replay(replay)int648 325 54 346 138 ... 25 208 59 364array([  8, 325,  54, 346, 138, 405,  20, 129, 104,  46, 302, 408, 101, 219,\n       316, 126,  98, 231, 241, 385, 193, 198,  90, 329, 137, 200,  80, 355,\n       317,  33,  64, 213, 368,  49, 435, 134, 254, 330,  60,  39, 218, 109,\n       301, 133, 328, 181, 156, 395,  43, 249,  27, 153, 211, 420, 366, 186,\n       163,  63, 202,  45,  69,  31, 167, 177,  95, 151, 392, 387,  18, 286,\n       102, 290, 195, 428, 403,  97, 406, 412, 374, 263, 371,  41, 212,  52,\n       238, 345,   4, 117, 407,  56, 103, 118, 319,  57, 128, 294,  15, 273,\n       327, 281, 378, 121, 113, 284, 422, 389, 216,  58, 418, 309, 123, 116,\n       222, 122,  11, 361, 179, 255, 239, 225, 381, 424, 343, 287, 341, 184,\n       380, 196, 174, 306, 252, 148, 416, 391,  25, 208,  59, 364])Data variables: (6)μ(chain, draw, race)float644.303e+03 3.375e+03 ... 3.955e+03array([[[4302.79553938, 3375.01011989, 4048.86315149],\n        [4323.86457361, 3295.74222841, 3954.05048638],\n        [4239.6739907 , 3448.28894514, 4235.71022401],\n        ...,\n        [4250.95500899, 3592.96680679, 3907.78214913],\n        [4321.36051851, 3657.60296464, 4041.53244852],\n        [4378.58347689, 3680.20202035, 4041.44904971]],\n\n       [[4141.06331072, 3385.94471985, 4135.36324411],\n        [4138.80737046, 3670.4073407 , 4045.58916728],\n        [4194.72695819, 3676.40161761, 3931.11602939],\n        ...,\n        [4066.10358294, 3429.85753425, 4004.41005399],\n        [4049.09082588, 3382.29455772, 3954.15834981],\n        [4422.29348522, 3549.2372973 , 4066.31337586]],\n\n       [[4212.20435522, 3476.71136441, 4104.05998443],\n        [4139.66832504, 3371.93545224, 4198.27086105],\n        [4369.19834033, 3797.3129873 , 3828.50248262],\n        ...,\n        [4320.92217993, 3755.9095065 , 4165.54070368],\n        [4231.08308098, 3713.65002654, 3964.55758405],\n        [4274.41509776, 3407.86141075, 3931.29076179]],\n\n       [[4314.0372303 , 3573.73152574, 4058.10772914],\n        [4114.59770596, 3463.36390945, 4018.74209014],\n        [4464.57525582, 3705.14913054, 4089.96639073],\n        ...,\n        [4504.44403668, 3638.42652115, 3911.16769539],\n        [4256.83526394, 3886.43065114, 3954.52402571],\n        [4256.83526394, 3886.43065114, 3954.52402571]]])helper(chain, draw, replay)float640.7894 0.5491 ... 1.849 -0.6287array([[[ 0.78936952,  0.5490887 ,  0.16650833, ...,  0.08329766,\n         -0.65050062,  0.29929534],\n        [ 1.58545972, -0.65388647, -0.65379105, ...,  1.82456396,\n         -0.19362754,  1.51704283],\n        [-0.95306398,  1.74037107, -1.06139752, ...,  0.49580659,\n         -1.65596289,  0.84096651],\n        ...,\n        [ 0.83862077,  0.72070695,  0.48932378, ..., -0.2039098 ,\n         -1.29151567,  1.33576567],\n        [ 0.36248347,  1.46751904,  2.691205  , ...,  1.45830882,\n         -0.35768443,  2.55530514],\n        [ 1.08745544,  1.27714706,  0.61058664, ...,  1.29672473,\n         -0.50342047,  2.35664708]],\n\n       [[ 1.03478446,  0.34214246, -0.02786286, ..., -1.94344799,\n         -1.74622235,  0.23291672],\n        [-1.13226371,  0.59456024, -0.63842647, ...,  0.36564697,\n         -0.64213818,  1.34387537],\n        [-1.13076226,  0.23707974, -1.6545314 , ..., -0.40823523,\n         -2.01952747,  1.78589938],\n        ...,\n        [ 1.07091423, -0.57066178,  0.11606562, ..., -1.94859916,\n         -0.54562754,  0.47149236],\n        [ 1.73615185, -0.27786063, -0.27283103, ..., -1.55429249,\n         -0.73246607,  0.73910541],\n        [-0.65499351,  1.15010742,  0.65100113, ..., -0.51231793,\n         -0.53271042,  1.44304005]],\n\n       [[ 0.49833716, -0.51620112,  0.13741193, ...,  0.501271  ,\n          1.09108997, -0.73316122],\n        [-0.25632797,  0.70929871, -0.30192588, ...,  0.74061067,\n          0.88777097,  0.1928401 ],\n        [ 0.24746999, -0.78483546,  0.05723709, ..., -0.06153329,\n         -2.12186644,  0.20316985],\n        ...,\n        [ 2.14989454,  1.12821892,  0.49014978, ...,  0.70910716,\n         -0.11305358, -0.26844358],\n        [ 0.79712821, -0.98653506,  0.27999657, ...,  0.02147621,\n          0.45285935, -0.223395  ],\n        [ 1.17281098, -0.88489812,  2.22847395, ...,  0.52398786,\n         -0.19246968,  0.97882991]],\n\n       [[ 0.15828557, -0.38448579, -0.2247607 , ...,  0.09849247,\n         -0.17279176,  0.79657494],\n        [ 0.3954921 , -0.67537986, -0.07942884, ...,  1.00072493,\n         -0.15591947,  0.71581227],\n        [ 2.66366278,  0.60968907, -0.42128861, ..., -0.09888939,\n          0.52831846,  0.29878227],\n        ...,\n        [-0.54199678, -0.4028017 , -1.59414456, ...,  2.31936087,\n          0.07555759,  0.57843756],\n        [ 1.20184084,  0.51496559,  0.73758509, ...,  1.82835713,\n          1.8487901 , -0.62868765],\n        [ 1.20184084,  0.51496559,  0.73758509, ...,  1.82835713,\n          1.8487901 , -0.62868765]]])σ(chain, draw, race)float64140.4 252.7 183.9 ... 72.88 55.78array([[[140.36011672, 252.69076651, 183.8904596 ],\n        [161.51391431,  82.67823509, 212.55352477],\n        [108.91799094,  66.69192358,  88.42575348],\n        ...,\n        [ 31.69679346, 101.09097511,  32.56956811],\n        [148.25558444,  50.48937963, 103.02389557],\n        [170.53973503,  42.45131739,   7.22075687]],\n\n       [[ 81.39860974,  97.00223551,   5.23367679],\n        [ 28.60567987, 120.58701662, 119.74595091],\n        [ 24.85236716,  47.00875165,  62.47517088],\n        ...,\n        [ 27.08894035, 109.52428738,  89.74107174],\n        [101.36114868,  44.00684249,  63.21585206],\n        [ 45.44140301,  42.24565628,  32.21911083]],\n\n       [[ 85.15403518,  72.24973621, 101.9819129 ],\n        [ 69.33515359,  91.23129494, 140.7847575 ],\n        [ 45.56776301, 101.34687473,  65.85446419],\n        ...,\n        [ 39.67975301,  28.28842643,  52.09586543],\n        [227.54401728,  35.04100696,  46.01622763],\n        [141.84401665, 229.02600795,   5.01932257]],\n\n       [[170.73574582,   6.64000168, 256.76699449],\n        [ 77.7241646 ,  58.94863209,  63.38158179],\n        [ 92.43500404,  57.06879911,  99.61428138],\n        ...,\n        [ 34.21264796,  52.28070628, 126.57472452],\n        [179.13659055,  72.88096661,  55.7761304 ],\n        [179.13659055,  72.88096661,  55.7761304 ]]])MMR(chain, draw, replay)float643.574e+03 3.514e+03 ... 3.919e+03array([[[3574.47650843, 3513.75976477, 4079.48244533, ...,\n         4314.48720853, 4211.49119573, 4103.90070894],\n        [3426.82524018, 3241.68004931, 3815.08489516, ...,\n         4618.55704024, 4292.59103243, 4276.50328733],\n        [3384.72727505, 3564.35763932, 4141.85534813, ...,\n         4293.67624891, 4059.3098399 , 4310.07332137],\n        ...,\n        [3677.74379789, 3665.82377497, 3923.71921341, ...,\n         4244.49172209, 4210.01810344, 3951.28746012],\n        [3675.90453027, 3731.69709048, 4318.79087106, ...,\n         4537.56294544, 4268.33180356, 4304.78993801],\n        [3726.36593656, 3734.41859557, 4045.85794741, ...,\n         4599.72656921, 4292.73028387, 4058.46582531]],\n\n       [[3486.32112569, 3419.1333035 , 4135.21741888, ...,\n         3982.86934633, 3998.92323941, 4136.58225494],\n        [3533.87103837, 3742.10358598, 3969.14018261, ...,\n         4149.26695054, 4120.43857122, 4206.51280099],\n        [3623.24589541, 3687.54644032, 3827.74889736, ...,\n         4184.58134634, 4144.53692   , 4042.69039863],\n        ...,\n        [3547.14865165, 3367.35620982, 4014.82590683, ...,\n         4013.31809649, 4051.32311109, 4046.72228372],\n        [3458.69711886, 3370.06678874, 3936.91110358, ...,\n         3891.5459539 , 3974.84722316, 4000.8815279 ],\n        [3521.5666664 , 3597.8243401 , 4087.28805338, ...,\n         4399.01303974, 4398.08637637, 4112.80684303]],\n\n       [[3512.71609283, 3439.41596971, 4118.0735164 , ...,\n         4254.88960386, 4305.11506861, 4029.29080031],\n        [3348.55031945, 3436.64569193, 4155.76429966, ...,\n         4191.01867944, 4201.22206179, 4225.4198078 ],\n        [3822.39329786, 3717.77236616, 3832.27180076, ...,\n         4366.39440592, 4272.50963317, 3841.88212429],\n        ...,\n        [3816.72664015, 3787.82504444, 4191.07548085, ...,\n         4349.0593768 , 4316.43624179, 4151.55590297],\n        [3741.58220182, 3679.08084456, 3977.44197015, ...,\n         4235.96986521, 4334.12851785, 3954.27778896],\n        [3676.46562714, 3205.19672793, 3942.47619137, ...,\n         4348.73963984, 4247.11442498, 3936.20382483]],\n\n       [[3574.78254219, 3571.17853943, 4000.39660036, ...,\n         4330.853415  , 4284.53550087, 4262.64188132],\n        [3486.67762767, 3423.55119059, 4013.70776491, ...,\n         4192.37821489, 4102.47899548, 4064.11140391],\n        [3857.16116685, 3739.94335347, 4048.00002814, ...,\n         4455.43441496, 4513.41037517, 4119.72937191],\n        ...,\n        [3610.09054683, 3617.36776396, 3709.38928727, ...,\n         4583.79551357, 4507.02906206, 3984.38327026],\n        [3974.02197305, 3923.96184115, 3995.66366771, ...,\n         4584.36092651, 4588.0212183 , 3919.4582613 ],\n        [3974.02197305, 3923.96184115, 3995.66366771, ...,\n         4584.36092651, 4588.0212183 , 3919.4582613 ]]])MMR_diff(chain, draw, replay)float64-173.5 -432.2 ... 651.0 -320.5array([[[-173.52349157, -432.24023523,  277.48244533, ...,\n          337.48720853,  274.49119573, -136.09929106],\n        [-321.17475982, -704.31995069,   13.08489516, ...,\n          641.55704024,  355.59103243,   36.50328733],\n        [-363.27272495, -381.64236068,  339.85534813, ...,\n          316.67624891,  122.3098399 ,   70.07332137],\n        ...,\n        [ -70.25620211, -280.17622503,  121.71921341, ...,\n          267.49172209,  273.01810344, -288.71253988],\n        [ -72.09546973, -214.30290952,  516.79087106, ...,\n          560.56294544,  331.33180356,   64.78993801],\n        [ -21.63406344, -211.58140443,  243.85794741, ...,\n          622.72656921,  355.73028387, -181.53417469]],\n\n       [[-261.67887431, -526.8666965 ,  333.21741888, ...,\n            5.86934633,   61.92323941, -103.41774506],\n        [-214.12896163, -203.89641402,  167.14018261, ...,\n          172.26695054,  183.43857122,  -33.48719901],\n        [-124.75410459, -258.45355968,   25.74889736, ...,\n          207.58134634,  207.53692   , -197.30960137],\n        ...,\n        [-200.85134835, -578.64379018,  212.82590683, ...,\n           36.31809649,  114.32311109, -193.27771628],\n        [-289.30288114, -575.93321126,  134.91110358, ...,\n          -85.4540461 ,   37.84722316, -239.1184721 ],\n        [-226.4333336 , -348.1756599 ,  285.28805338, ...,\n          422.01303974,  461.08637637, -127.19315697]],\n\n       [[-235.28390717, -506.58403029,  316.0735164 , ...,\n          277.88960386,  368.11506861, -210.70919969],\n        [-399.44968055, -509.35430807,  353.76429966, ...,\n          214.01867944,  264.22206179,  -14.5801922 ],\n        [  74.39329786, -228.22763384,   30.27180076, ...,\n          389.39440592,  335.50963317, -398.11787571],\n        ...,\n        [  68.72664015, -158.17495556,  389.07548085, ...,\n          372.0593768 ,  379.43624179,  -88.44409703],\n        [  -6.41779818, -266.91915544,  175.44197015, ...,\n          258.96986521,  397.12851785, -285.72221104],\n        [ -71.53437286, -740.80327207,  140.47619137, ...,\n          371.73963984,  310.11442498, -303.79617517]],\n\n       [[-173.21745781, -374.82146057,  198.39660036, ...,\n          353.853415  ,  347.53550087,   22.64188132],\n        [-261.32237233, -522.44880941,  211.70776491, ...,\n          215.37821489,  165.47899548, -175.88859609],\n        [ 109.16116685, -206.05664653,  246.00002814, ...,\n          478.43441496,  576.41037517, -120.27062809],\n        ...,\n        [-137.90945317, -328.63223604,  -92.61071273, ...,\n          606.79551357,  570.02906206, -255.61672974],\n        [ 226.02197305,  -22.03815885,  193.66366771, ...,\n          607.36092651,  651.0212183 , -320.5417387 ],\n        [ 226.02197305,  -22.03815885,  193.66366771, ...,\n          607.36092651,  651.0212183 , -320.5417387 ]]])winrate(chain, draw, replay)float640.3884 0.244 ... 0.846 0.3018array([[[0.38840135, 0.24397871, 0.67393861, ..., 0.70745598,\n         0.67221636, 0.41190061],\n        [0.30145528, 0.13670806, 0.50855856, ..., 0.84272985,\n         0.71716256, 0.52386025],\n        [0.27877826, 0.26921766, 0.70873675, ..., 0.69606027,\n         0.57933223, 0.54571002],\n        ...,\n        [0.45417135, 0.32451442, 0.57895556, ..., 0.66816823,\n         0.67136651, 0.31963761],\n        [0.45297858, 0.36337639, 0.79449259, ..., 0.81256357,\n         0.70411153, 0.5422807 ],\n        [0.485852  , 0.36502532, 0.65431758, ..., 0.83608882,\n         0.71723646, 0.38343406]],\n\n       [[0.33521265, 0.20123624, 0.7051384 , ..., 0.50383932,\n         0.5404183 , 0.43275976],\n        [0.36348169, 0.3696986 , 0.60762375, ..., 0.61081735,\n         0.61774329, 0.47810857],\n        [0.41910992, 0.3370959 , 0.51683711, ..., 0.63254534,\n         0.63251832, 0.37372364],\n        ...,\n        [0.37155715, 0.18033725, 0.6357291 , ..., 0.52373938,\n         0.57423102, 0.37619612],\n        [0.31930178, 0.181388  , 0.58734606, ..., 0.44433253,\n         0.5247373 , 0.34849273],\n        [0.35606613, 0.28678965, 0.67841064, ..., 0.75105153,\n         0.76967316, 0.41755699]],\n\n       [[0.35077423, 0.20990227, 0.69572652, ..., 0.67417267,\n         0.7237621 , 0.36555445],\n        [0.26014989, 0.20870266, 0.71619203, ..., 0.63645155,\n         0.66626863, 0.49046362],\n        [0.54851081, 0.35499039, 0.51979175, ..., 0.73475424,\n         0.70638391, 0.26082117],\n        ...,\n        [0.54483632, 0.39798298, 0.73459157, ..., 0.72582073,\n         0.72964516, 0.4424017 ],\n        [0.49580194, 0.33216402, 0.61279042, ..., 0.66320592,\n         0.73867943, 0.32134157],\n        [0.45334239, 0.12582742, 0.59087079, ..., 0.72565421,\n         0.69241571, 0.31111641]],\n\n       [[0.38859158, 0.27274337, 0.62694182, ..., 0.71623942,\n         0.71286761, 0.51480671],\n        [0.33542055, 0.20310077, 0.63505131, ..., 0.63727424,\n         0.60658696, 0.38693233],\n        [0.57092553, 0.36838244, 0.65558424, ..., 0.7776215 ,\n         0.81879732, 0.42196867],\n        ...,\n        [0.41075375, 0.29736221, 0.43971402, ..., 0.83029583,\n         0.81630679, 0.33875661],\n        [0.64368704, 0.48558788, 0.62404084, ..., 0.83050419,\n         0.84598416, 0.30180419],\n        [0.64368704, 0.48558788, 0.62404084, ..., 0.83050419,\n         0.84598416, 0.30180419]]])Attributes: (6)created_at :2020-06-20T10:35:36.555638arviz_version :0.8.3inference_library :pymc3inference_library_version :3.9.1sampling_time :18.211029291152954tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  posterior_predictive\n                  \n                  \n                      \n                          \n\n\nShow/Hide data repr\n\n\n\n\n\nShow/Hide attributes\n\n\n\n\n\n\n\nxarray.DatasetDimensions:chain: 4draw: 2000race: 3replay: 138Coordinates: (4)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])replay(replay)int648 325 54 346 138 ... 25 208 59 364array([  8, 325,  54, 346, 138, 405,  20, 129, 104,  46, 302, 408, 101, 219,\n       316, 126,  98, 231, 241, 385, 193, 198,  90, 329, 137, 200,  80, 355,\n       317,  33,  64, 213, 368,  49, 435, 134, 254, 330,  60,  39, 218, 109,\n       301, 133, 328, 181, 156, 395,  43, 249,  27, 153, 211, 420, 366, 186,\n       163,  63, 202,  45,  69,  31, 167, 177,  95, 151, 392, 387,  18, 286,\n       102, 290, 195, 428, 403,  97, 406, 412, 374, 263, 371,  41, 212,  52,\n       238, 345,   4, 117, 407,  56, 103, 118, 319,  57, 128, 294,  15, 273,\n       327, 281, 378, 121, 113, 284, 422, 389, 216,  58, 418, 309, 123, 116,\n       222, 122,  11, 361, 179, 255, 239, 225, 381, 424, 343, 287, 341, 184,\n       380, 196, 174, 306, 252, 148, 416, 391,  25, 208,  59, 364])race(race)&lt;U7'Terran' 'Protoss' 'Zerg'array(['Terran', 'Protoss', 'Zerg'], dtype='&lt;U7')Data variables: (4)win(chain, draw, replay)int640 0 1 1 0 0 1 0 ... 1 0 1 0 1 1 1 0array([[[0, 0, 1, ..., 1, 0, 0],\n        [1, 0, 0, ..., 1, 0, 0],\n        [0, 0, 1, ..., 1, 0, 0],\n        ...,\n        [1, 0, 0, ..., 1, 1, 1],\n        [1, 1, 1, ..., 1, 1, 1],\n        [0, 0, 1, ..., 1, 1, 1]],\n\n       [[1, 0, 0, ..., 0, 1, 0],\n        [1, 0, 1, ..., 1, 1, 0],\n        [0, 0, 1, ..., 1, 1, 0],\n        ...,\n        [1, 0, 1, ..., 0, 0, 1],\n        [1, 0, 1, ..., 1, 1, 0],\n        [1, 0, 1, ..., 0, 1, 0]],\n\n       [[1, 1, 0, ..., 0, 0, 0],\n        [1, 0, 1, ..., 1, 1, 0],\n        [0, 1, 0, ..., 1, 1, 1],\n        ...,\n        [1, 0, 0, ..., 1, 1, 0],\n        [0, 1, 1, ..., 1, 1, 0],\n        [1, 0, 0, ..., 1, 1, 0]],\n\n       [[0, 0, 1, ..., 1, 1, 1],\n        [0, 0, 1, ..., 1, 0, 0],\n        [0, 1, 0, ..., 1, 0, 1],\n        ...,\n        [0, 0, 0, ..., 0, 1, 1],\n        [1, 0, 0, ..., 1, 0, 0],\n        [0, 0, 0, ..., 1, 1, 0]]])μ(chain, draw, race)float644.303e+03 3.375e+03 ... 3.955e+03array([[[4302.79553938, 3375.01011989, 4048.86315149],\n        [4323.86457361, 3295.74222841, 3954.05048638],\n        [4239.6739907 , 3448.28894514, 4235.71022401],\n        ...,\n        [4250.95500899, 3592.96680679, 3907.78214913],\n        [4321.36051851, 3657.60296464, 4041.53244852],\n        [4378.58347689, 3680.20202035, 4041.44904971]],\n\n       [[4141.06331072, 3385.94471985, 4135.36324411],\n        [4138.80737046, 3670.4073407 , 4045.58916728],\n        [4194.72695819, 3676.40161761, 3931.11602939],\n        ...,\n        [4066.10358294, 3429.85753425, 4004.41005399],\n        [4049.09082588, 3382.29455772, 3954.15834981],\n        [4422.29348522, 3549.2372973 , 4066.31337586]],\n\n       [[4212.20435522, 3476.71136441, 4104.05998443],\n        [4139.66832504, 3371.93545224, 4198.27086105],\n        [4369.19834033, 3797.3129873 , 3828.50248262],\n        ...,\n        [4320.92217993, 3755.9095065 , 4165.54070368],\n        [4231.08308098, 3713.65002654, 3964.55758405],\n        [4274.41509776, 3407.86141075, 3931.29076179]],\n\n       [[4314.0372303 , 3573.73152574, 4058.10772914],\n        [4114.59770596, 3463.36390945, 4018.74209014],\n        [4464.57525582, 3705.14913054, 4089.96639073],\n        ...,\n        [4504.44403668, 3638.42652115, 3911.16769539],\n        [4256.83526394, 3886.43065114, 3954.52402571],\n        [4256.83526394, 3886.43065114, 3954.52402571]]])σ(chain, draw, race)float64140.4 252.7 183.9 ... 72.88 55.78array([[[140.36011672, 252.69076651, 183.8904596 ],\n        [161.51391431,  82.67823509, 212.55352477],\n        [108.91799094,  66.69192358,  88.42575348],\n        ...,\n        [ 31.69679346, 101.09097511,  32.56956811],\n        [148.25558444,  50.48937963, 103.02389557],\n        [170.53973503,  42.45131739,   7.22075687]],\n\n       [[ 81.39860974,  97.00223551,   5.23367679],\n        [ 28.60567987, 120.58701662, 119.74595091],\n        [ 24.85236716,  47.00875165,  62.47517088],\n        ...,\n        [ 27.08894035, 109.52428738,  89.74107174],\n        [101.36114868,  44.00684249,  63.21585206],\n        [ 45.44140301,  42.24565628,  32.21911083]],\n\n       [[ 85.15403518,  72.24973621, 101.9819129 ],\n        [ 69.33515359,  91.23129494, 140.7847575 ],\n        [ 45.56776301, 101.34687473,  65.85446419],\n        ...,\n        [ 39.67975301,  28.28842643,  52.09586543],\n        [227.54401728,  35.04100696,  46.01622763],\n        [141.84401665, 229.02600795,   5.01932257]],\n\n       [[170.73574582,   6.64000168, 256.76699449],\n        [ 77.7241646 ,  58.94863209,  63.38158179],\n        [ 92.43500404,  57.06879911,  99.61428138],\n        ...,\n        [ 34.21264796,  52.28070628, 126.57472452],\n        [179.13659055,  72.88096661,  55.7761304 ],\n        [179.13659055,  72.88096661,  55.7761304 ]]])winrate(chain, draw, replay)float640.3884 0.244 ... 0.846 0.3018array([[[0.38840135, 0.24397871, 0.67393861, ..., 0.70745598,\n         0.67221636, 0.41190061],\n        [0.30145528, 0.13670806, 0.50855856, ..., 0.84272985,\n         0.71716256, 0.52386025],\n        [0.27877826, 0.26921766, 0.70873675, ..., 0.69606027,\n         0.57933223, 0.54571002],\n        ...,\n        [0.45417135, 0.32451442, 0.57895556, ..., 0.66816823,\n         0.67136651, 0.31963761],\n        [0.45297858, 0.36337639, 0.79449259, ..., 0.81256357,\n         0.70411153, 0.5422807 ],\n        [0.485852  , 0.36502532, 0.65431758, ..., 0.83608882,\n         0.71723646, 0.38343406]],\n\n       [[0.33521265, 0.20123624, 0.7051384 , ..., 0.50383932,\n         0.5404183 , 0.43275976],\n        [0.36348169, 0.3696986 , 0.60762375, ..., 0.61081735,\n         0.61774329, 0.47810857],\n        [0.41910992, 0.3370959 , 0.51683711, ..., 0.63254534,\n         0.63251832, 0.37372364],\n        ...,\n        [0.37155715, 0.18033725, 0.6357291 , ..., 0.52373938,\n         0.57423102, 0.37619612],\n        [0.31930178, 0.181388  , 0.58734606, ..., 0.44433253,\n         0.5247373 , 0.34849273],\n        [0.35606613, 0.28678965, 0.67841064, ..., 0.75105153,\n         0.76967316, 0.41755699]],\n\n       [[0.35077423, 0.20990227, 0.69572652, ..., 0.67417267,\n         0.7237621 , 0.36555445],\n        [0.26014989, 0.20870266, 0.71619203, ..., 0.63645155,\n         0.66626863, 0.49046362],\n        [0.54851081, 0.35499039, 0.51979175, ..., 0.73475424,\n         0.70638391, 0.26082117],\n        ...,\n        [0.54483632, 0.39798298, 0.73459157, ..., 0.72582073,\n         0.72964516, 0.4424017 ],\n        [0.49580194, 0.33216402, 0.61279042, ..., 0.66320592,\n         0.73867943, 0.32134157],\n        [0.45334239, 0.12582742, 0.59087079, ..., 0.72565421,\n         0.69241571, 0.31111641]],\n\n       [[0.38859158, 0.27274337, 0.62694182, ..., 0.71623942,\n         0.71286761, 0.51480671],\n        [0.33542055, 0.20310077, 0.63505131, ..., 0.63727424,\n         0.60658696, 0.38693233],\n        [0.57092553, 0.36838244, 0.65558424, ..., 0.7776215 ,\n         0.81879732, 0.42196867],\n        ...,\n        [0.41075375, 0.29736221, 0.43971402, ..., 0.83029583,\n         0.81630679, 0.33875661],\n        [0.64368704, 0.48558788, 0.62404084, ..., 0.83050419,\n         0.84598416, 0.30180419],\n        [0.64368704, 0.48558788, 0.62404084, ..., 0.83050419,\n         0.84598416, 0.30180419]]])Attributes: (4)created_at :2020-06-20T10:35:37.090348arviz_version :0.8.3inference_library :pymc3inference_library_version :3.9.1\n                      \n                  \n            \n            \n            \n                  \n                  log_likelihood\n                  \n                  \n                      \n                          \n\n\nShow/Hide data repr\n\n\n\n\n\nShow/Hide attributes\n\n\n\n\n\n\n\nxarray.DatasetDimensions:chain: 4draw: 2000replay: 138Coordinates: (3)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])replay(replay)int648 325 54 346 138 ... 25 208 59 364array([  8, 325,  54, 346, 138, 405,  20, 129, 104,  46, 302, 408, 101, 219,\n       316, 126,  98, 231, 241, 385, 193, 198,  90, 329, 137, 200,  80, 355,\n       317,  33,  64, 213, 368,  49, 435, 134, 254, 330,  60,  39, 218, 109,\n       301, 133, 328, 181, 156, 395,  43, 249,  27, 153, 211, 420, 366, 186,\n       163,  63, 202,  45,  69,  31, 167, 177,  95, 151, 392, 387,  18, 286,\n       102, 290, 195, 428, 403,  97, 406, 412, 374, 263, 371,  41, 212,  52,\n       238, 345,   4, 117, 407,  56, 103, 118, 319,  57, 128, 294,  15, 273,\n       327, 281, 378, 121, 113, 284, 422, 389, 216,  58, 418, 309, 123, 116,\n       222, 122,  11, 361, 179, 255, 239, 225, 381, 424, 343, 287, 341, 184,\n       380, 196, 174, 306, 252, 148, 416, 391,  25, 208,  59, 364])Data variables: (1)win(chain, draw, replay)float64-0.9457 -0.2797 ... -0.1673 -1.198array([[[-0.94571606, -0.27968575, -1.1206696 , ..., -0.34607987,\n         -0.39717502, -0.88697319],\n        [-1.1991336 , -0.14700236, -0.7104125 , ..., -0.17110883,\n         -0.33245274, -0.64653033],\n        [-1.27733859, -0.31363962, -1.23352778, ..., -0.36231903,\n         -0.54587916, -0.60566754],\n        ...,\n        [-0.78928073, -0.39232348, -0.86501689, ..., -0.40321529,\n         -0.39844008, -1.14056741],\n        [-0.79191044, -0.45157668, -1.58227317, ..., -0.20756113,\n         -0.35081851, -0.61197151],\n        [-0.72185124, -0.45417016, -1.0622348 , ..., -0.17902043,\n         -0.3323497 , -0.95858761]],\n\n       [[-1.09299019, -0.22469005, -1.2212492 , ..., -0.68549787,\n         -0.61541181, -0.83757254],\n        [-1.01202636, -0.46155716, -0.93553407, ..., -0.4929573 ,\n         -0.48168229, -0.73791744],\n        [-0.86962207, -0.41112495, -0.72740143, ..., -0.45800338,\n         -0.4580461 , -0.9842387 ],\n        ...,\n        [-0.99005259, -0.1988623 , -1.00985747, ..., -0.64676108,\n         -0.5547235 , -0.97764469],\n        [-1.1416186 , -0.20014505, -0.88514597, ..., -0.81118206,\n         -0.64485752, -1.0541379 ],\n        [-1.03263881, -0.33797888, -1.13447981, ..., -0.28628102,\n         -0.26178932, -0.87333424]],\n\n       [[-1.04761249, -0.23559864, -1.18982837, ..., -0.39426901,\n         -0.32329253, -1.00634002],\n        [-1.34649731, -0.23408148, -1.25945741, ..., -0.45184699,\n         -0.40606233, -0.71240417],\n        [-0.60054829, -0.43849007, -0.73353543, ..., -0.30821921,\n         -0.34759641, -1.34392027],\n        ...,\n        [-0.60726986, -0.50746956, -1.3264854 , ..., -0.32045223,\n         -0.31519695, -0.81553699],\n        [-0.70157875, -0.40371268, -0.94878918, ..., -0.41066975,\n         -0.30289124, -1.13525063],\n        [-0.7911076 , -0.13447746, -0.89372426, ..., -0.32068168,\n         -0.36756877, -1.16758814]],\n\n       [[-0.9452264 , -0.31847587, -0.98602091, ..., -0.33374078,\n         -0.33845955, -0.66396377],\n        [-1.09237016, -0.22702705, -1.00799851, ..., -0.45055519,\n         -0.49990719, -0.94950547],\n        [-0.56049651, -0.45947119, -1.06590574, ..., -0.25151537,\n         -0.1999187 , -0.86282421],\n        ...,\n        [-0.88976139, -0.35291376, -0.57930794, ..., -0.18597321,\n         -0.20296503, -1.08247338],\n        [-0.44054263, -0.66473054, -0.97827477, ..., -0.1857223 ,\n         -0.16725464, -1.19797685],\n        [-0.44054263, -0.66473054, -0.97827477, ..., -0.1857223 ,\n         -0.16725464, -1.19797685]]])Attributes: (4)created_at :2020-06-20T10:35:37.087198arviz_version :0.8.3inference_library :pymc3inference_library_version :3.9.1\n                      \n                  \n            \n            \n            \n                  \n                  sample_stats\n                  \n                  \n                      \n                          \n\n\nShow/Hide data repr\n\n\n\n\n\nShow/Hide attributes\n\n\n\n\n\n\n\nxarray.DatasetDimensions:chain: 4draw: 2000Coordinates: (2)chain(chain)int640 1 2 3array([0, 1, 2, 3])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])Data variables: (10)step_size_bar(chain, draw)float640.4374 0.4374 ... 0.4903 0.4903array([[0.4374115 , 0.4374115 , 0.4374115 , ..., 0.4374115 , 0.4374115 ,\n        0.4374115 ],\n       [0.45402308, 0.45402308, 0.45402308, ..., 0.45402308, 0.45402308,\n        0.45402308],\n       [0.41388167, 0.41388167, 0.41388167, ..., 0.41388167, 0.41388167,\n        0.41388167],\n       [0.49034327, 0.49034327, 0.49034327, ..., 0.49034327, 0.49034327,\n        0.49034327]])energy_error(chain, draw)float64-1.016 0.3132 ... 0.6163 0.0array([[-1.01642236,  0.31318099, -0.14019916, ..., -0.00712864,\n        -0.41741377, -0.08385882],\n       [-0.38042493, -1.50079731, -0.16318778, ..., -0.24735973,\n        -0.2886005 , -0.17678952],\n       [-2.01224887, -0.89066887,  0.47713108, ..., -1.70039887,\n         0.31851951,  0.11482341],\n       [ 1.43938357,  0.2229521 ,  0.32288281, ..., -0.04535445,\n         0.61629039,  0.        ]])step_size(chain, draw)float640.4386 0.4386 ... 0.425 0.425array([[0.43855251, 0.43855251, 0.43855251, ..., 0.43855251, 0.43855251,\n        0.43855251],\n       [0.48249608, 0.48249608, 0.48249608, ..., 0.48249608, 0.48249608,\n        0.48249608],\n       [0.44391835, 0.44391835, 0.44391835, ..., 0.44391835, 0.44391835,\n        0.44391835],\n       [0.42501071, 0.42501071, 0.42501071, ..., 0.42501071, 0.42501071,\n        0.42501071]])max_energy_error(chain, draw)float64-1.287 1.11 -0.4291 ... 1.535 5.196array([[-1.28737009,  1.11007453, -0.42910467, ...,  0.91347544,\n         0.52800764,  0.47859559],\n       [-0.48591337, -1.50079731,  2.16250099, ..., -0.24735973,\n         3.77015272, -0.76572754],\n       [-2.328056  ,  1.01810228,  0.80442549, ..., -1.70039887,\n        -0.55804924, 74.25084326],\n       [38.71084628,  1.20965313,  0.49249056, ...,  1.35746487,\n         1.53478843,  5.19565611]])diverging(chain, draw)boolFalse False False ... False Falsearray([[False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False],\n       [False, False, False, ..., False, False, False]])depth(chain, draw)int644 3 3 4 4 3 3 3 ... 3 3 3 3 3 3 3 3array([[4, 3, 3, ..., 3, 3, 4],\n       [3, 3, 3, ..., 3, 3, 3],\n       [4, 3, 3, ..., 4, 4, 4],\n       [3, 3, 3, ..., 3, 3, 3]])mean_tree_accept(chain, draw)float640.9054 0.7214 ... 0.4055 0.07633array([[0.90541692, 0.72142566, 0.93134252, ..., 0.85369685, 0.9554831 ,\n        0.86302654],\n       [0.99759711, 0.99963932, 0.88165678, ..., 0.96310244, 0.78523323,\n        0.97673572],\n       [0.99816125, 0.88683585, 0.75217549, ..., 0.96863687, 0.92512264,\n        0.86677524],\n       [0.40260816, 0.83998264, 0.84497975, ..., 0.76832421, 0.4055396 ,\n        0.07633182]])energy(chain, draw)float64356.5 384.7 373.6 ... 368.5 373.4array([[356.54520829, 384.72052031, 373.61948961, ..., 390.95997962,\n        395.89754948, 395.0117275 ],\n       [410.94614041, 383.5146222 , 361.83684381, ..., 373.49242948,\n        364.71453224, 360.19568382],\n       [391.28602749, 376.75745726, 387.21260602, ..., 390.37738103,\n        377.07691302, 389.32502442],\n       [358.39179418, 363.878841  , 377.07222589, ..., 378.63812646,\n        368.46859802, 373.42582631]])lp(chain, draw)float64-302.1 -305.8 ... -302.2 -302.2array([[-302.08558028, -305.77270943, -300.43477915, ..., -320.70266421,\n        -315.38015305, -311.86956419],\n       [-324.9312429 , -298.99886287, -298.64220017, ..., -301.73942136,\n        -301.70998423, -295.83681186],\n       [-310.88193965, -301.57205804, -313.61330102, ..., -309.33294253,\n        -310.28663405, -315.73379124],\n       [-294.91496388, -306.34122894, -309.38563566, ..., -304.01227822,\n        -302.20722956, -302.20722956]])tree_size(chain, draw)float6415.0 7.0 7.0 15.0 ... 7.0 7.0 7.0array([[15.,  7.,  7., ...,  7.,  7., 15.],\n       [ 7.,  7.,  7., ...,  7.,  7.,  7.],\n       [15.,  7.,  7., ..., 15., 15., 15.],\n       [ 7.,  7.,  7., ...,  7.,  7.,  7.]])Attributes: (6)created_at :2020-06-20T10:35:36.562793arviz_version :0.8.3inference_library :pymc3inference_library_version :3.9.1sampling_time :18.211029291152954tuning_steps :2000\n                      \n                  \n            \n            \n            \n                  \n                  prior\n                  \n                  \n                      \n                          \n\n\nShow/Hide data repr\n\n\n\n\n\nShow/Hide attributes\n\n\n\n\n\n\n\nxarray.DatasetDimensions:chain: 1draw: 2000race: 3replay: 138Coordinates: (4)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])race(race)&lt;U7'Terran' 'Protoss' 'Zerg'array(['Terran', 'Protoss', 'Zerg'], dtype='&lt;U7')replay(replay)int648 325 54 346 138 ... 25 208 59 364array([  8, 325,  54, 346, 138, 405,  20, 129, 104,  46, 302, 408, 101, 219,\n       316, 126,  98, 231, 241, 385, 193, 198,  90, 329, 137, 200,  80, 355,\n       317,  33,  64, 213, 368,  49, 435, 134, 254, 330,  60,  39, 218, 109,\n       301, 133, 328, 181, 156, 395,  43, 249,  27, 153, 211, 420, 366, 186,\n       163,  63, 202,  45,  69,  31, 167, 177,  95, 151, 392, 387,  18, 286,\n       102, 290, 195, 428, 403,  97, 406, 412, 374, 263, 371,  41, 212,  52,\n       238, 345,   4, 117, 407,  56, 103, 118, 319,  57, 128, 294,  15, 273,\n       327, 281, 378, 121, 113, 284, 422, 389, 216,  58, 418, 309, 123, 116,\n       222, 122,  11, 361, 179, 255, 239, 225, 381, 424, 343, 287, 341, 184,\n       380, 196, 174, 306, 252, 148, 416, 391,  25, 208,  59, 364])Data variables: (3)μ(chain, draw, race)float644.487e+03 3.816e+03 ... 4.079e+03array([[[4487.3036091 , 3816.4730759 , 3841.54847432],\n        [3678.10941335, 4259.6222888 , 3309.53839094],\n        [4523.44352926, 3771.63792973, 4095.71172882],\n        ...,\n        [3793.7352304 , 3936.80991215, 3897.05804332],\n        [4287.27113474, 4067.38137671, 3856.85673442],\n        [3792.63645476, 3763.38063103, 4079.25539045]]])σ(chain, draw, race)float6410.45 53.54 169.3 ... 210.5 146.2array([[[ 10.45030045,  53.53690568, 169.32540234],\n        [ 23.57617238,  10.93212404,  83.18141646],\n        [125.6331917 ,  94.68059879, 168.58509729],\n        ...,\n        [ 54.01739286, 112.35908554, 107.02562404],\n        [144.26356103,  77.82838645,   9.93539497],\n        [ 81.81960096, 210.54208204, 146.24259794]]])winrate(chain, draw, replay)float640.4452 0.4443 ... 0.4049 0.3675array([[[0.44516586, 0.44432904, 0.49543905, ..., 0.79673329,\n         0.80140205, 0.28212387],\n        [0.7972168 , 0.68737573, 0.21071926, ..., 0.31760374,\n         0.3228919 , 0.08274033],\n        [0.46947403, 0.43159697, 0.78947532, ..., 0.82439068,\n         0.80940984, 0.32718542],\n        ...,\n        [0.74439685, 0.57396175, 0.48594935, ..., 0.37104134,\n         0.37754386, 0.3333573 ],\n        [0.70701564, 0.56655281, 0.54098671, ..., 0.57389202,\n         0.76269576, 0.26823939],\n        [0.59530994, 0.28697282, 0.72372083, ..., 0.36763267,\n         0.40492739, 0.36749525]]])Attributes: (4)created_at :2020-06-20T10:35:37.093692arviz_version :0.8.3inference_library :pymc3inference_library_version :3.9.1\n                      \n                  \n            \n            \n            \n                  \n                  prior_predictive\n                  \n                  \n                      \n                          \n\n\nShow/Hide data repr\n\n\n\n\n\nShow/Hide attributes\n\n\n\n\n\n\n\nxarray.DatasetDimensions:chain: 1draw: 2000replay: 138Coordinates: (3)chain(chain)int640array([0])draw(draw)int640 1 2 3 4 ... 1996 1997 1998 1999array([   0,    1,    2, ..., 1997, 1998, 1999])replay(replay)int648 325 54 346 138 ... 25 208 59 364array([  8, 325,  54, 346, 138, 405,  20, 129, 104,  46, 302, 408, 101, 219,\n       316, 126,  98, 231, 241, 385, 193, 198,  90, 329, 137, 200,  80, 355,\n       317,  33,  64, 213, 368,  49, 435, 134, 254, 330,  60,  39, 218, 109,\n       301, 133, 328, 181, 156, 395,  43, 249,  27, 153, 211, 420, 366, 186,\n       163,  63, 202,  45,  69,  31, 167, 177,  95, 151, 392, 387,  18, 286,\n       102, 290, 195, 428, 403,  97, 406, 412, 374, 263, 371,  41, 212,  52,\n       238, 345,   4, 117, 407,  56, 103, 118, 319,  57, 128, 294,  15, 273,\n       327, 281, 378, 121, 113, 284, 422, 389, 216,  58, 418, 309, 123, 116,\n       222, 122,  11, 361, 179, 255, 239, 225, 381, 424, 343, 287, 341, 184,\n       380, 196, 174, 306, 252, 148, 416, 391,  25, 208,  59, 364])Data variables: (1)win(chain, draw, replay)int641 1 0 0 1 1 1 0 ... 0 0 0 1 1 0 0 0array([[[1, 1, 0, ..., 1, 1, 0],\n        [1, 1, 0, ..., 0, 0, 0],\n        [1, 0, 0, ..., 1, 1, 0],\n        ...,\n        [1, 1, 0, ..., 0, 0, 0],\n        [1, 1, 0, ..., 1, 0, 0],\n        [0, 0, 0, ..., 0, 0, 0]]])Attributes: (4)created_at :2020-06-20T10:35:37.095318arviz_version :0.8.3inference_library :pymc3inference_library_version :3.9.1\n                      \n                  \n            \n            \n            \n                  \n                  observed_data\n                  \n                  \n                      \n                          \n\n\nShow/Hide data repr\n\n\n\n\n\nShow/Hide attributes\n\n\n\n\n\n\n\nxarray.DatasetDimensions:replay: 138Coordinates: (1)replay(replay)int648 325 54 346 138 ... 25 208 59 364array([  8, 325,  54, 346, 138, 405,  20, 129, 104,  46, 302, 408, 101, 219,\n       316, 126,  98, 231, 241, 385, 193, 198,  90, 329, 137, 200,  80, 355,\n       317,  33,  64, 213, 368,  49, 435, 134, 254, 330,  60,  39, 218, 109,\n       301, 133, 328, 181, 156, 395,  43, 249,  27, 153, 211, 420, 366, 186,\n       163,  63, 202,  45,  69,  31, 167, 177,  95, 151, 392, 387,  18, 286,\n       102, 290, 195, 428, 403,  97, 406, 412, 374, 263, 371,  41, 212,  52,\n       238, 345,   4, 117, 407,  56, 103, 118, 319,  57, 128, 294,  15, 273,\n       327, 281, 378, 121, 113, 284, 422, 389, 216,  58, 418, 309, 123, 116,\n       222, 122,  11, 361, 179, 255, 239, 225, 381, 424, 343, 287, 341, 184,\n       380, 196, 174, 306, 252, 148, 416, 391,  25, 208,  59, 364])Data variables: (1)win(replay)float641.0 0.0 0.0 1.0 ... 1.0 1.0 1.0 1.0array([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0.,\n       0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1.,\n       0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n       1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.,\n       0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n       1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1.,\n       0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1.,\n       1., 1.])Attributes: (4)created_at :2020-06-20T10:35:37.096098arviz_version :0.8.3inference_library :pymc3inference_library_version :3.9.1\n                      \n                  \n            \n            \n            \n                  \n                  constant_data\n                  \n                  \n                      \n                          \n\n\nShow/Hide data repr\n\n\n\n\n\nShow/Hide attributes\n\n\n\n\n\n\n\nxarray.DatasetDimensions:replay: 138Coordinates: (1)replay(replay)int648 325 54 346 138 ... 25 208 59 364array([  8, 325,  54, 346, 138, 405,  20, 129, 104,  46, 302, 408, 101, 219,\n       316, 126,  98, 231, 241, 385, 193, 198,  90, 329, 137, 200,  80, 355,\n       317,  33,  64, 213, 368,  49, 435, 134, 254, 330,  60,  39, 218, 109,\n       301, 133, 328, 181, 156, 395,  43, 249,  27, 153, 211, 420, 366, 186,\n       163,  63, 202,  45,  69,  31, 167, 177,  95, 151, 392, 387,  18, 286,\n       102, 290, 195, 428, 403,  97, 406, 412, 374, 263, 371,  41, 212,  52,\n       238, 345,   4, 117, 407,  56, 103, 118, 319,  57, 128, 294,  15, 273,\n       327, 281, 378, 121, 113, 284, 422, 389, 216,  58, 418, 309, 123, 116,\n       222, 122,  11, 361, 179, 255, 239, 225, 381, 424, 343, 287, 341, 184,\n       380, 196, 174, 306, 252, 148, 416, 391,  25, 208,  59, 364])Data variables: (2)enemy_race(replay)int321 1 2 2 1 1 2 1 ... 2 2 0 2 2 0 0 2array([1, 1, 2, 2, 1, 1, 2, 1, 1, 2, 2, 2, 1, 1, 0, 2, 2, 0, 0, 2, 0, 2,\n       2, 1, 1, 2, 2, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2, 1, 1, 2, 2, 2, 1, 0,\n       0, 1, 0, 2, 2, 2, 1, 0, 2, 1, 0, 2, 1, 0, 1, 1, 2, 2, 2, 2, 1, 2,\n       1, 2, 0, 0, 0, 1, 0, 2, 2, 0, 0, 0, 0, 1, 0, 1, 2, 0, 1, 2, 2, 1,\n       2, 1, 0, 2, 2, 2, 1, 0, 0, 2, 1, 1, 1, 1, 0, 0, 0, 1, 0, 2, 0, 2,\n       0, 1, 0, 0, 2, 0, 0, 0, 2, 1, 0, 1, 1, 0, 2, 1, 0, 2, 0, 1, 2, 2,\n       0, 2, 2, 0, 0, 2], dtype=int32)enemy_mmr(replay)int323748 3946 3802 ... 3977 3937 4240array([3748, 3946, 3802, 3816, 3701, 3861, 3907, 4066, 3757, 4350, 3964,\n       3894, 3932, 3952, 3955, 3912, 3744, 3492, 4642, 3756, 3664, 3769,\n       3871, 3631, 3631, 3685, 3626, 3647, 3626, 3736, 3784, 3856, 3866,\n       3847, 3883, 3761, 3686, 3827, 3771, 3752, 3803, 3922, 3888, 3900,\n       3962, 3992, 3798, 3797, 3790, 3966, 3881, 3823, 3904, 3920, 3907,\n       3961, 3874, 3570, 4359, 4009, 3859, 3988, 3885, 4020, 3935, 3908,\n       4596, 4851, 3941, 3808, 3942, 3854, 3956, 3976, 3998, 3953, 4068,\n       3960, 4016, 4055, 3925, 4104, 3989, 4001, 4237, 4161, 4024, 4108,\n       4125, 4028, 4012, 3992, 4144, 4029, 4051, 4131, 4407, 4037, 3965,\n       3944, 4169, 3882, 4018, 4066, 3828, 3957, 4181, 4160, 3886, 3963,\n       3875, 3956, 3976, 3953, 3757, 4007, 3911, 3967, 4935, 3898, 3880,\n       3927, 3948, 3833, 3894, 3782, 3786, 3845, 3863, 3966, 3864, 3843,\n       3840, 4033, 3914, 3977, 3937, 4240], dtype=int32)Attributes: (4)created_at :2020-06-20T10:35:37.097169arviz_version :0.8.3inference_library :pymc3inference_library_version :3.9.1\n                      \n                  \n            \n            \n              \n            \n            \n\n\nThat InferenceData ArviZ object lets us see everything the model picked up. It’s really, really neat.\nWe had a few divergences; not too many, though Let’s take a look at how the sampling went:\n\nvar_names = \"μ σ\".split()\naz.plot_trace(output, var_names=var_names);\n\n\n\n\n\n\n\n\n\naz.summary(output, var_names = var_names)\n\n\n\n\n\n\n\n\nmean\nsd\nhdi_3%\nhdi_97%\nmcse_mean\nmcse_sd\ness_mean\ness_sd\ness_bulk\ness_tail\nr_hat\n\n\n\n\nμ[0]\n4275.797\n116.495\n4066.919\n4504.642\n1.418\n1.005\n6750.0\n6715.0\n6767.0\n4891.0\n1.0\n\n\nμ[1]\n3625.102\n122.610\n3403.476\n3865.610\n1.407\n0.995\n7597.0\n7597.0\n7622.0\n4916.0\n1.0\n\n\nμ[2]\n3984.044\n105.523\n3785.491\n4182.183\n1.353\n0.957\n6082.0\n6082.0\n6082.0\n4802.0\n1.0\n\n\nσ[0]\n78.531\n59.788\n0.019\n184.598\n0.819\n0.692\n5331.0\n3737.0\n4413.0\n3198.0\n1.0\n\n\nσ[1]\n76.320\n58.370\n0.030\n180.889\n0.773\n0.547\n5699.0\n5699.0\n4607.0\n3176.0\n1.0\n\n\nσ[2]\n85.882\n64.475\n0.266\n201.367\n0.970\n0.686\n4420.0\n4420.0\n3717.0\n3351.0\n1.0\n\n\n\n\n\n\n\n\naz.plot_pair(output, var_names='μ', divergences=True);\n\n\n\n\n\n\n\n\n\naz.plot_pair(output, var_names='σ', divergences=True);"
  },
  {
    "objectID": "posts/utterances.html",
    "href": "posts/utterances.html",
    "title": "Now with comments through utteranc.es!",
    "section": "",
    "text": "As promised, I’ve switched the site’s comment system from Disqus to utteranc.es. It ended up being pretty simple to do in Nikola. If I’m correct about how this is supposed to work, comments on the site will now appear as issues on the site’s repo. Of course, I would love to test out how this works :)\n\nThe reason I switched away from Disqus was their pretty horrid privacy record, for which a quick websearch will provide ample sources. Utterances is cleverly built around GitHub issues, and while it does require a GitHub account, I think it really is the lesser evil in this case.\nAdmittedly, I do still feel a little queasy about this. While GitHub does provide repositories for various, definitely programming lite repositories, I can’t help but think that this takes advantage of a resource given freely in ways that are not entirely as-intended-by-providers.\nTake Travis CI, for example. It was a great service, the first project I’m aware of that worked with a “completely free online testing, for every commit, for every open source project out there” model. And, well, they got bought out a few years ago; last year was the year when the new owners (as I suspect) tried to plug the money sink and introduce crippling limits, and a lot of open source projects had to migrate away. I don’t know of a single one that still uses Travis, and I fully expect the paid version to go out of business soon.\nTragedy of the commons, a little?\nBut you can’t solve all problems everywhere at the same time, or at least I haven’t yet found a way to. So, for now, utteranc.es it is!"
  },
  {
    "objectID": "posts/bayes-sc2-1.html",
    "href": "posts/bayes-sc2-1.html",
    "title": "Bayesian modeling of StarCraft II ladder performance",
    "section": "",
    "text": "I’ve been in a bit of a pickle recently. I really need to figure out Bayesian inference as practice for my masters’ thesis. I’ve been wondering, what kind of cool project - hopefully with my own data - could I make?, I thought as I fired up StarCraft II in the evening, as I usually do to unwind nowadays. What kind of fun use of PyMC3, the neat Python library for probabilistic programming, could I showcase?, I wondered as I watched my ladder ratings fall from the distraction. What kind of useful knowledge could I try to acquire using it?, I thought, watching my game performance fluctuate over the course of months.\nAnd then it hit me.\nIn this post, I’m going to use PyMC3 to analyse my 2019 StarCraft II ladder games. In particular, I’m going to look at the relation between MMR - MatchMaking Rating, a metric of ladder performance - mine and that of my enemies - and what it can tell us about my win chances."
  },
  {
    "objectID": "posts/bayes-sc2-1.html#a-few-facts-about-starcraft",
    "href": "posts/bayes-sc2-1.html#a-few-facts-about-starcraft",
    "title": "Bayesian modeling of StarCraft II ladder performance",
    "section": "A few facts about StarCraft",
    "text": "A few facts about StarCraft\n\nSC2 is a fast-paced real-time action/strategy game played competitively all over the world;\nEach game of SC2 lasts about 10-30 minutes;\nThere are three races in SC2, which means factions, each with a completely different playstyle and toolset; I play Protoss (the advanced space aliens) and the other two suck are Terran (scrappy future humans) and Zerg (hivemind insectoid aliens).\n\nThere is no tribal animosity between players of the races in the community whatsoever.\n\nEach 1v1 game pits two randomly selected players of similar MMR. Better players have higher MMR, and it’s used to find worthy (adequate) opponents for you. A Protoss player has three different possible matchups - Protoss vs Protoss (PvP), PvT and PvZ."
  },
  {
    "objectID": "posts/bayes-sc2-1.html#a-few-facts-about-bayesian-inference",
    "href": "posts/bayes-sc2-1.html#a-few-facts-about-bayesian-inference",
    "title": "Bayesian modeling of StarCraft II ladder performance",
    "section": "A few facts about Bayesian inference",
    "text": "A few facts about Bayesian inference\n\nit’s an alternate, computationally intensive approach to statistics (of which you probably know frequentist statistics)\nit’s a really neat tool to formulate complex models of processes occuring between entities\nit can let you infer knowledge about quantities not directly included in your data at all (so-called “latent” quantities)\nit can combine data with your initial assumptions (“priors”) or initial beliefs about quantities in your system\n\nthis means you need to explicitly state what you believe about the data first\n\nit returns probability distributions for your parameters, rather than simple point estimates like means or variances\n\nthis makes handling asymmetric uncertainties and error bars much easier\n\nit’s a great framework for learning new knowledge from data, as I’ll try to show\n\nIn this post, we’re going to use my own dataset of ladder replays. The motivation is this: there are days when I play terribly and there are days when I play my heart out. It does, however, feel like my performance fluctuates a lot. I thought I could use Bayesian modelling to learn something about these fluctuations.\nI was going to have an example of pulling this data using ZephyrBlu’s replay parser library and an unreleased custom wrapper I have for that. However, since I’m getting a ton of warnings that would distract from the main ideas, in the name of simplicity I’ll just post the parsed version on GitHub. I’ll come back to parsing those once I finish that library of mine.\n\nimport pandas as pd\n\ndf = pd.read_csv(\"https://raw.githubusercontent.com/StanczakDominik/stanczakdominik.github.io/src/files/replays.csv\", index_col=0)\ndf['time_played_at'] = pd.to_datetime(df.time_played_at)\ndf\n\n\n\n\n\n\n\n\ntime_played_at\nwin\nrace\nenemy_race\nmmr\nmmr_diff\nenemy_nickame\n\n\n\n\n0\n2020-05-27 10:32:29+00:00\nTrue\nProtoss\nTerran\n4004\n-169\ngiletjaune\n\n\n1\n2020-06-09 17:11:15+00:00\nFalse\nProtoss\nZerg\n4186\n39\ndjakette\n\n\n2\n2020-02-02 17:27:27+00:00\nTrue\nProtoss\nTerran\n3971\n58\nSyocto\n\n\n3\n2019-12-20 18:53:00+00:00\nTrue\nZerg\nTerran\n2984\n-106\nJason\n\n\n4\n2019-12-09 20:36:21+00:00\nTrue\nProtoss\nZerg\n4015\n-9\n&lt;OGCOСK&gt;&lt;sp/&gt;ShushYo\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n432\n2019-11-04 20:53:20+00:00\nFalse\nProtoss\nTerran\n3800\n-83\n&lt;MiClan&gt;&lt;sp/&gt;MiSHANYA\n\n\n433\n2020-05-04 12:43:06+00:00\nTrue\nProtoss\nTerran\n3926\n95\nStaMinA\n\n\n434\n2020-02-02 17:15:06+00:00\nFalse\nProtoss\nZerg\n4012\n-80\n&lt;0mg&gt;&lt;sp/&gt;Sroljo\n\n\n435\n2020-04-19 11:48:32+00:00\nTrue\nProtoss\nZerg\n0\n0\nshadowofmich\n\n\n436\n2020-04-30 18:34:01+00:00\nTrue\nProtoss\nTerran\n3964\n-91\n&lt;BRs&gt;&lt;sp/&gt;GoodFellas\n\n\n\n\n437 rows × 7 columns"
  },
  {
    "objectID": "posts/bayes-sc2-1.html#digression---race-separated-data",
    "href": "posts/bayes-sc2-1.html#digression---race-separated-data",
    "title": "Bayesian modeling of StarCraft II ladder performance",
    "section": "Digression - race separated data",
    "text": "Digression - race separated data\nAltair makes it very easy to look at data from each of the matchups in the dataset:\n\naltair.Chart(data2019).mark_circle().encode(\n    x=altair.X('enemy_mmr', scale=altair.Scale(zero=False)),\n    y=altair.Y('expected_winrate'),\n    color='win',\n    facet='enemy_race',\n).interactive()\n\n\n\n\n\n\nThere are a few things we can already say from this:\n\nEnemy Protosses at 4k MMR were already a large challenge, as I won no games whatsoever against those. This is, in fact, what inspired me to write this post!\nPvT was my best matchup - there are the most wins there.\nPvZ games were mostly even, though there are a good amount of lost games that I should probably have won! This seems to point to MMR not being enough to estimate my winrate properly - for example, strategical variation in Zerg play. I’m still sort of confused on what to do with a very defensive, lategame-oriented Zerg player.\n\nLet’s see if our estimates were right:\n\ndata2019.groupby('enemy_race').win.mean()\n\nenemy_race\nProtoss    0.309524\nTerran     0.729167\nZerg       0.519231\nName: win, dtype: float64\n\n\nOuch. That PvP still hurts. Well, enough sulking! We’ll get back to this point later - promise - but for nowo, let’s get right to"
  },
  {
    "objectID": "posts/scipy-ivp-makeshift-poincare-sections-of-the-rossler-attractor.html",
    "href": "posts/scipy-ivp-makeshift-poincare-sections-of-the-rossler-attractor.html",
    "title": "scipy.integrate.solve_ivp and makeshift Poincaré sections of the Rossler attractor",
    "section": "",
    "text": "I’ve just stumbled upon a relatively recent addition to scipy: integrate.solve_ivp, which looks amazing for the simulation of dynamical systems and solving equations where you’d like to detect discrete events occuring (say, collisions). Let’s a look at what it’s all about, and then use it to simulate the Rossler attractor!\n\nrossler_attractor\n\n\n\n\n\n\n\n\n\nAs usual, we start with the imports:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy.integrate import solve_ivp\nplt.rcParams['figure.figsize'] = [11, 6]  # for larger pictures\n\nFirst let’s remake an example from solve_ivp’s documentation - an upward cannon shot, for which we’ll want to get the trajectory from the moment of the launch until the cannonball hits the ground.\nWe define a derivative for our ODE (Newton’s law with \\(g = -10 \\text{m}/\\text{s}^2\\) for simplicity):\n\ndef upward_cannon(t, y):\n        return [y[1],  # the derivative of the position is the velocity\n            -10    # the derivative of the velocity is the acceleration ~ -10 m/s^2\n           ] \n\nWe then define an event - a function that returns 0 once the event we’re looking for has occured (in this case, once \\(y = 0\\)). This is pretty simple:\n\ndef hit_ground(t, y):\n    return y[0]\n\nThis is where the cool part begins. We want to specify that the iteration should stop once the ground is hit, and we want to additionally specify that the function must go from positive to negative - otherwise, the start (from y = 0) would count! This is done, as per SciPy docs, via monkey patching and I won’t deny I giggled when I figured this out:\n\nhit_ground.terminal = True\nhit_ground.direction = -1\nhit_ground, hit_ground.terminal, hit_ground.direction\n\n(&lt;function __main__.hit_ground(t, y)&gt;, True, -1)\n\n\nLet’s run the function now:\n\nsol = solve_ivp(upward_cannon,    # derivative function\n                [0, 10],          # time span for integration - make this generous\n                [0, 10],          # initial condition\n                events=hit_ground)\nsol\n\n  message: 'A termination event occurred.'\n     nfev: 38\n     njev: 0\n      nlu: 0\n      sol: None\n   status: 1\n  success: True\n        t: array([0.00000000e+00, 9.99900005e-05, 1.09989001e-03, 1.10988901e-02,\n       1.11088891e-01, 1.11098890e+00, 2.00000000e+00])\n t_events: [array([2.])]\n        y: array([[ 0.00000000e+00,  9.99850015e-04,  1.09928513e-02,\n         1.10372974e-01,  1.04918520e+00,  4.93840733e+00,\n         1.77635684e-15],\n       [ 1.00000000e+01,  9.99900010e+00,  9.98900110e+00,\n         9.88901110e+00,  8.88911109e+00, -1.10988896e+00,\n        -1.00000000e+01]])\n\n\nAll right, the cannonball’s flight was successfully terminated.\n\nplt.plot(sol.t, sol.y[0], label=\"vertical position\")\nplt.plot(sol.t, sol.y[1], label=\"vertical velocity\")\nplt.legend();\n\n\n\n\n\n\n\n\nGosh, ain’t that plot ugly, though! We can fix it with the t_eval parameter to solve_ivp, which forces calculations of the function at the prescribed times. We’ll also add another event that calculates when the cannonball stops in mid-air:\n\ndef zero_velocity(t, y):\n    return y[1]\n\nt = np.linspace(0, 10)\nsol_t_eval = solve_ivp(upward_cannon,\n                       [0, 10],\n                       [0, 10],\n                       events=[hit_ground, zero_velocity],\n                       t_eval=t)\nplt.plot(sol_t_eval.t, sol_t_eval.y[0])\nplt.plot(sol_t_eval.t, sol_t_eval.y[1])\nsol_t_eval.t_events\n\n\n\n\n\n\n\n\nOf course, the first array of event times corresponds to hitting-the-ground-with-a-thud, and the second corresponds to stopping in midair. Notice how the trajectory seems to stop early, though:\n\nsol_t_eval.t.max()\n\n1.836734693877551\n\n\nBut that’s just a case of having few (50 by default) points in our linspace:\n\nsol_t_eval_fixed = solve_ivp(upward_cannon,\n                           [0, 10],\n                           [0, 10],\n                           events=[hit_ground, zero_velocity],\n                           t_eval=np.linspace(0, 10, 200))\nplt.plot(sol_t_eval_fixed.t, sol_t_eval_fixed.y[0])\nplt.plot(sol_t_eval_fixed.t, sol_t_eval_fixed.y[1])\nsol_t_eval_fixed.t.max()\n\n1.9597989949748744\n\n\n\n\n\n\n\n\n\nMuch better! And now, let’s abandon this toy example and do something I’ve been putting off for a good while, now:\n\nPoincaré section of the Rossler attractor\nAs seen on Wikipedia (and it is amazing that if you go to edit, you can just copypaste the LaTeX source), the Rossler system is defined by the following set of differential equations: \\[ \\begin{cases}  \\frac{dx}{dt} = -y - z \\\\ \\frac{dy}{dt} = x + ay \\\\ \\frac{dz}{dt} = b + z(x-c) \\end{cases} \\]\nWe’ll take sample parameters from Wikipedia and set up the derivative function:\n\na=0.1\nb=0.1\nc=14\nx_section = 10\n\ndef rossler(t, vector):\n    x, y, z = vector   # for readability\n    return [-y -z,\n            x + a * y,\n            b + z * (x - c),\n           ]\n\nWhat we want to do here is figure out a way to take a Poincaré (with whom I have a love-hate relationship, the love coming from his achievements and the hate coming from the é I have had to paste multiple times here) section. The way I think about Poincaré sections is that they’re simply intersections of the trajectory of a system (the \\(\\vec{y}(t) = (x(t), y(t), z(t)\\) curve) with some particular surface. Wikipedia has a more formal definition of those.\nWe’ll define our Poincaré section as \\(x = 10, \\dot{x} &lt; 0\\). I picked \\(x=10\\) simply because the plots came out nicely that way.\nEDIT: THE NEXT PART HAS TURNED OUT TO BE WILDLY INCORRECT, PLEASE TAKE NOTE THAT WHAT I SAID HERE WAS WRONG. THE PROPER SOLUTION IS BELOW\nUnfortunately, solve_ivp at the time of writing does not seem to support saving values of the vector \\(\\vec{y} = (x, y, z)\\) at event times - it simply saves the times themselves. That’s probably a future PR - but for now, we can hack that ourselves, given that we’re just interested in collecting a bunch of points at \\(x \\approx 10\\):\n\nevents = []\ndef poincare(t, vector):\n    x = vector[0]\n    if np.isclose(x, x_section, rtol=1e-9, atol=1e-12):\n        events.append((t, vector))\n    return x - x_section\npoincare.direction = -1    # decreasing x\n\nsol = solve_ivp(rossler,\n               [0, 500000],\n               [-0.2, 0.2, 5.2],\n               events=poincare)\n\n\nlen(sol.t_events[0])/len(events)\n\n0.4678223495702006\n\n\nTurns out we got about twice as many section events as we should have. This is reasonable - as the solver of the equation defined by poincare(x,y,z) = 0 equation creeps closer and closer to a point where it’s exactly satisfied, poincare is evaluated multiple times - and events gets multiple entries from each actual P-section. No matter! For most of our plots, this is going to be fine.\nWe’ll plot the whole trajectory we got in glorious 3D, with the Poincaré section in orange:\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.plot(sol.y[0], sol.y[1], sol.y[2], ',')\n\nvectors = np.array([e[1] for e in events])\nt = np.array([e[0] for e in events])\nx, y, z = vectors.T\nax.plot(x, y, z, \".\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"z\");\n\n\n\n\n\n\n\n\nWe’ll take another picture from a different angle to show that this is indeed the Poincaré section we were looking for:\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.view_init(30, -90)\nax.plot(sol.y[0], sol.y[1], sol.y[2], ',')\nax.plot(x, y, z, \".\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"z\");\n\n\n\n\n\n\n\n\nWe can also plot simply \\(y(t_n)\\) for each passage through the \\(x = 10\\) plane (in the correct direction):\n\nplt.plot(t, y, \",\")\nplt.xlabel(\"t\")\nplt.ylabel(\"y\");\n\n\n\n\n\n\n\n\nWhich is of course rather unenlightening. What we would be ultimately interested in is a Poincaré map - a map of \\(y(t_{n+1})\\) plotted against \\(y(t_n)\\). This has many useful properties that I’m just beginning to learn about. Let’s see what we get:\n\nplt.plot(y[:-1], y[1:], \",\")\nplt.xlabel(r\"$y(t_n)$\")\nplt.ylabel(r\"$y(t_{n+1})$\")\n\nText(0, 0.5, '$y(t_{n+1})$')\n\n\n\n\n\n\n\n\n\nNotice that \",\" passed to plt.plot draws a single pixel at each point’s location. If you look closely, you can see something like a 3D-rotated peak of the Rossler attractor (it’s sort of shaped like a U, except the open-ended lines are bent towards the left).\nThere is also a line corresponding to \\(y(t_n) = y(t_{n+1})\\). This one is to be expected as an artifact of our faulty point-gathering method. Remember that we gathered about twice as many points as there were points located by solve_ivp? Well, this just shows that those twice-as-many-points are mostly pairs of adjacent points close to each other! To illustrate what I mean, let me calculate \\(|y(t_{n+1}) - y(t_n)|\\) for each \\(t_n\\) in the trajectory and plot a histogram of that:\n\nplt.hist(np.abs(np.diff(y)), bins=100)\nplt.xlabel(r\"|y(t_n) = y(t_{n+1})|\")\n\nText(0.5, 0, '|y(t_n) = y(t_{n+1})|')\n\n\n\n\n\n\n\n\n\nThat’s about 100k points that didn’t move much at all between two hits of the \\(x=10\\) plane! That’s clearly incorrect.\n\n\nFixed hack-less version\nAs MatthewFlamm points out on GitHub, though the documentation I had needed to see was only available on scipy’s master branch, the dense_output flag to solve_ivp solves our issue:\n\ndef poincare(t, vector):\n    x = vector[0]\n    return x - x_section\n\npoincare.direction = -1    # decreasing x\nsol = solve_ivp(rossler,\n               [0, 500000],\n               [-0.2, 0.2, 5.2],\n               events=poincare,\n               dense_output=True)\n\nsol.sol\n\n&lt;scipy.integrate._ivp.common.OdeSolution at 0x7f1a0d35cb38&gt;\n\n\nThis returns a callable that we can use to interpolate the solution to any time point we choose, especially the event times:\n\nt = sol.t_events[0]\nvectors = sol.sol(t)\nvectors\n\narray([[10.        , 10.        , 10.        , ..., 10.        ,\n        10.        , 10.        ],\n       [ 6.43880307, 12.76136452,  8.36333749, ...,  7.63756891,\n         9.95585964, 13.35530381],\n       [ 0.03343946,  0.23215879, 19.95229153, ..., 23.0356289 ,\n         0.06066276,  3.4934677 ]])\n\n\nAnd now we just redo the plots we had before, making them a little nicer:\n\nrossler_attractor = plt.figure(figsize=(12, 8))\nax = rossler_attractor.add_subplot(111, projection='3d')\nax.plot(sol.y[0], sol.y[1], sol.y[2], ',', alpha=0.02)\n\nx, y, z = vectors\nax.plot(x, y, z, \".\", alpha=0.02)\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"z\");\n\n\n\n\n\n\n\n\nFrom the other angle:\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.view_init(30, -90)\nax.plot(sol.y[0], sol.y[1], sol.y[2], ',')\nax.plot(x, y, z, \".\")\nax.set_xlabel(\"x\")\nax.set_ylabel(\"y\")\nax.set_zlabel(\"z\");\n\n\n\n\n\n\n\n\n\nplt.plot(t, y, \",\")\nplt.xlabel(\"t\")\nplt.ylabel(\"y\");\n\n\n\n\n\n\n\n\n\\(y(t_{n+1})\\) plotted against \\(y(t_n)\\):\n\nplt.plot(y[:-1], y[1:], \",\")\nplt.xlabel(r\"$y(t_n)$\")\nplt.ylabel(r\"$y(t_{n+1})$\")\n\nText(0, 0.5, '$y(t_{n+1})$')\n\n\n\n\n\n\n\n\n\nAs you can see, that actually looks like it should, the histogram below should also not have a peak at 0:\n\nplt.hist(np.abs(np.diff(y)), bins=100)\nplt.xlabel(r\"|y(t_n) = y(t_{n+1})|\")\n\nText(0.5, 0, '|y(t_n) = y(t_{n+1})|')\n\n\n\n\n\n\n\n\n\nAnd it does not, indeed!\nThus, solve_ivp becomes even more awesome than I had thought it was! Check it out!"
  },
  {
    "objectID": "posts/parallelizable-numpy-implementation-of-2d-ising-model.html",
    "href": "posts/parallelizable-numpy-implementation-of-2d-ising-model.html",
    "title": "Parallelizable Numpy implementation of 2D Ising model",
    "section": "",
    "text": "In the next few posts I’d like to discuss a fun project I’ve been procrastinating things with recently - a parallelizable (up to the GPU level) Python Numpy implementation of the 2D Ising model, which I won’t introduce at length here because it’s been covered really well in multiple places out there and I’d rather not repeat them.\nThe gist of it is that we have a grid of discrete spins represented by integers, +1 for spin up and -1 for spin down. Each spin interacts with its nearest neighbors via a sum of products of that spin’s value and the neighbor’s value, times minus a positive constant J. The minus is there so that spins pointing the same way decrease the total energy and spins pointing in the opposite direction increase it.  Let’s get to it!\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nFor now, we’ll run a 16x16 grid, representing spin up as black and spin down as white.\n\nplt.rcParams['figure.figsize'] = [11, 6]  # for larger pictures\nplt.rcParams['image.cmap'] = 'plasma'    # well duh\nsize = (16, 16)\n\nnp.random.seed(20)\na = np.random.randint(low=0, high=2, size=size) * 2 - 1\nplt.imshow(a, cmap='binary')\nplt.colorbar();\n\n\n\n\n\n\n\n\nThe usual Monte Carlo Markov Chain (Metropolis algorithm) implementation for this goes like this:\n\npick a spin (point on the grid) at random\ncalculate the change in energy \\(\\Delta E\\) that would result from flipping it\nuse that change in energy to figure out how likely the flipped state is via a Boltzmann \\(\\exp(\\beta \\Delta E)\\) factor (clearly non-negative), calculated from interactions with neighboring spins\nuse that probability with an uniformly-distributed random number to reject or accept the state with the flipped state\nrun a boatload of iterations\n\nIt’s a nice algorithm, don’t get me wrong - but it seems needlessly complex to parallelize if you’re just picking one spin at a time. Why not try to flip every spin at a time? Let’s try that!\nFirst up, for each spin, we’ll calculate its interaction terms, with periodic boundary conditions. This just means that for each spin, we add up the sum of its neighbors’ spins (check the math, it really works like that), reaching to the other side on the edges. numpy.roll is amazing for that:\n\ninteractions = np.roll(a, 1, 0) + np.roll(a, 1, 1) + np.roll(a, -1, 0) + np.roll(a, -1, 1)\nplt.imshow(interactions)\nplt.colorbar();\n\n\n\n\n\n\n\n\nWe’ll just keep our constants “reasonable” (equal to 1), as simulationistas are wont to do. We’ll keep writing them though, so as to keep in mind where they should go in. The full energy is just the local spin’s value times its neighbors’ spins (the local spin factors out of the sum), times a constant:\n\nJ = 1 # a material constant defining the strength of the interaction\n\ncurrent_energy = - J * a * interactions\nplt.imshow(current_energy)\nplt.colorbar();\n\n\n\n\n\n\n\n\nWe’d expect the low energy spins to remain as they are, and the high energy spins to flip, so as to decrease the system’s energy.\nIt’s also simple to calculate the change in energy: boldly keeping the interaction terms constant, we just change the sign on a, so:\n\ndeltaE = 2 * J * a * interactions # = final state - initial state = -J interactions * ( (-a) - (a) )\nplt.imshow(deltaE)\nplt.colorbar();\n\n\n\n\n\n\n\n\nLet’s now get the Boltzmann factor:\n\nbeta = 1 # inverse thermal energy 1 / (k_B*T)\nboltzmann = np.exp(-beta * deltaE)\nplt.imshow(boltzmann)\nplt.colorbar();\n\n\n\n\n\n\n\n\nOuch, look at that colorbar! Note that we’re only comparing the Boltzmann factor (which is in the range of \\([0, \\infty)\\) ) with a standard (phew, almost callled it a normal) random number with a uniform distribution from the \\((0, 1)\\) range. I’m not going to bother paying attention to the boundaries because they’re infinitely unlikely for floats anyway.\nThus, a “better” (though admittedly uglier) plot that shows spins that are definitely going to flip in yellow is:\n\nplt.imshow(boltzmann, vmax = 1)\nplt.colorbar();\n\n\n\n\n\n\n\n\nGiven how ugly this plot was, let’s take a quick look at the distribution of these Boltzmann factors, inverstigating their arguments first:\n\nnp.unique(-beta * deltaE)\n\narray([-8, -4,  0,  4,  8])\n\n\nAnd now the actual Boltzmann factors, though we’ll just force each one above 1 to be 1 exactly:\n\nnp.unique(boltzmann)\n\narray([3.35462628e-04, 1.83156389e-02, 1.00000000e+00, 5.45981500e+01,\n       2.98095799e+03])\n\n\nIn other words, because of the discrete and simple (\\(s = \\pm1\\)) nature of the system, there are only a few possiblities.\nWe still need to figure out which spins will actually be flipped. That’s quite simple as well.\n\nrandoms = np.random.random(size)\nflip_these = randoms &lt; boltzmann\n\n_, axes = plt.subplots(ncols=2)\naxes[0].imshow(randoms);\naxes[1].imshow(flip_these.astype(int), cmap='binary');\n\n\n\n\n\n\n\n\nNote that we can now show the difference between which spins are definitely going to be flipped, and which spins are just “randomly” flipped:\n\nflip_these_for_sure = boltzmann &gt;= 1\n_, axes = plt.subplots(ncols=2)\naxes[0].imshow(flip_these ^ flip_these_for_sure)\naxes[1].imshow(boltzmann, vmax = 1);\n\n\n\n\n\n\n\n\nIt’s not many, but it’s important to take those into account! And now we flip all of them, with a “before vs after” picture:\n\na_new = a.copy() # I'm keeping `a` for plotting, but you could certainly do it in-place\n\na_new[flip_these] *= -1\n\n_, axes = plt.subplots(ncols=2)\naxes[0].imshow(a, cmap='binary')\naxes[1].imshow(a_new, cmap = 'binary');\n\n\n\n\n\n\n\n\nDid that work? Well… not quite, as you may already see. We made a little assumption in there that is going to mess this up. Let me just resummarize the computational loop really quickly, and check that I did so correctly. Note how the actual code is pretty short, too.\n\ndef iteration(a, J = 1, beta = 1):\n    interactions = np.roll(a, 1, 0) + np.roll(a, 1, 1) + np.roll(a, -1, 0) + np.roll(a, -1, 1)\n    deltaE = 2 * J * a * interactions\n    boltzmann = np.exp(-beta * deltaE)\n    flip_these = np.random.random(a.shape) &lt; boltzmann\n    new_a = a.copy()     # this could be neglected for an in-place implementation\n    new_a[flip_these] *= -1\n    return new_a\n\n_, axes = plt.subplots(ncols=2)\naxes[0].imshow(a, cmap='binary')\naxes[1].imshow(iteration(a), cmap = 'binary');\n\n\n\n\n\n\n\n\nDo you see the problem yet? If not, this Python translation of a popular song should do it:\n\nold_boss = np.array([[1, -1],       # excuse my variable naming\n                    [-1, 1]])       # this is all part of the plan\n\n_, axes = plt.subplots(ncols=3)\naxes[0].imshow(old_boss, cmap='binary')\n\nboss = iteration(old_boss)\naxes[1].imshow(boss, cmap = 'binary');   # oh no...\n\nnew_boss = iteration(boss)\naxes[2].imshow(new_boss, cmap = 'binary');  # oh, yeaaah\n\n\n\n\n\n\n\n\nAnd if you haven’t guessed yet:\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo(\"zYMD_W_r3Fg\", start=472)\n\n\n        \n        \n\n\nNote how I am merciful and spared you from having to lower your volume beforehand! You can trust me with these things.\nRemember that earlier we boldly (ctrl-f that if you don’t recall!) kept the interaction terms constant. But that’s a terrible mistake in terms of causality! Let’s take a look at this again:\n\nplt.imshow(new_boss, cmap='binary')\n\n\n\n\n\n\n\n\nThe black spins depend on the white spins for their energy calculation and vice versa. You can’t update the white spins at the same time as the black ones are changing.\nThis pattern, though, provides a way out of this conundrum. We can use old_boss as a mask! We’ll take turns updating the black and white spins this way.\n\ndef better_iteration(a, mask, J = 1, beta = 1):\n    interactions = np.roll(a, 1, 0) + np.roll(a, 1, 1) + np.roll(a, -1, 0) + np.roll(a, -1, 1)\n    deltaE = 2 * J * a * interactions # = final state - initial state = interactions * ( (-a) - (a) )\n    boltzmann = np.exp(-beta * deltaE) * mask    # this has been modified!\n    flip_these = np.random.random(a.shape) &lt; boltzmann\n    new_a = a.copy()\n    new_a[flip_these] *= -1\n    return new_a\n\n_, axes = plt.subplots(ncols=3)\naxes[0].imshow(old_boss, cmap='binary')\n\nbetter_boss = better_iteration(old_boss, mask=old_boss)\naxes[1].imshow(better_boss, cmap = 'binary');\n\nbetter_new_boss = better_iteration(better_boss, mask= 1 - old_boss) # note the negation here\naxes[2].imshow(better_boss, cmap = 'binary');\n\n\n\n\n\n\n\n\nAnd to demonstrate that this doesn’t get us spurious updates on the full grid:\n\na_mask = np.ones_like(a)\na_mask[::2, ::2] = 0\na_mask[1::2, 1::2] = 0\n\ndef full_iteration(a, mask, J = 1, beta = 1):\n    intermediate = better_iteration(a, mask, J, beta)\n    return better_iteration(intermediate, 1-mask, J, beta)\n\n_, axes = plt.subplots(ncols=4)\naxes[0].imshow(a_mask, cmap='binary');\naxes[1].imshow(a, cmap='binary')\naxes[2].imshow(better_iteration(a, a_mask), cmap='binary')\n\nfinal_a = full_iteration(a, a_mask)\naxes[3].imshow(final_a, cmap = 'binary');\n\n\n\n\n\n\n\n\nAnd I think we’ve earned this:\n\nYouTubeVideo(\"zYMD_W_r3Fg\", start=462) # headphone warning!\n\n\n        \n        \n\n\nLet’s do one last check:\n\na_iterated = a.copy()\nnp.random.seed(0)\nfor i in range(10):\n    a_iterated = full_iteration(a_iterated, a_mask)\nplt.imshow(a_iterated, cmap='binary');\n\n\n\n\n\n\n\n\nWell, that’s certainly physical behavior, though annoying - stable domains are a hallmark of the 2D Ising model under the critical temperature of about 2.3 (with \\(k_b = 1\\)). At high temperatures, the system behaves more randomly:\n\nbeta = 1 / (1 * 10)\na_iterated = a.copy()\nnp.random.seed(0)\nfor i in range(10):\n    a_iterated = full_iteration(a_iterated, a_mask, beta = beta)\nplt.imshow(a_iterated, cmap='binary');\n\n\n\n\n\n\n\n\nWhile under the critical temperature, the long-term stable state is the single domain, all-spins-parallel state:\n\na_iterated = a.copy()\nnp.random.seed(0)\nfor i in range(1000):\n    a_iterated = full_iteration(a_iterated, a_mask)\nplt.imshow(a_iterated, cmap='binary');\n\n\n\n\n\n\n\n\nAnd that beautiful plot is probably a good place to finish for today! Next up, benchmarking and optimization (as in, just straight up dumping this on the GPU)!"
  },
  {
    "objectID": "posts/on-the-recent-on-the-boris-solver-in-particle-in-cell-simulations-paper.html",
    "href": "posts/on-the-recent-on-the-boris-solver-in-particle-in-cell-simulations-paper.html",
    "title": "On the recent “On the Boris solver in Particle-in-cell simulations” paper",
    "section": "",
    "text": "I recently came across a pretty cool paper by Zenitani and Umeda named “On the Boris solver in particle-in-cell simulation”. There are many splendid descriptions of the Boris solver on the Internet, so while I would rather not duplicate them, here’s a brief overview. In PIC simulations, the Boris solver (or pusher) is the usual algorithm of choice for moving and accelerating particles in given electric and magnetic fields.\nYou may wonder, since the equations of motion are ordinary differential equations, what’s wrong with using the usual Runge-Kutta 4 solver? As it turns out, that one has a pretty major flaw. It has great accuracy for short term calculations, but over time your particle’s motion will lose energy. This is a deal breaker for periodic motion, and simulations of, for example, plasma waves need to conserve that energy to provide accurate results.\nBoris came up with his solver in the 1950’s, and in a single sentence: the algorithm splits the acceleration via electric field into two parts and sticks a rotation about the magnetic field between them. This turns out to conserve energy and will probably come up again on this blog as I read more about symplexicity. \nHowever, there’s a catch. There’s a single basic and dense textbook for particle simulation, called “Plasma Physics via Computer Simulation” by Birdsall and Langdon. It has been referenced in most PIC papers I’ve read. The Boris solver as described by this PIC bible involves a vector quantity (following the authors we’ll call this part of the Boris-B algorithm):\n\\[\\vec{t} = \\frac{\\theta}{2} \\vec{b} \\tag{Boris-B}\\]\n\\(\\vec{b}\\) being the unit vector in the direction of the magnetic field \\(\\vec{B}\\) and \\(\\theta \\sim dt |\\vec{B}|\\). However, what Boris originally had in his derivation was (the Boris-A algorithm):\n\\[\\vec{t} = \\tan{\\frac{\\theta}{2}} \\vec{b} \\tag{Boris-A}\\]\nAnd there’s a subtle difference there! Well, it’s subtle if you have \\(\\frac{\\theta}{2} &lt;&lt; 1\\) and quickly stops being subtle if you\n\nhave large \\(\\theta\\), which you probably shouldn’t as it’s proportional to the timestep\ncare about the performance of your pusher, which you probably should\n\nThe version in B&L’s book is a simplification (admittedly one that B&L pointed out was being made) that incorporates a slight error in the calculation, but turns out to be a bit faster (tangents were quite expensive to calculate back then). For a very simple comparison of the two:\n\nfrom math import tan\ntheta = 0.1\n\njust_division = %timeit -o theta/2\n\n38 ns ± 2.43 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n\n\n\ntangent_division = %timeit -o tan(theta/2)\n\n81.2 ns ± 3.39 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)\n\n\n\n(tangent_division.average) / just_division.average\n\n2.138698576440618\n\n\nAnd that’s on a modern CPU with a modern math library in a modern language! At the time of writing of B&L’s book, this was indeed something people found valuable to optimize out of their code.\nWhat the authors of this paper did was take a few more steps of the calculation in the entire Boris-A algorithm and rewrite them into the Boris-C version, which turns out to be * just as accurate as Boris-A (see the plots in the paper for some really neat results) * “only 25% slower than the Boris-B solver” * “faster than the Boris-A solver” (where Boris-A was 46% slower than Boris-B)\nThis is neat, so I figured we could maybe do this in Python really quickly to show how it works.\nLet’s start with the classic version. We’ll first include a couple of helpers:\n\nimport numpy as np\n\ncharge = 1\nmass = 1\nlightspeed = 1\n\n\ndef epsilon(electric_field, timestep):\n    return charge * timestep / (2*mass) * electric_field \n\ndef gamma_from_velocity(velocity):\n    return np.sqrt(1 - np.sum((velocity / lightspeed)**2))\n\ndef gamma_from_u(u):\n    return np.sqrt(1+np.sum((u/lightspeed)**2))\n\nWe can now proceed to implement the various versions of the Boris solver. I’m mostly just going through the paper and turning the equations into code, nothing crazy.u_t_minus_half is the velocity at time \\(t-\\Delta t /2\\), as the Boris solver takes particle velocities as shifted in time: with a timestep \\(\\Delta t\\), you get positions at \\(t = 0, \\Delta t, 2 \\Delta t ...\\), while your velocities are defined at times \\(t = -\\Delta t / 2, + \\Delta t / 2, 3 \\Delta t / 2...\\)\n\ndef BorisA(position, u_t_minus_half, electric_field, magnetic_field, timestep):\n    # Equations 3, 6, 7a, 8, 9, 5\n    uminus = u_t_minus_half + epsilon(electric_field, timestep)  # Eq. 3\n    magfield_norm = np.linalg.norm(magnetic_field)\n    theta = charge * timestep / mass / gamma_from_u(uminus) * magfield_norm  # Eq. 6\n        \n    b = magnetic_field / magfield_norm\n    \n    t = np.tan(theta/2) * b # Eq. 7a\n    \n    uprime = uminus + np.cross(uminus, t)  # Eq. 8\n    uplus = uminus + 2/(1+(t**2).sum()) * np.cross(uprime, t)  # Eq. 9\n    u_t_plus_half = uplus + epsilon(electric_field, timestep) # Eq. 5\n    new_position = u_t_plus_half / gamma_from_u(u_t_plus_half) * timestep + position # Eq. 1\n    return new_position, u_t_plus_half \n\ndef BorisB(position, u_t_minus_half, electric_field, magnetic_field, timestep):\n    # 3, 7b, 8, 9, 5\n    uminus = u_t_minus_half + epsilon(electric_field, timestep)  # Eq. 3\n    \n    # Eq. 7a\n    t = charge * timestep / (2 * mass * gamma_from_u(uminus)) * magnetic_field\n    \n    uprime = uminus + np.cross(uminus, t)  # Eq. 8\n    uplus = uminus + 2/(1+(t**2).sum()) * np.cross(uprime, t)  # Eq. 9\n    u_t_plus_half = uplus + epsilon(electric_field, timestep) # Eq. 5\n    new_position = u_t_plus_half / gamma_from_u(u_t_plus_half) * timestep + position # Eq. 1\n    return new_position, u_t_plus_half \n    \ndef BorisC(position, u_t_minus_half, electric_field, magnetic_field, timestep):\n    # 3, 6, 11, 12, 5\n    uminus = u_t_minus_half + epsilon(electric_field, timestep)  # Eq. 3\n    magfield_norm = np.linalg.norm(magnetic_field)\n    theta = charge * timestep / mass / gamma_from_u(uminus) * magfield_norm  # Eq. 6\n    \n    b = magnetic_field / magfield_norm\n    \n    u_parallel_minus = np.dot(uminus, b) * b # Eq. 11\n    uplus = u_parallel_minus + (uminus - u_parallel_minus) * np.cos(theta) + np.cross(uminus, b) * np.sin(theta) # Eq. 12\n    u_t_plus_half = uplus + epsilon(electric_field, timestep) # Eq. 5\n    new_position = u_t_plus_half / gamma_from_u(u_t_plus_half) * timestep + position # Eq. 1\n    return new_position, u_t_plus_half \n\nWe can now start implementing the authors’ test cases as seen in the paper. We’ll first define a helper plotting function:\n\ndef plot(r, v, trajectory_format = \".:\", timeseries_format = \".--\"):\n    x, y, z = r.T\n    fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n    axes[0,0].plot(x, timeseries_format, label=\"x\")\n    axes[0,0].plot(y, timeseries_format, label=\"y\")\n    axes[0,0].plot(z, timeseries_format, label=\"z\")\n    axes[0,0].set_xlabel(\"Iteration\")\n    axes[0,0].legend(loc='best')\n    \n    axes[1,0].plot(x, y, trajectory_format)\n    axes[1,0].set_xlabel(\"X\")\n    axes[1,0].set_ylabel(\"Y\")\n    \n    vx, vy, vz = v.T\n    axes[0, 1].plot(vx, timeseries_format, label=\"Vx\")\n    axes[0, 1].plot(vy, timeseries_format, label=\"Vy\")\n    axes[0, 1].plot(vz, timeseries_format, label=\"Vz\")\n    axes[0, 1].legend(loc='best')\n    axes[0, 1].set_xlabel(\"Iteration\")\n    axes[0, 1].set_ylabel(\"Velocity\")\n                      \n    axes[1, 1].plot(vx, vy, trajectory_format)\n    axes[1, 1].set_xlabel(\"X Velocity\")\n    axes[1, 1].set_ylabel(\"Y Velocity\")\n    plt.tight_layout()\n    return r, v\n\nAnd now we can start to implement the first test case:\n\nMovement in constant crossed electric and magnetic fields\n\ndef drift(pusher, E=1, B=1):\n    electric_field = np.array([E, 0, 0])\n    magnetic_field = np.array([0, 0, B])\n    \n    # initial conditions\n    u_t_minus_half = np.array([1, 0, 0])\n    position = np.zeros(3)\n    timestep = np.pi/6\n    \n    # I'm taking this a bit longer than the authors, so that the plots look nicer\n    t = np.arange(0, 120/np.pi, timestep) \n    \n    positions = []\n    velocities = []\n    for i in t:\n        positions.append(position)\n        velocities.append(u_t_minus_half)\n        position, u_t_minus_half = pusher(position, u_t_minus_half, electric_field, magnetic_field, timestep)\n\n    \n    r = np.array(positions)\n    v = np.array(velocities)\n    return r, v\n\nplot(*drift(BorisA, E=0, B=1));\n\n\n\n\n\n\n\n\nThat looks reasonable.\n\nplot(*drift(BorisB, E=0, B=1));\n\n\n\n\n\n\n\n\n\nplot(*drift(BorisC, E=0, B=1));\n\n\n\n\n\n\n\n\nPretty indistinguishable from the BorisA case! In fact, that’s what the authors claim and what we can check numerically:\n\nfor name, array_A, array_B, array_C in zip([\"position\", \"velocity\"], drift(BorisA), drift(BorisB), drift(BorisC)):\n    print(f\"BorisA and BorisC {'' if np.allclose(array_A, array_C, atol=1e-20, rtol=1e-15) else 'DO NOT '}agree on {name} for rotation.\")\n    print(f\"BorisB and BorisC {'' if np.allclose(array_B, array_C, atol=1e-20, rtol=1e-15) else 'DO NOT '}agree on {name} for rotation.\")\n\nBorisA and BorisC agree on position for rotation.\nBorisB and BorisC DO NOT agree on position for rotation.\nBorisA and BorisC agree on velocity for rotation.\nBorisB and BorisC DO NOT agree on velocity for rotation.\n\n\nI went through the different cases presented for this part in the paper, and they seem to agree as well. Let’s reproduce another example, the \\(\\vec{E} \\times \\vec{B}\\) drift. I won’t show the BorisB plot here, as it doesn’t visually differ, though the difference is there:\n\nborisC_drift = plot(*drift(BorisC, E=1, B=1))\nborisB_drift = drift(BorisB, E=1, B=1)\nborisA_drift = drift(BorisA, E=1, B=1)\nfor name, array_A, array_B, array_C in zip([\"position\", \"velocity\"], borisA_drift, borisB_drift, borisC_drift):\n    print(f\"BorisA and BorisC {'' if np.allclose(array_A, array_C, atol=1e-20, rtol=1e-15) else 'DO NOT '}agree on {name} on the ExB drift.\")\n    print(f\"BorisB and BorisC {'' if np.allclose(array_B, array_C, atol=1e-20, rtol=1e-15) else 'DO NOT '}agree on {name} on the ExB drift.\")\n\nBorisA and BorisC agree on position on the ExB drift.\nBorisB and BorisC DO NOT agree on position on the ExB drift.\nBorisA and BorisC agree on velocity on the ExB drift.\nBorisB and BorisC DO NOT agree on velocity on the ExB drift.\n\n\n\n\n\n\n\n\n\n\n\nLong term stability tests\nThe authors define this as a long time run in the following fields:\n\\[ \\phi = \\frac{0.01}{\\sqrt{x^2 + y^2)}} \\] \\[ \\vec{B} = \\sqrt{x^2 + y^2} \\] with \\(\\vec{E} = -\\nabla \\phi\\) as usual. Let’s just calculate the derivative with SymPy really quickly here:\n\nfrom sympy.abc import x, y\nphi = 0.01 * (x**2 + y**2)**-0.5\nphi\n\nfrom sympy import lambdify\n\nEx = -phi.diff(x)\nEy = -phi.diff(y)\nEx = lambdify((x, y), Ex)\nEy = lambdify((x, y), Ey)\n\ndef stability(pusher, time_range=8e2):\n    u_t_minus_half = np.array([0.1, 0, 0])\n    position = np.array([0.9, 0, 0])\n    timestep = np.pi/10\n    t = np.arange(0, time_range, timestep)    \n    \n    positions = []\n    velocities = []\n    for i in t:\n        x, y, z = position\n        magnetic_field = np.array([0, 0, np.sqrt(x**2 + y**2)])\n        electric_field = np.array([Ex(x, y), Ey(x, y), 0])\n        positions.append(position)\n        velocities.append(u_t_minus_half)\n        position, u_t_minus_half = pusher(position, u_t_minus_half, electric_field, magnetic_field, timestep)\n    \n    r = np.array(positions)\n    v = np.array(velocities)\n    return r, v\n   \n\nplot(*stability(BorisA), trajectory_format=\".\");\n\n\n\n\n\n\n\n\n\nplot(*stability(BorisC), trajectory_format=\".\");\n\n\n\n\n\n\n\n\n\nplot(*stability(BorisB), trajectory_format=\".\");\n\n\n\n\n\n\n\n\nNote how the inner side of the velocity space trajectory becomes circular for the BorisB case and keeps making a neat pattern in the BorisA and BorisC cases!\n\nrtol = 1e-9\nfor name, array_A, array_B, array_C in zip([\"position\", \"velocity\"], stability(BorisA), stability(BorisB), stability(BorisC)):\n    print(f\"BorisA and BorisC {'' if np.allclose(array_A, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n    print(f\"BorisB and BorisC {'' if np.allclose(array_B, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n\nBorisA and BorisC agree on position for long term stability for relative tolerance 1e-09.\nBorisB and BorisC DO NOT agree on position for long term stability for relative tolerance 1e-09.\nBorisA and BorisC agree on velocity for long term stability for relative tolerance 1e-09.\nBorisB and BorisC DO NOT agree on velocity for long term stability for relative tolerance 1e-09.\n\n\nDo note that there does seem to be some long term error creeping in, as I had to lower rtol:\n\nrtol = 1e-10\nfor name, array_A, array_B, array_C in zip([\"position\", \"velocity\"], stability(BorisA), stability(BorisB), stability(BorisC)):\n    print(f\"BorisA and BorisC {'' if np.allclose(array_A, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n    print(f\"BorisB and BorisC {'' if np.allclose(array_B, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n\nBorisA and BorisC agree on position for long term stability for relative tolerance 1e-10.\nBorisB and BorisC DO NOT agree on position for long term stability for relative tolerance 1e-10.\nBorisA and BorisC DO NOT agree on velocity for long term stability for relative tolerance 1e-10.\nBorisB and BorisC DO NOT agree on velocity for long term stability for relative tolerance 1e-10.\n\n\n\nrtol = 1e-15\nfor name, array_A, array_B, array_C in zip([\"position\", \"velocity\"], stability(BorisA), stability(BorisB), stability(BorisC)):\n    print(f\"BorisA and BorisC {'' if np.allclose(array_A, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n    print(f\"BorisB and BorisC {'' if np.allclose(array_B, array_C, atol=1e-20, rtol=rtol) else 'DO NOT '}agree on {name} for long term stability for relative tolerance {rtol}.\")\n\nBorisA and BorisC DO NOT agree on position for long term stability for relative tolerance 1e-15.\nBorisB and BorisC DO NOT agree on position for long term stability for relative tolerance 1e-15.\nBorisA and BorisC DO NOT agree on velocity for long term stability for relative tolerance 1e-15.\nBorisB and BorisC DO NOT agree on velocity for long term stability for relative tolerance 1e-15.\n\n\nStill, the results look all right, especially if we were to overlay the trajectories. Let’s calculate a bunch of long-time trajectories (like the authors do) and make a quick adjustment to the plotting function:\n\nborisA_stability = stability(BorisA, time_range=2e5)\nborisB_stability = stability(BorisB, time_range=2e5)\nborisC_stability = stability(BorisC, time_range=2e5)\n\ndef plot_shared(*tuples, r_format=\",\", v_format = \".\", alpha=0.1, start_at):\n    fig, axes = plt.subplots(ncols=2, figsize=(12, 7))\n    for r, v in tuples:\n        x, y, z = r[int(start_at):].T\n        axes[0].set_title(\"x-y position trajectory\")\n        axes[0].plot(x, y, r_format, alpha=alpha)\n        vx, vy, vz = v[int(start_at):].T\n        \n        axes[1].set_title(\"x-y velocity trajectory\")\n        axes[1].plot(vx, vy, v_format, alpha=alpha)\n    plt.tight_layout()\n\n\nplot_shared(borisA_stability,\n            borisC_stability,\n            r_format=\".\",\n            v_format=\".\",\n            alpha=0.8,\n            start_at = len(borisA_stability[0]) * 0.99\n           )\n\n\n\n\n\n\n\n\n\nplot_shared(borisB_stability,\n            borisC_stability,\n            r_format=\".\",\n            v_format=\".\",\n            alpha=0.8,\n            start_at = len(borisA_stability[0]) * 0.99\n           )\n\n\n\n\n\n\n\n\n\n\nBenchmark\nAnd to finalize, let’s benchmark the results, to see if BorisC is indeed faster than BorisA:\n\nelectric_field = np.array([1, 0, 0])\nmagnetic_field = np.array([0, 0, 1])\n\n# initial conditions\nu_t_minus_half = np.array([1, 0, 0])\nposition = np.zeros(3)\ntimestep = np.pi/6\n\n%timeit BorisA(position, u_t_minus_half, electric_field, magnetic_field, timestep)\n\n101 µs ± 1.29 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n\n\n%timeit BorisB(position, u_t_minus_half, electric_field, magnetic_field, timestep)\n\n89.9 µs ± 2.03 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n\n\n%timeit BorisC(position, u_t_minus_half, electric_field, magnetic_field, timestep)\n\n71.8 µs ± 2.3 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n\n… and this clearly points to some inefficiency in my BorisB implementation. BorisC does seem to beat BorisA, though, so I guess we didn’t manage to falsify the result!"
  },
  {
    "objectID": "posts/julia-composability.html",
    "href": "posts/julia-composability.html",
    "title": "Julia - the magic of composability",
    "section": "",
    "text": "I’ve been getting into the Julia ecosystem a little (turns out there’s a lot of tutorials you get to watch when doing the dishes), and I’ve been meaning to get some hands on experience. Today, I’d like to reproduce Mosè Giordano’s gist example, which I first saw in this JuliaCon talk by Stefan Karpinski. I think it really illustrates Alan Edelman’s points on Julia’s composability from the talk I linked last time.\nWhat we’ll do today is combine a simple example of an ODE solution with an uncertainty package, and see how easy or difficult it becomes using Julia.\n\n\nDifferential equations\nLet’s try to use Julia’s DifferentialEquations.jl package to solve the simple system:\n\\[\\frac{du}{dt} = \\alpha u\\]\n\nusing DifferentialEquations\n\nα = 1.01\nu0 = 1/2             # initial value\ntspan = (0.0, 1.0)   # range of values\n\n(0.0, 1.0)\n\n\nSimple enough! Let’s define the derivative function. Note that it seems like the derivative’s signature as accepted by DifferentialEquations.ODEProblem has changed since Mosè Giordano wrote his gist.\n\nf(u, p, t) = α * u\n\nf (generic function with 1 method)\n\n\n\nprob = ODEProblem(f, u0, tspan)\n\n\nODEProblem with uType Float64 and tType Float64. In-place: false\ntimespan: (0.0, 1.0)\nu0: 0.5\n\n\n\nThese explicit reprs are pretty darn cool, to be honest. Let’s get to solving it, as seen in the example:\n\nsol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-8)\n\nretcode: Success\nInterpolation: specialized 4th order \"free\" interpolation\nt: 17-element Array{Float64,1}:\n 0.0\n 0.012407826196308189\n 0.04250125658161484\n 0.08178046092620397\n 0.12887379439591745\n 0.18409790041494495\n 0.24627449404376492\n 0.3147928829168652\n 0.38859624030646006\n 0.46686165530000767\n 0.5487159959104151\n 0.6334345501790717\n 0.7203628343994752\n 0.8089578125953629\n 0.8987653123338385\n 0.9894159840028138\n 1.0\nu: 17-element Array{Float64,1}:\n 0.5\n 0.5063053789114713\n 0.5219304636285521\n 0.5430526974619144\n 0.5695067474049924\n 0.6021743238204087\n 0.6412025113764279\n 0.687147458356146\n 0.7403257567387032\n 0.8012222468290549\n 0.8702767411264873\n 0.9480213225441934\n 1.0350184806191094\n 1.131902913018661\n 1.239373221095387\n 1.3582036259485553\n 1.3728005076225749\n\n\nAnd now we can use Plots.jl to plot the solution. I adapted the docs for DifferentialEquations to do that.\n\nusing Plots\nplot(sol,linewidth=5,title=\"Solution to the linear ODE with a thick line\",\n     xaxis=\"Time (t)\",yaxis=\"u(t)\",label=\"Numerical solution\")\nplot!(sol.t, t-&gt;0.5*exp(1.01t),lw=3,ls=:dash,label=\"True Solution!\")\n\n\n\n\n\n\n\n\nAnd that’s all neat. On to the main course!\n\n\nOnce more, with uncertainty!\nFor convenience (and practice!), we’ll wrap our previous computation in a function:\n\nfunction compute(α, u0, tspan)\n    f(u, p, t) = α * u\n    noisy_prob = ODEProblem(f, u0, tspan)\n    noisy_sol = solve(noisy_prob, Tsit5(), reltol=1e-8, abstol=1e-8)\n    noisy_sol             # Julia returns the last statement in a function\nend\n\ncompute (generic function with 1 method)\n\n\nAnd now we’ll add Measurements.jl and input a slightly noisy value for the \\(\\alpha\\) constant, using \\(\\pm\\) (typed in Jupyter as \\pm&lt;TAB&gt;):\n\nusing Measurements\nnoisy_sol =compute(1.01 ± 0.1,\n    1/2 ± 0,  # this had to be the same type, or we'd get an exception:\n    (0.0, 1.0)\n    )\n\nretcode: Success\nInterpolation: specialized 4th order \"free\" interpolation\nt: 17-element Array{Float64,1}:\n 0.0\n 0.012407826196308189\n 0.042501278333560696\n 0.0817804940926822\n 0.12887384498704435\n 0.18409796286152927\n 0.24627457447456758\n 0.31479297816557983\n 0.3885963515160237\n 0.4668617724420117\n 0.5487161305960653\n 0.6334346972152323\n 0.7203630000154827\n 0.808957991167541\n 0.8987655040395068\n 0.9894161889652783\n 1.0\nu: 17-element Array{Measurement{Float64},1}:\n     0.5 ± 0.0\n 0.50631 ± 0.00063\n  0.5219 ± 0.0022\n  0.5431 ± 0.0044\n  0.5695 ± 0.0073\n   0.602 ± 0.011\n   0.641 ± 0.016\n   0.687 ± 0.022\n    0.74 ± 0.029\n   0.801 ± 0.037\n    0.87 ± 0.048\n   0.948 ± 0.06\n   1.035 ± 0.075\n   1.132 ± 0.092\n    1.24 ± 0.11\n    1.36 ± 0.13\n    1.37 ± 0.14\n\n\nAnd you can already see the uncertainties propagate onwards! The further in time, the more effect that slight discrepancy in the value of \\(\\alpha\\) (or \\(R_0\\), these days…) will have. That makes sense!\nThe amazing thing is that I can stick noisy_sol right into the plot command of Plots.jl:\n\nplot(noisy_sol, lab=\"Numerical with uncertainty\",\n     linewidth=5, xaxis=\"Time (t)\",yaxis=\"u(t)\",\n)\nplot!(sol.t, t-&gt;0.5*exp(1.01t),lw=3,ls=:dash,label=\"True Solution!\")\n\n\n\n\n\n\n\n\nJust like that! And that’s just a little magical to me.\nWe can also put an uncertainty on the initial condition - we rarely know those with perfect accuracy.\n\nnoisy_sol =compute(1.01,   # interestingly the constant can be a \"precise\" float\n    1/2 ± 0.3,\n    (0.0, 1.0)\n    )\n\nretcode: Success\nInterpolation: specialized 4th order \"free\" interpolation\nt: 17-element Array{Float64,1}:\n 0.0\n 0.012407826196308189\n 0.042501278333560696\n 0.0817804940926822\n 0.12887384498704435\n 0.18409796286152927\n 0.24627457447456758\n 0.31479297816557983\n 0.3885963515160237\n 0.4668617724420117\n 0.5487161305960653\n 0.6334346972152323\n 0.7203630000154827\n 0.808957991167541\n 0.8987655040395068\n 0.9894161889652783\n 1.0\nu: 17-element Array{Measurement{Float64},1}:\n  0.5 ± 0.3\n 0.51 ± 0.3\n 0.52 ± 0.31\n 0.54 ± 0.33\n 0.57 ± 0.34\n  0.6 ± 0.36\n 0.64 ± 0.38\n 0.69 ± 0.41\n 0.74 ± 0.44\n  0.8 ± 0.48\n 0.87 ± 0.52\n 0.95 ± 0.57\n 1.04 ± 0.62\n 1.13 ± 0.68\n 1.24 ± 0.74\n 1.36 ± 0.81\n 1.37 ± 0.82\n\n\n\nplot(noisy_sol, lab=\"Numerical with uncertainty\",\n     linewidth=5, xaxis=\"Time (t)\",yaxis=\"u(t)\",\n)\nplot!(sol.t, t-&gt;0.5*exp(1.01t),lw=3,ls=:dash,label=\"True Solution!\")\n\n\n\n\n\n\n\n\nWe can also combine the uncertainties to our heart’s content! I’m choosing to run this with a very large uncertainty on the constant, to illustrate the point.\n\nnoisy_sol =compute(1.01 ± 0.8,\n    1/2 ± 0.3,\n    (0.0, 1.0)\n    )\nplot(noisy_sol, lab=\"Numerical with uncertainty\",\n     linewidth=5, xaxis=\"Time (t)\",yaxis=\"u(t)\",\n)\nplot!(sol.t, t-&gt;0.5*exp(1.01t),lw=3,ls=:dash,label=\"True Solution!\")\n\n\n\n\n\n\n\n\n\n\nComposability\nAnd here’s the point I wanted to make.\nNobody wrote uncertainty support for DifferentialEquations.jl. Here’s a search for references to measurements in that package - nil. It just popped out of Julia’s composable type system.\nThe plotting part is admittedly custom and can be found here. But still - imagine, in Python, combining scipy.integrate.odeint, uncertainties and maybe even astropy.units or pint, or what have you.\nHere it “just works”. Magic!\nActually, speaking of unit packages…\n\nusing Unitful\n1u\"s\" ^ 3\n\n1 s^3\n\n\nAnd if we try…\n\n0.5u\"m\" ± 40u\"cm\"\n\n0.5 ± 0.4 m\n\n\nWell, maybe…\n\n1.01u\"1/s\" ± 0.1u\"1/s\"\n\n1.01 ± 0.1 s^-1\n\n\nMaybe, maybe, maybe…\n\nnoisy_sol =compute(1.01u\"1/s\",\n    0.5u\"m\" ± 40u\"cm\",  # I had to cheat a little here\n    (0.0u\"s\", 1.0u\"s\"),\n    )\n\nMethodError: MethodError: no method matching Float64(::Measurement{Float64})\nClosest candidates are:\n  Float64(::Real, !Matched::RoundingMode) where T&lt;:AbstractFloat at rounding.jl:200\n  Float64(::T) where T&lt;:Number at boot.jl:715\n  Float64(!Matched::Int8) at float.jl:60\n  ...\nMethodError: no method matching Float64(::Measurement{Float64})\nClosest candidates are:\n  Float64(::Real, !Matched::RoundingMode) where T&lt;:AbstractFloat at rounding.jl:200\n  Float64(::T) where T&lt;:Number at boot.jl:715\n  Float64(!Matched::Int8) at float.jl:60\n  ...\n\nStacktrace:\n [1] convert(::Type{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}}, ::Quantity{Measurement{Float64},𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}) at /home/dominik/.julia/packages/Unitful/MOEUx/src/conversion.jl:110\n [2] ode_determine_initdt(::Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}}, ::Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}, ::Float64, ::Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}, ::Float64, ::Float64, ::typeof(DiffEqBase.ODE_DEFAULT_NORM), ::ODEProblem{Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},Tuple{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}},false,DiffEqBase.NullParameters,ODEFunction{false,var\"#f#9\"{Quantity{Float64,𝐓^-1,Unitful.FreeUnits{(s^-1,),𝐓^-1,nothing}}},LinearAlgebra.UniformScaling{Bool},Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing},Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},DiffEqBase.StandardODEProblem}, ::OrdinaryDiffEq.ODEIntegrator{Tsit5,false,Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},Nothing,Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},DiffEqBase.NullParameters,Float64,Float64,Float64,Array{Quantity{Measurement{Float64},𝐋 𝐓^-1,Unitful.FreeUnits{(m, s^-1),𝐋 𝐓^-1,nothing}},1},ODESolution{Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},1,Array{Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},1},Nothing,Nothing,Array{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},1},Array{Array{Quantity{Measurement{Float64},𝐋 𝐓^-1,Unitful.FreeUnits{(m, s^-1),𝐋 𝐓^-1,nothing}},1},1},ODEProblem{Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},Tuple{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}},false,DiffEqBase.NullParameters,ODEFunction{false,var\"#f#9\"{Quantity{Float64,𝐓^-1,Unitful.FreeUnits{(s^-1,),𝐓^-1,nothing}}},LinearAlgebra.UniformScaling{Bool},Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing},Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},DiffEqBase.StandardODEProblem},Tsit5,OrdinaryDiffEq.InterpolationData{ODEFunction{false,var\"#f#9\"{Quantity{Float64,𝐓^-1,Unitful.FreeUnits{(s^-1,),𝐓^-1,nothing}}},LinearAlgebra.UniformScaling{Bool},Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing},Array{Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},1},Array{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},1},Array{Array{Quantity{Measurement{Float64},𝐋 𝐓^-1,Unitful.FreeUnits{(m, s^-1),𝐋 𝐓^-1,nothing}},1},1},OrdinaryDiffEq.Tsit5ConstantCache{Float64,Float64}},DiffEqBase.DEStats},ODEFunction{false,var\"#f#9\"{Quantity{Float64,𝐓^-1,Unitful.FreeUnits{(s^-1,),𝐓^-1,nothing}}},LinearAlgebra.UniformScaling{Bool},Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing},OrdinaryDiffEq.Tsit5ConstantCache{Float64,Float64},OrdinaryDiffEq.DEOptions{Float64,Float64,Float64,Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},typeof(DiffEqBase.ODE_DEFAULT_NORM),typeof(LinearAlgebra.opnorm),CallbackSet{Tuple{},Tuple{}},typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN),typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE),typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK),DataStructures.BinaryHeap{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},DataStructures.LessThan},DataStructures.BinaryHeap{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},DataStructures.LessThan},Nothing,Nothing,Int64,Tuple{},Tuple{},Tuple{}},Quantity{Measurement{Float64},𝐋 𝐓^-1,Unitful.FreeUnits{(m, s^-1),𝐋 𝐓^-1,nothing}},Measurement{Float64},Nothing,OrdinaryDiffEq.DefaultInit}) at /home/dominik/.julia/packages/OrdinaryDiffEq/NsugH/src/initdt.jl:148\n [3] auto_dt_reset! at /home/dominik/.julia/packages/OrdinaryDiffEq/NsugH/src/integrators/integrator_interface.jl:297 [inlined]\n [4] handle_dt!(::OrdinaryDiffEq.ODEIntegrator{Tsit5,false,Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},Nothing,Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},DiffEqBase.NullParameters,Float64,Float64,Float64,Array{Quantity{Measurement{Float64},𝐋 𝐓^-1,Unitful.FreeUnits{(m, s^-1),𝐋 𝐓^-1,nothing}},1},ODESolution{Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},1,Array{Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},1},Nothing,Nothing,Array{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},1},Array{Array{Quantity{Measurement{Float64},𝐋 𝐓^-1,Unitful.FreeUnits{(m, s^-1),𝐋 𝐓^-1,nothing}},1},1},ODEProblem{Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},Tuple{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}},false,DiffEqBase.NullParameters,ODEFunction{false,var\"#f#9\"{Quantity{Float64,𝐓^-1,Unitful.FreeUnits{(s^-1,),𝐓^-1,nothing}}},LinearAlgebra.UniformScaling{Bool},Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing},Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},DiffEqBase.StandardODEProblem},Tsit5,OrdinaryDiffEq.InterpolationData{ODEFunction{false,var\"#f#9\"{Quantity{Float64,𝐓^-1,Unitful.FreeUnits{(s^-1,),𝐓^-1,nothing}}},LinearAlgebra.UniformScaling{Bool},Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing},Array{Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},1},Array{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},1},Array{Array{Quantity{Measurement{Float64},𝐋 𝐓^-1,Unitful.FreeUnits{(m, s^-1),𝐋 𝐓^-1,nothing}},1},1},OrdinaryDiffEq.Tsit5ConstantCache{Float64,Float64}},DiffEqBase.DEStats},ODEFunction{false,var\"#f#9\"{Quantity{Float64,𝐓^-1,Unitful.FreeUnits{(s^-1,),𝐓^-1,nothing}}},LinearAlgebra.UniformScaling{Bool},Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing},OrdinaryDiffEq.Tsit5ConstantCache{Float64,Float64},OrdinaryDiffEq.DEOptions{Float64,Float64,Float64,Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},typeof(DiffEqBase.ODE_DEFAULT_NORM),typeof(LinearAlgebra.opnorm),CallbackSet{Tuple{},Tuple{}},typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN),typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE),typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK),DataStructures.BinaryHeap{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},DataStructures.LessThan},DataStructures.BinaryHeap{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},DataStructures.LessThan},Nothing,Nothing,Int64,Tuple{},Tuple{},Tuple{}},Quantity{Measurement{Float64},𝐋 𝐓^-1,Unitful.FreeUnits{(m, s^-1),𝐋 𝐓^-1,nothing}},Measurement{Float64},Nothing,OrdinaryDiffEq.DefaultInit}) at /home/dominik/.julia/packages/OrdinaryDiffEq/NsugH/src/solve.jl:450\n [5] __init(::ODEProblem{Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},Tuple{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}},false,DiffEqBase.NullParameters,ODEFunction{false,var\"#f#9\"{Quantity{Float64,𝐓^-1,Unitful.FreeUnits{(s^-1,),𝐓^-1,nothing}}},LinearAlgebra.UniformScaling{Bool},Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing},Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},DiffEqBase.StandardODEProblem}, ::Tsit5, ::Tuple{}, ::Tuple{}, ::Tuple{}, ::Type{Val{true}}; saveat::Tuple{}, tstops::Tuple{}, d_discontinuities::Tuple{}, save_idxs::Nothing, save_everystep::Bool, save_on::Bool, save_start::Bool, save_end::Bool, callback::Nothing, dense::Bool, calck::Bool, dt::Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}, dtmin::Nothing, dtmax::Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}, force_dtmin::Bool, adaptive::Bool, gamma::Rational{Int64}, abstol::Float64, reltol::Float64, qmin::Rational{Int64}, qmax::Int64, qsteady_min::Int64, qsteady_max::Int64, qoldinit::Rational{Int64}, fullnormalize::Bool, failfactor::Int64, beta1::Nothing, beta2::Nothing, maxiters::Int64, internalnorm::typeof(DiffEqBase.ODE_DEFAULT_NORM), internalopnorm::typeof(LinearAlgebra.opnorm), isoutofdomain::typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN), unstable_check::typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK), verbose::Bool, timeseries_errors::Bool, dense_errors::Bool, advance_to_tstop::Bool, stop_at_next_tstop::Bool, initialize_save::Bool, progress::Bool, progress_steps::Int64, progress_name::String, progress_message::typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE), userdata::Nothing, allow_extrapolation::Bool, initialize_integrator::Bool, alias_u0::Bool, alias_du0::Bool, initializealg::OrdinaryDiffEq.DefaultInit, kwargs::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}) at /home/dominik/.julia/packages/OrdinaryDiffEq/NsugH/src/solve.jl:413\n [6] #__solve#360 at /home/dominik/.julia/packages/OrdinaryDiffEq/NsugH/src/solve.jl:4 [inlined]\n [7] solve_call(::ODEProblem{Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}},Tuple{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}},false,DiffEqBase.NullParameters,ODEFunction{false,var\"#f#9\"{Quantity{Float64,𝐓^-1,Unitful.FreeUnits{(s^-1,),𝐓^-1,nothing}}},LinearAlgebra.UniformScaling{Bool},Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing,Nothing},Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}},DiffEqBase.StandardODEProblem}, ::Tsit5; merge_callbacks::Bool, kwargs::Base.Iterators.Pairs{Symbol,Float64,Tuple{Symbol,Symbol},NamedTuple{(:reltol, :abstol),Tuple{Float64,Float64}}}) at /home/dominik/.julia/packages/DiffEqBase/uSSHl/src/solve.jl:92\n [8] #solve_up#459 at /home/dominik/.julia/packages/DiffEqBase/uSSHl/src/solve.jl:114 [inlined]\n [9] #solve#458 at /home/dominik/.julia/packages/DiffEqBase/uSSHl/src/solve.jl:102 [inlined]\n [10] compute(::Quantity{Float64,𝐓^-1,Unitful.FreeUnits{(s^-1,),𝐓^-1,nothing}}, ::Quantity{Measurement{Float64},𝐋,Unitful.FreeUnits{(m,),𝐋,nothing}}, ::Tuple{Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}},Quantity{Float64,𝐓,Unitful.FreeUnits{(s,),𝐓,nothing}}}) at ./In[6]:4\n [11] top-level scope at In[15]:1\n\n\nAaaaand I guess that’s our weekly reminder that Julia’s multiple dispatch system is not magic, and doesn’t solve every problem for us!\nIf you do have an idea about how to actually run that, please say so in the comments! In the meantime, I hope you enjoyed this short read! :)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dominik Stańczak-Marikin's blog",
    "section": "",
    "text": "A pandemic story, or, what I learned working with nuclear fusion\n\n\n\n\n\n\nfusion\n\n\nipplm\n\n\nwork\n\n\nstatus\n\n\nlife\n\n\nfeatured\n\n\n\n\n\n\n\n\n\nNov 15, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nThe importance of good notation\n\n\n\n\n\n\nstatus\n\n\nmasters thesis\n\n\nmathjax\n\n\nscience\n\n\n\n\n\n\n\n\n\nMay 1, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nPlasmaPy v0.6.0 release!\n\n\n\n\n\n\nplasmapy\n\n\nrelease\n\n\npython\n\n\nprojects\n\n\n\nRelease of PlasmaPy version 0.6.0\n\n\n\n\n\nMar 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nFusion 1/4: Nuclear power as of 2021\n\n\n\n\n\n\nfusion\n\n\nscience\n\n\n\n\n\n\n\n\n\nFeb 13, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nNow with comments through utteranc.es!\n\n\n\n\n\n\nblogging\n\n\nstatus\n\n\n\n\n\n\n\n\n\nFeb 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nSymPy for physics homework in 2020\n\n\n\n\n\n\npython\n\n\nsympy\n\n\n\n\n\n\n\n\n\nSep 29, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nJulia - the magic of composability\n\n\n\n\n\n\njulia\n\n\n\n What we’ll do today is combine a simple example of an ODE solution with an uncertainty package, and see how easy or difficult it becomes using Julia.\n\n\n\n\n\nJul 21, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nParsing SC2 replays for later analysis\n\n\n\n\n\n\nstarcraft\n\n\npython\n\n\nStarCraft II Bayesian replay analysis\n\n\n\nI’ve realized I owe you an explanation on how to parse your own SC2 replays for the series of posts on Bayesian SC2 replay data analysis. Let’s go through it!\n\n\n\n\n\nJun 23, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nLearning my per-matchup MMR in Starcraft II through PyMC3\n\n\n\n\n\n\nbayes\n\n\npymc3\n\n\nstarcraft\n\n\npython\n\n\nStarCraft II Bayesian replay analysis\n\n\n\nIn this post, we’ll redo last post’s SC2 replay analysis, except we’ll take the three matchups available and infer separate MMR values for each of those.\n\n\n\n\n\nJun 20, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian modeling of StarCraft II ladder performance\n\n\n\n\n\n\nbayes\n\n\npymc3\n\n\nstarcraft\n\n\npython\n\n\nStarCraft II Bayesian replay analysis\n\n\n\nIn this post, I’m going to use PyMC3 to analyse my 2019 StarCraft II ladder games. In particular, I’m going to look at the relation between MMR - MatchMaking Rating, a metric of ladder performance - mine and that of my enemies - and what it can tell us about my win chances. \n\n\n\n\n\nJun 10, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nDealing with backlogs\n\n\n\n\n\n\npython\n\n\nscripting\n\n\ntech\n\n\n\n\n\n\n\n\n\nMay 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nFirst JOSS review!\n\n\n\n\n\n\nscience\n\n\nopen-science\n\n\nstatus\n\n\n\n\n\n\n\n\n\nMay 2, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nExport YouTube’s Watch Later playlist\n\n\n\n\n\n\nyoutube\n\n\ntech\n\n\n\n\n\n\n\n\n\nFeb 17, 2020\n\n\n\n\n\n\n\n\n\n\n\n\nBetter Numba calculation of inter-particle distance matrices\n\n\n\n\n\n\nnumba\n\n\nsimulation\n\n\nnbody\n\n\npython\n\n\ntech/N-body simulation\n\n\n\n\n\n\n\n\n\nJul 25, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSimple Binder usage with Sphinx-Gallery through Jupytext\n\n\n\n\n\n\npython\n\n\nsphinx\n\n\nplasmapy\n\n\njupyter\n\n\n\n\n\n\n\n\n\nJul 6, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nPost mortem for my engineering thesis code, PythonPIC\n\n\n\n\n\n\nplasma\n\n\nparticle-in-cell\n\n\npython\n\n\nsimulation\n\n\nprojects\n\n\n\n\n\n\n\n\n\nApr 3, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nOn the recent “On the Boris solver in Particle-in-cell simulations” paper\n\n\n\n\n\n\nplasma\n\n\npaper\n\n\nsimulation\n\n\nparticle-in-cell\n\n\npython\n\n\nfeatured\n\n\n\n\n\n\n\n\n\nMar 27, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nParticle in Cell methods\n\n\n\n\n\n\nplasma\n\n\nparticle-in-cell\n\n\npopular science\n\n\nscience\n\n\nfeatured\n\n\n\n\n\n\n\n\n\nMar 19, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nFirst dive into Julia\n\n\n\n\n\n\njulia\n\n\n\n\n\n\n\n\n\nMar 14, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nParsing and plotting LaTeX expressions with SymPy\n\n\n\n\n\n\npython\n\n\nsympy\n\n\n\n\n\n\n\n\n\nMar 12, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nCuPy speedup of naive N-Body vectorized force calculation\n\n\n\n\n\n\ncupy\n\n\nnbody\n\n\ngpu\n\n\npython\n\n\nN-body simulation\n\n\n\n\n\n\n\n\n\nMar 4, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nscipy.integrate.solve_ivp and makeshift Poincaré sections of the Rossler attractor\n\n\n\n\n\n\nscipy\n\n\nnonlinear dynamics\n\n\nsimulation\n\n\npython\n\n\nrossler\n\n\n\n\n\n\n\n\n\nFeb 24, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nQuantitative data analysis of the 2D Ising model\n\n\n\n\n\n\nising\n\n\nnumpy\n\n\npython\n\n\nsimulation\n\n\n2D Ising model\n\n\n\n\n\n\n\n\n\nFeb 5, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nParallelizable Numpy implementation of 2D Ising model\n\n\n\n\n\n\nnumpy\n\n\nising\n\n\nsimulation\n\n\npython\n\n\n2D Ising model\n\n\n\nIn which we implement an efficient 2D Ising model simulation in a few lines of Numpy code.\n\n\n\n\n\nJan 29, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nNumPy-ish GPU computations with CuPy\n\n\n\n\n\n\ncupy\n\n\nnumpy\n\n\npython\n\n\nbenchmark\n\n\n\n\n\n\n\n\n\nJan 22, 2019\n\n\n\n\n\n\nNo matching items"
  }
]